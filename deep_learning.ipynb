{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0qLbF83FhyHE",
        "x4iN-sbqP2kZ",
        "n4wTBXC_pq82",
        "qKJSaYukohKF",
        "hANDWaR4FwnQ",
        "hv3kKzmmW1cc",
        "pyensvkbh4aa",
        "E10tP-KdJScL",
        "GuKtnPK_VIS1",
        "OFaydrarvCFb",
        "XepwuIUxvqed",
        "bzLGoPqi997M",
        "StAOraZ3no8H",
        "3msMlPmbjW3p",
        "cXMnwDefjXLX",
        "ihXyJN6IW1sj",
        "cjkx0hPlpCAW",
        "eICt-a4Hk6GP",
        "Sv0GTvfMn-Ry",
        "VjdaHVwsW1Hl",
        "CtZL4iV5oRAn",
        "0MiiNfkAdEmr",
        "Z-T2lwEodZNf",
        "1K6JJx2Hj_po",
        "AMYUBmOwj_8N",
        "YB-StOuYkAMP",
        "vd_vfQf1kAeH",
        "X6LQD38_9nGm",
        "i7oaPmi8cs94",
        "4cGSn-srXWme",
        "n_oliqdmpW2K",
        "FmEFErTspXGJ",
        "Ngk5ZNinpXU6",
        "gBpHGlyepXqp",
        "J0YZ9ORPXW3u"
      ],
      "authorship_tag": "ABX9TyO4di8GKY87FKeM+K1dsVcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a74cac3c56b24850a137701c854dc24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_593fd4d8bcb14d978f7018c77e051600",
              "IPY_MODEL_d555bd740e3a4d0eb050d3692e6088af",
              "IPY_MODEL_264c0a2608d746abb50d8199bb07f5e4"
            ],
            "layout": "IPY_MODEL_52e550ff7ad2455e8ecab9d1afccf94c"
          }
        },
        "593fd4d8bcb14d978f7018c77e051600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a59fda4e21a440c8cca073aa1d96b38",
            "placeholder": "​",
            "style": "IPY_MODEL_1d9060ac61d0412cb034bd07a234feaa",
            "value": "100%"
          }
        },
        "d555bd740e3a4d0eb050d3692e6088af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_884ce49ee215460a9f253af22edef78c",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e22c660bbde945adbf216b2939001b88",
            "value": 9912422
          }
        },
        "264c0a2608d746abb50d8199bb07f5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd0592879da2438e9e780d2c53f319e2",
            "placeholder": "​",
            "style": "IPY_MODEL_57cb89c3b128400785964b96e46b3820",
            "value": " 9912422/9912422 [00:00&lt;00:00, 9153530.54it/s]"
          }
        },
        "52e550ff7ad2455e8ecab9d1afccf94c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a59fda4e21a440c8cca073aa1d96b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9060ac61d0412cb034bd07a234feaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "884ce49ee215460a9f253af22edef78c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e22c660bbde945adbf216b2939001b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd0592879da2438e9e780d2c53f319e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57cb89c3b128400785964b96e46b3820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82980009c9ce436d84e5340d0624e355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c18e7d32a3414710800376e4ad3f82ec",
              "IPY_MODEL_5aee56b3ea534a53aba9f61bd43804d7",
              "IPY_MODEL_78eb8bd2830d46a9a731073f482b80ac"
            ],
            "layout": "IPY_MODEL_a30cbc82bff74ae3bf76149812b68cb2"
          }
        },
        "c18e7d32a3414710800376e4ad3f82ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_717426b5f7f34f90b6a9d605860c048b",
            "placeholder": "​",
            "style": "IPY_MODEL_899031868bc3427a9d6d6cf8dfe1a93a",
            "value": "100%"
          }
        },
        "5aee56b3ea534a53aba9f61bd43804d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f25ed6cb7274403eb8e8f779e7e9937f",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cd7d9fb519d47ce9a9decd81c0e060c",
            "value": 28881
          }
        },
        "78eb8bd2830d46a9a731073f482b80ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dbe1a39536941388d1119c5f0f13cb4",
            "placeholder": "​",
            "style": "IPY_MODEL_83ca57aceab24ca9aa7511b5594aba41",
            "value": " 28881/28881 [00:00&lt;00:00, 1025131.54it/s]"
          }
        },
        "a30cbc82bff74ae3bf76149812b68cb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717426b5f7f34f90b6a9d605860c048b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "899031868bc3427a9d6d6cf8dfe1a93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f25ed6cb7274403eb8e8f779e7e9937f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd7d9fb519d47ce9a9decd81c0e060c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7dbe1a39536941388d1119c5f0f13cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83ca57aceab24ca9aa7511b5594aba41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "968352747d664161b026a74193da6381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f474464a36f94ae5a3d8e90db627b787",
              "IPY_MODEL_b43a91c50aad421abde5694593500649",
              "IPY_MODEL_4567f6ab0c3b4d698dd7a0b9b734ab91"
            ],
            "layout": "IPY_MODEL_a1f90575be6b4851a3cf5fe475be1eb6"
          }
        },
        "f474464a36f94ae5a3d8e90db627b787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f35927e8ee49c49f4af002ba6808e3",
            "placeholder": "​",
            "style": "IPY_MODEL_f219d2ca0fd54655b7622e48c5290249",
            "value": "100%"
          }
        },
        "b43a91c50aad421abde5694593500649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06ea5dee1337461bb87d3d7ba8ad154f",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcc9a8bc772240ff9d75677163fc8585",
            "value": 1648877
          }
        },
        "4567f6ab0c3b4d698dd7a0b9b734ab91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d68b0852b344ab5879ae5712fbcb2d0",
            "placeholder": "​",
            "style": "IPY_MODEL_33a58bd8564d491bb085fe8278772de4",
            "value": " 1648877/1648877 [00:00&lt;00:00, 19693687.72it/s]"
          }
        },
        "a1f90575be6b4851a3cf5fe475be1eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f35927e8ee49c49f4af002ba6808e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f219d2ca0fd54655b7622e48c5290249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06ea5dee1337461bb87d3d7ba8ad154f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc9a8bc772240ff9d75677163fc8585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d68b0852b344ab5879ae5712fbcb2d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a58bd8564d491bb085fe8278772de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8f1a63398f941b1a6703ad24cbb91dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f687fd8f1a1b43b2adaf2540f54ff051",
              "IPY_MODEL_16eabf5e40cb4e50849fa3d0e21151ec",
              "IPY_MODEL_12f8818fac9e46a38c27b70a6abe5e22"
            ],
            "layout": "IPY_MODEL_6e7f10fe1d994d1c9b122f7ecb7988ad"
          }
        },
        "f687fd8f1a1b43b2adaf2540f54ff051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2560cb529df04494ac2270875df46b05",
            "placeholder": "​",
            "style": "IPY_MODEL_32e6265cd48f42bbb17bc012b199dfa6",
            "value": "100%"
          }
        },
        "16eabf5e40cb4e50849fa3d0e21151ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b19493f59a024b71a188dbdba423f1fe",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_661abc2e1ce442af9ad81aad257067a9",
            "value": 4542
          }
        },
        "12f8818fac9e46a38c27b70a6abe5e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c714aa539f8740738103f5a37d4a2d01",
            "placeholder": "​",
            "style": "IPY_MODEL_d82b546705914748b1403edc4c9e3890",
            "value": " 4542/4542 [00:00&lt;00:00, 214680.45it/s]"
          }
        },
        "6e7f10fe1d994d1c9b122f7ecb7988ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2560cb529df04494ac2270875df46b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32e6265cd48f42bbb17bc012b199dfa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b19493f59a024b71a188dbdba423f1fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "661abc2e1ce442af9ad81aad257067a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c714aa539f8740738103f5a37d4a2d01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d82b546705914748b1403edc4c9e3890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SungchulLee/colab/blob/main/deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 0 Installment Extra Packages***"
      ],
      "metadata": {
        "id": "0qLbF83FhyHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVCe6r72P2Uc",
        "outputId": "27f29978-fcb0-46df-c06e-ca92c06fe938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.4-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 KB\u001b[0m \u001b[31m927.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html5lib>=1.1\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.3.5)\n",
            "Collecting beautifulsoup4>=4.11.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.8/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.8/dist-packages (from yfinance) (2022.7)\n",
            "Collecting cryptography>=3.3.2\n",
            "  Downloading cryptography-39.0.0-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozendict>=2.3.4\n",
            "  Downloading frozendict-2.3.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from html5lib>=1.1->yfinance) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
            "Installing collected packages: soupsieve, requests, html5lib, frozendict, cryptography, beautifulsoup4, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Attempting uninstall: html5lib\n",
            "    Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.1 cryptography-39.0.0 frozendict-2.3.4 html5lib-1.1 requests-2.28.2 soupsieve-2.3.2.post1 yfinance-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 1 Tensor***"
      ],
      "metadata": {
        "id": "x4iN-sbqP2kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\begin{array}{lcc}\n",
        "&\\text{Numpy Array}&\\text{Tensor}\\\\\\hline\n",
        "\\text{Run on CPU}&✔&✔\\\\\n",
        "\\text{Run on GPU}&&✔\\\\\n",
        "\\text{Support Auto Differentiation}&&✔\\\\\n",
        "\\end{array}$$"
      ],
      "metadata": {
        "id": "f-rikyuSPMRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Gentle Introduction of Tensors [beginner_source/nlp/pytorch_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/nlp/pytorch_tutorial.py)"
      ],
      "metadata": {
        "id": "n4wTBXC_pq82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/nlp/pytorch_tutorial.py\n",
        "\"\"\"\n",
        "Introduction to PyTorch\n",
        "***********************\n",
        "\n",
        "Introduction to Torch's tensor library\n",
        "======================================\n",
        "\n",
        "All of deep learning is computations on tensors, which are\n",
        "generalizations of a matrix that can be indexed in more than 2 dimensions. \n",
        "We will see exactly what this means in-depth later. \n",
        "First, let's look what we can do with tensors.\n",
        "\"\"\"\n",
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "################################################################################\n",
        "# Creating Tensors\n",
        "#\n",
        "# Tensors can be created from Python lists with the torch.tensor()\n",
        "# function.\n",
        "################################################################################\n",
        "\n",
        "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
        "V_data = [1., 2., 3.]\n",
        "V = torch.tensor(V_data)\n",
        "print(V)\n",
        "\n",
        "# Creates a matrix\n",
        "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
        "M = torch.tensor(M_data)\n",
        "print(M)\n",
        "\n",
        "# Create a 3D tensor of size 2x2x2.\n",
        "T_data = [[[1., 2.], [3., 4.]],\n",
        "          [[5., 6.], [7., 8.]]]\n",
        "T = torch.tensor(T_data)\n",
        "print(T)\n",
        "\n",
        "################################################################################\n",
        "# What is a 3D tensor anyway? \n",
        "# Think about it like this. \n",
        "# If you have a vector, indexing into the vector gives you a scalar. \n",
        "# If you have a matrix, indexing into the matrix gives you a vector. \n",
        "# If you have a 3D tensor, then indexing into the tensor gives you a matrix!\n",
        "#\n",
        "# A note on terminology:\n",
        "# when I say \"tensor\" in this tutorial, \n",
        "# it refers to any torch.Tensor object. \n",
        "# Matrices and vectors are special cases of torch.Tensors, \n",
        "# where their dimension is 2 and 1 respectively. \n",
        "# When I am talking about 3D tensors, \n",
        "# I will explicitly use the term \"3D tensor\".\n",
        "################################################################################\n",
        "\n",
        "# Index into V and get a scalar (0 dimensional tensor)\n",
        "print(V[0])\n",
        "# Get a Python number from it\n",
        "print(V[0].item())\n",
        "\n",
        "# Index into M and get a vector\n",
        "print(M[0])\n",
        "\n",
        "# Index into T and get a matrix\n",
        "print(T[0])\n",
        "\n",
        "################################################################################\n",
        "# You can also create tensors of other data types. \n",
        "# To create a tensor of integer types, try torch.tensor([[1, 2], [3, 4]]) \n",
        "# (where all elements in the list are integers).\n",
        "# You can also specify a data type by passing in ``dtype=torch.data_type``.\n",
        "# Check the documentation for more data types, but\n",
        "# Float and Long will be the most common.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# You can create a tensor with random data and \n",
        "# the supplied dimensionality with torch.randn()\n",
        "################################################################################\n",
        "\n",
        "x = torch.randn((3, 4, 5))\n",
        "print(x)\n",
        "\n",
        "################################################################################\n",
        "# Operations with Tensors\n",
        "#\n",
        "# You can operate on tensors in the ways you would expect.\n",
        "################################################################################\n",
        "\n",
        "x = torch.tensor([1., 2., 3.])\n",
        "y = torch.tensor([4., 5., 6.])\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "################################################################################\n",
        "# See `the documentation <https://pytorch.org/docs/torch.html>` \n",
        "# for a complete list of the massive number of operations available to you. \n",
        "# They expand beyond just mathematical operations.\n",
        "#\n",
        "# One helpful operation that we will make use of later is concatenation.\n",
        "################################################################################\n",
        "\n",
        "# By default, it concatenates along the first axis (concatenates rows)\n",
        "x_1 = torch.randn(2, 5)\n",
        "y_1 = torch.randn(3, 5)\n",
        "z_1 = torch.cat([x_1, y_1])\n",
        "print(z_1)\n",
        "\n",
        "# Concatenate columns:\n",
        "x_2 = torch.randn(2, 3)\n",
        "y_2 = torch.randn(2, 5)\n",
        "# second arg specifies which axis to concat along\n",
        "z_2 = torch.cat([x_2, y_2], 1)\n",
        "print(z_2)\n",
        "\n",
        "# If your tensors are not compatible, torch will complain.\n",
        "# Uncomment to see the error\n",
        "# torch.cat([x_1, x_2])\n",
        "\n",
        "################################################################################\n",
        "# Reshaping Tensors\n",
        "#\n",
        "# Use the .view() method to reshape a tensor. \n",
        "# This method receives heavy use, because  \n",
        "# many neural network components expect their inputs to have a certain shape. \n",
        "# Often you will need to reshape before passing your data to the component.\n",
        "################################################################################\n",
        "\n",
        "x = torch.randn(2, 3, 4)\n",
        "print(x)\n",
        "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
        "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
        "print(x.view(2, -1))\n",
        "\n",
        "################################################################################\n",
        "# Computation Graphs and Automatic Differentiation\n",
        "#\n",
        "# The concept of a computation graph is essential \n",
        "# to efficient deep learning programming, because  \n",
        "# it allows you to not have to write the back propagation gradients yourself. \n",
        "# A computation graph is simply a specification of \n",
        "# how your data is combined to give you the output. \n",
        "# Since the graph totally specifies what parameters were involved \n",
        "# with which operations, \n",
        "# it contains enough information to compute derivatives. \n",
        "# This probably sounds vague, \n",
        "# so let's see what is going on using the fundamental flag ``requires_grad``.\n",
        "#\n",
        "# First, think from a programmers perspective. \n",
        "# What is stored in the torch.Tensor objects we were creating above? \n",
        "# Obviously the data and the shape, and maybe a few other things. \n",
        "# But when we added two tensors together, we got an output tensor. \n",
        "# All this output tensor knows is its data and shape. \n",
        "# It has no idea that it was the sum of two other tensors\n",
        "# (it could have been read in from a file, \n",
        "# it could be the result of some other operation, etc.)\n",
        "#\n",
        "# If ``requires_grad=True``, \n",
        "# the Tensor object keeps track of how it was created. \n",
        "# Let's see it in action.\n",
        "################################################################################\n",
        "\n",
        "# Tensor factory methods have a ``requires_grad`` flag\n",
        "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
        "\n",
        "# With requires_grad=True, you can still do all the operations you previously\n",
        "# could\n",
        "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "# BUT z knows something extra.\n",
        "print(z.grad_fn)\n",
        "\n",
        "################################################################################\n",
        "# So Tensors know what created them. \n",
        "# z knows that it wasn't read in from a file, \n",
        "# it wasn't the result of a multiplication or exponential or whatever. \n",
        "# And if you keep following z.grad_fn, you will find yourself at x and y.\n",
        "#\n",
        "# But how does that help us compute a gradient?\n",
        "################################################################################\n",
        "\n",
        "# Let's sum up all the entries in z\n",
        "s = z.sum()\n",
        "print(s)\n",
        "print(s.grad_fn)\n",
        "\n",
        "################################################################################\n",
        "# So now, what is the derivative of this sum \n",
        "# with respect to the first component of x? \n",
        "# In math, we want\n",
        "#\n",
        "# .. math::\n",
        "#\n",
        "#    \\frac{\\partial s}{\\partial x_0}\n",
        "#\n",
        "# Well, s knows that it was created as a sum of the tensor z. \n",
        "# z knows that it was the sum x + y. So\n",
        "#\n",
        "# .. math::  s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\n",
        "#\n",
        "# And so s contains enough information \n",
        "# to determine that the derivative we want is 1!\n",
        "#\n",
        "# Of course this glosses over the challenge of \n",
        "# how to actually compute that derivative. \n",
        "# The point here is that s is carrying along enough information that \n",
        "# it is possible to compute it. \n",
        "# In reality, the developers of Pytorch program the sum() and + operations to  \n",
        "# know how to compute their gradients, and run the back propagation algorithm. \n",
        "# An in-depth discussion of that algorithm is beyond the scope of this tutorial.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Let's have Pytorch compute the gradient, and see that we were right:\n",
        "# (note if you run this block multiple times, the gradient will increment.\n",
        "# That is because Pytorch *accumulates* the gradient into the .grad property, \n",
        "# since for many models this is very convenient.)\n",
        "################################################################################\n",
        "\n",
        "# calling .backward() on any variable will run backprop, starting from it.\n",
        "s.backward()\n",
        "print(x.grad)\n",
        "\n",
        "################################################################################\n",
        "# Understanding what is going on in the block below is crucial \n",
        "# for being a successful programmer in deep learning.\n",
        "################################################################################\n",
        "\n",
        "x = torch.randn(2, 2)\n",
        "y = torch.randn(2, 2)\n",
        "# By default, user created Tensors have ``requires_grad=False``\n",
        "print(x.requires_grad, y.requires_grad)\n",
        "z = x + y\n",
        "# So you can't backprop through z\n",
        "print(z.grad_fn)\n",
        "\n",
        "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
        "# flag in-place. The input flag defaults to ``True`` if not given.\n",
        "x = x.requires_grad_()\n",
        "y = y.requires_grad_()\n",
        "# z contains enough information to compute gradients, as we saw above\n",
        "z = x + y\n",
        "print(z.grad_fn)\n",
        "# If any input to an operation has ``requires_grad=True``, so will the output\n",
        "print(z.requires_grad)\n",
        "\n",
        "# Now z has the computation history that relates itself to x and y\n",
        "# Can we just take its values, and **detach** it from its history?\n",
        "new_z = z.detach()\n",
        "\n",
        "# ... does new_z have information to backprop to x and y?\n",
        "# NO!\n",
        "print(new_z.grad_fn)\n",
        "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
        "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
        "# about how it was computed.\n",
        "# In essence, we have broken the Tensor away from its past history\n",
        "\n",
        "################################################################################\n",
        "# You can also stop autograd \n",
        "# from tracking history on Tensors with ``.requires_grad=True`` \n",
        "# by wrapping the code block in ``with torch.no_grad():``\n",
        "################################################################################\n",
        "\n",
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "\tprint((x ** 2).requires_grad)"
      ],
      "metadata": {
        "id": "qXmUtOCdprJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Tensors work like Numpy Array [beginner_source/introyt/tensors_deeper_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/tensors_deeper_tutorial.py) [english](https://www.youtube.com/watch?v=r7QDUPb2dCM)"
      ],
      "metadata": {
        "id": "qKJSaYukohKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/tensors_deeper_tutorial.py\n",
        "\"\"\"\n",
        "Introduction to PyTorch Tensors\n",
        "===============================\n",
        "\n",
        "Follow along with the video below or on \n",
        "`youtube <https://www.youtube.com/watch?v=r7QDUPb2dCM>`.\n",
        "\n",
        "Tensors are the central data abstraction in PyTorch. \n",
        "This interactive notebook provides \n",
        "an in-depth introduction to the ``torch.Tensor`` class.\n",
        "\n",
        "First things first, let’s import the PyTorch module. \n",
        "We’ll also add Python’s math module to facilitate some of the examples.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "################################################################################\n",
        "# Creating Tensors\n",
        "# \n",
        "# The simplest way to create a tensor is with the ``torch.empty()`` call:\n",
        "################################################################################ \n",
        "\n",
        "x = torch.empty(3, 4)\n",
        "print(type(x))\n",
        "print(x)\n",
        "\n",
        "################################################################################\n",
        "# Let’s unpack what we just did:\n",
        "# \n",
        "# -  We created a tensor using one of the numerous factory methods\n",
        "#    attached to the ``torch`` module.\n",
        "# -  The tensor itself is 2-dimensional, having 3 rows and 4 columns.\n",
        "# -  The type of the object returned is ``torch.Tensor``, \n",
        "#    which is an alias for ``torch.FloatTensor``; \n",
        "#    by default, PyTorch tensors are populated \n",
        "#    with 32-bit floating point numbers. (More on data types below.)\n",
        "# -  You will probably see some random-looking values \n",
        "#    when printing your tensor. \n",
        "#    The ``torch.empty()`` call allocates memory for the tensor, \n",
        "#    but does not initialize it with any values - so what you’re seeing is\n",
        "#    whatever was in memory at the time of allocation.\n",
        "# \n",
        "# A brief note about tensors and their number of dimensions, and terminology:\n",
        "# \n",
        "# -  You will sometimes see a 1-dimensional tensor called a *vector.* \n",
        "# -  Likewise, a 2-dimensional tensor is often referred to as a *matrix.* \n",
        "# -  Anything with more than two dimensions is generally just called a tensor.\n",
        "# \n",
        "# More often than not, you’ll want to initialize your tensor with some value. \n",
        "# Common cases are all zeros, all ones, or random values, and the\n",
        "# ``torch`` module provides factory methods for all of these:\n",
        "################################################################################ \n",
        "\n",
        "zeros = torch.zeros(2, 3)\n",
        "print(zeros)\n",
        "\n",
        "ones = torch.ones(2, 3)\n",
        "print(ones)\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random = torch.rand(2, 3)\n",
        "print(random)\n",
        "\n",
        "################################################################################\n",
        "# The factory methods all do just what you’d expect - we have a tensor\n",
        "# full of zeros, another full of ones, and another with random values\n",
        "# between 0 and 1.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Random Tensors and Seeding\n",
        "# \n",
        "# Speaking of the random tensor, did you notice the call to\n",
        "# ``torch.manual_seed()`` immediately preceding it? \n",
        "# Initializing tensors, such as a model’s learning weights, \n",
        "# with random values is common but there are times - \n",
        "# especially in research settings - where you’ll want\n",
        "# some assurance of the reproducibility of your results. \n",
        "# Manually setting your random number generator’s seed is the way to do this. \n",
        "# Let’s look more closely:\n",
        "################################################################################\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random1 = torch.rand(2, 3)\n",
        "print(random1)\n",
        "\n",
        "random2 = torch.rand(2, 3)\n",
        "print(random2)\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random3 = torch.rand(2, 3)\n",
        "print(random3)\n",
        "\n",
        "random4 = torch.rand(2, 3)\n",
        "print(random4)\n",
        "\n",
        "################################################################################\n",
        "# What you should see above is that \n",
        "# ``random1`` and ``random3`` carry identical values, \n",
        "# as do ``random2`` and ``random4``. \n",
        "# Manually setting the RNG’s seed resets it, \n",
        "# so that identical computations depending on\n",
        "# random number should, in most settings, provide identical results.\n",
        "# \n",
        "# For more information, see the `PyTorch documentation on\n",
        "# reproducibility <https://pytorch.org/docs/stable/notes/randomness.html>`.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Tensor Shapes\n",
        "# \n",
        "# Often, when you’re performing operations on two or more tensors, \n",
        "# they will need to be of the same *shape* - that is, \n",
        "# having the same number of\n",
        "# dimensions and the same number of cells in each dimension. \n",
        "# For that, we have the ``torch.*_like()`` methods:\n",
        "################################################################################\n",
        "\n",
        "x = torch.empty(2, 2, 3)\n",
        "print(x.shape)\n",
        "print(x)\n",
        "\n",
        "empty_like_x = torch.empty_like(x)\n",
        "print(empty_like_x.shape)\n",
        "print(empty_like_x)\n",
        "\n",
        "zeros_like_x = torch.zeros_like(x)\n",
        "print(zeros_like_x.shape)\n",
        "print(zeros_like_x)\n",
        "\n",
        "ones_like_x = torch.ones_like(x)\n",
        "print(ones_like_x.shape)\n",
        "print(ones_like_x)\n",
        "\n",
        "rand_like_x = torch.rand_like(x)\n",
        "print(rand_like_x.shape)\n",
        "print(rand_like_x)\n",
        "\n",
        "################################################################################\n",
        "# The first new thing in the code cell above is \n",
        "# the use of the ``.shape`` property on a tensor. \n",
        "# This property contains a list of the extent of each dimension of a tensor - \n",
        "# in our case, ``x`` is a three-dimensional tensor with shape 2 x 2 x 3.\n",
        "# \n",
        "# Below that, we call the ``.empty_like()``, ``.zeros_like()``,\n",
        "# ``.ones_like()``, and ``.rand_like()`` methods. \n",
        "# Using the ``.shape`` property, \n",
        "# we can verify that each of these methods returns a tensor of\n",
        "# identical dimensionality and extent.\n",
        "# \n",
        "# The last way to create a tensor that will cover is to specify its data\n",
        "# directly from a PyTorch collection:\n",
        "################################################################################\n",
        "\n",
        "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
        "print(some_constants)\n",
        "\n",
        "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
        "print(some_integers)\n",
        "\n",
        "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
        "print(more_integers)\n",
        "\n",
        "################################################################################\n",
        "# Using ``torch.tensor()`` is the most straightforward way \n",
        "# to create a tensor if you already have data in a Python tuple or list. \n",
        "# As shown above, nesting the collections will result in \n",
        "# a multi-dimensional tensor.\n",
        "# \n",
        "# .. note::\n",
        "#      ``torch.tensor()`` creates a copy of the data.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Tensor Data Types\n",
        "# \n",
        "# Setting the datatype of a tensor is possible a couple of ways:\n",
        "################################################################################\n",
        "\n",
        "a = torch.ones((2, 3), dtype=torch.int16)\n",
        "print(a)\n",
        "\n",
        "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
        "print(b)\n",
        "\n",
        "c = b.to(torch.int32)\n",
        "print(c)\n",
        "\n",
        "################################################################################\n",
        "# The simplest way to set the underlying data type of a tensor is \n",
        "# with an optional argument at creation time. \n",
        "# In the first line of the cell above,\n",
        "# we set ``dtype=torch.int16`` for the tensor ``a``. \n",
        "# When we print ``a``,\n",
        "# we can see that it’s full of ``1`` rather than ``1.`` \n",
        "# - Python’s subtle cue that this is an integer type rather than floating point.\n",
        "# \n",
        "# Another thing to notice about printing ``a`` is that, \n",
        "# unlike when we left ``dtype`` as the default (32-bit floating point), \n",
        "# printing the tensor also specifies its ``dtype``.\n",
        "# \n",
        "# You may have also spotted that \n",
        "# we went from specifying the tensor’s shape as a series of integer arguments, \n",
        "# to grouping those arguments in a tuple. \n",
        "# This is not strictly necessary - \n",
        "# PyTorch will take a series of \n",
        "# initial, unlabeled integer arguments as a tensor shape - \n",
        "# but when adding the optional arguments, it can make your intent more readable.\n",
        "# \n",
        "# The other way to set the datatype is with the ``.to()`` method. \n",
        "# In the cell above, we create a random floating point tensor ``b`` \n",
        "# in the usual way. \n",
        "# Following that, we create ``c`` by converting ``b`` to a 32-bit\n",
        "# integer with the ``.to()`` method. \n",
        "# Note that ``c`` contains all the same values as ``b``, \n",
        "# but truncated to integers.\n",
        "# \n",
        "# Available data types include:\n",
        "# \n",
        "# -  ``torch.bool``\n",
        "# -  ``torch.int8``\n",
        "# -  ``torch.uint8``\n",
        "# -  ``torch.int16``\n",
        "# -  ``torch.int32``\n",
        "# -  ``torch.int64``\n",
        "# -  ``torch.half``\n",
        "# -  ``torch.float``\n",
        "# -  ``torch.double``\n",
        "# -  ``torch.bfloat``\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Math & Logic with PyTorch Tensors\n",
        "# \n",
        "# Now that you know some of the ways to create a tensor… \n",
        "# what can you do with them?\n",
        "# \n",
        "# Let’s look at basic arithmetic first, and \n",
        "# how tensors interact with simple scalars:\n",
        "################################################################################\n",
        "\n",
        "ones = torch.zeros(2, 2) + 1\n",
        "twos = torch.ones(2, 2) * 2\n",
        "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
        "fours = twos ** 2\n",
        "sqrt2s = twos ** 0.5\n",
        "\n",
        "print(ones)\n",
        "print(twos)\n",
        "print(threes)\n",
        "print(fours)\n",
        "print(sqrt2s)\n",
        "\n",
        "################################################################################\n",
        "# As you can see above, \n",
        "# arithmetic operations between tensors and scalars,\n",
        "# such as addition, subtraction, multiplication, division, and\n",
        "# exponentiation are distributed over every element of the tensor. \n",
        "# Because the output of such an operation will be a tensor, \n",
        "# you can chain them together with the usual operator precedence rules, \n",
        "# as in the line where we create ``threes``.\n",
        "# \n",
        "# Similar operations between two tensors also behave \n",
        "# like you’d intuitively expect:\n",
        "################################################################################\n",
        "\n",
        "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
        "print(powers2)\n",
        "\n",
        "fives = ones + fours\n",
        "print(fives)\n",
        "\n",
        "dozens = threes * fours\n",
        "print(dozens)\n",
        "\n",
        "################################################################################\n",
        "# It’s important to note here that \n",
        "# all of the tensors in the previous code cell were of identical shape. \n",
        "# What happens when we try to perform a\n",
        "# binary operation on tensors if dissimilar shape?\n",
        "# \n",
        "# .. note::\n",
        "#      The following cell throws a run-time error. This is intentional.\n",
        "#\n",
        "#    a = torch.rand(2, 3)\n",
        "#    b = torch.rand(3, 2)\n",
        "#\n",
        "#    print(a * b)\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# In the general case, \n",
        "# you cannot operate on tensors of different shape this way, \n",
        "# even in a case like the cell above, \n",
        "# where the tensors have an identical number of elements.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# In Brief: Tensor Broadcasting\n",
        "# \n",
        "# .. note::\n",
        "#      If you are familiar with broadcasting semantics in NumPy ndarrays, \n",
        "#      you’ll find the same rules apply here.\n",
        "# \n",
        "# The exception to the same-shapes rule is *tensor broadcasting.* \n",
        "# Here’s an example:\n",
        "################################################################################\n",
        "\n",
        "rand = torch.rand(2, 4)\n",
        "doubled = rand * (torch.ones(1, 4) * 2)\n",
        "\n",
        "print(rand)\n",
        "print(doubled)\n",
        "\n",
        "################################################################################\n",
        "# What’s the trick here? \n",
        "# How is it we got to multiply a 2x4 tensor by a 1x4 tensor?\n",
        "# \n",
        "# Broadcasting is a way to perform an operation \n",
        "# between tensors that have similarities in their shapes. \n",
        "# In the example above, the one-row, four-column tensor \n",
        "# is multiplied by *both rows* of the two-row, four-column tensor.\n",
        "# \n",
        "# This is an important operation in Deep Learning. \n",
        "# The common example is \n",
        "# multiplying a tensor of learning weights by a *batch* of input tensors,\n",
        "# applying the operation to each instance in the batch separately, and\n",
        "# returning a tensor of identical shape - \n",
        "# just like our (2, 4) \\* (1, 4) example above returned \n",
        "# a tensor of shape (2, 4).\n",
        "# \n",
        "# The rules for broadcasting are:\n",
        "# \n",
        "# -  Each tensor must have at least one dimension - no empty tensors.\n",
        "# \n",
        "# -  Comparing the dimension sizes of the two tensors, \n",
        "#    *going from last to first:*\n",
        "# \n",
        "#    -  Each dimension must be equal, *or*\n",
        "# \n",
        "#    -  One of the dimensions must be of size 1, *or*\n",
        "# \n",
        "#    -  The dimension does not exist in one of the tensors\n",
        "# \n",
        "# Tensors of identical shape, of course, are trivially “broadcastable”, \n",
        "# as you saw earlier.\n",
        "# \n",
        "# Here are some examples of situations that \n",
        "# honor the above rules and allow broadcasting:\n",
        "################################################################################ \n",
        "\n",
        "a =     torch.ones(4, 3, 2)\n",
        "\n",
        "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
        "print(b)\n",
        "\n",
        "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
        "print(c)\n",
        "\n",
        "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
        "print(d)\n",
        "\n",
        "################################################################################\n",
        "# Look closely at the values of each tensor above: \n",
        "#\n",
        "# -  The multiplication operation that created ``b`` was \n",
        "#    broadcast over every “layer” of ``a``.\n",
        "# -  For ``c``, \n",
        "#    the operation was broadcast over ever layer and row of ``a`` - \n",
        "#    every 3-element column is identical. \n",
        "# -  For ``d``, we switched it around - now every *row* is identical,\n",
        "#    across layers and columns.\n",
        "# \n",
        "# For more information on broadcasting, see the `PyTorch documentation \n",
        "# <https://pytorch.org/docs/stable/notes/broadcasting.html>`\n",
        "# on the topic.\n",
        "# \n",
        "# Here are some examples of attempts at broadcasting that will fail:\n",
        "# \n",
        "# .. note::\n",
        "#       The following cell throws a run-time error. This is intentional.\n",
        "#\n",
        "#    a =     torch.ones(4, 3, 2)\n",
        "#\n",
        "#    b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
        "#\n",
        "#    c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
        "#\n",
        "#    d = a * torch.rand((0, ))   # can't broadcast with an empty tensor\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# More Math with Tensors\n",
        "# \n",
        "# PyTorch tensors have over three hundred operations \n",
        "# that can be performed on them.\n",
        "# \n",
        "# Here is a small sample from some of the major categories of operations:\n",
        "################################################################################\n",
        "\n",
        "# common functions\n",
        "a = torch.rand(2, 4) * 2 - 1\n",
        "print('Common functions:')\n",
        "print(torch.abs(a))\n",
        "print(torch.ceil(a))\n",
        "print(torch.floor(a))\n",
        "print(torch.clamp(a, -0.5, 0.5))\n",
        "\n",
        "# trigonometric functions and their inverses\n",
        "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
        "sines = torch.sin(angles)\n",
        "inverses = torch.asin(sines)\n",
        "print('\\nSine and arcsine:')\n",
        "print(angles)\n",
        "print(sines)\n",
        "print(inverses)\n",
        "\n",
        "# bitwise operations\n",
        "print('\\nBitwise XOR:')\n",
        "b = torch.tensor([1, 5, 11])\n",
        "c = torch.tensor([2, 7, 10])\n",
        "print(torch.bitwise_xor(b, c))\n",
        "\n",
        "# comparisons:\n",
        "print('\\nBroadcasted, element-wise equality comparison:')\n",
        "d = torch.tensor([[1., 2.], [3., 4.]])\n",
        "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
        "print(torch.eq(d, e)) # returns a tensor of type bool\n",
        "\n",
        "# reductions:\n",
        "print('\\nReduction ops:')\n",
        "print(torch.max(d))        # returns a single-element tensor\n",
        "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
        "print(torch.mean(d))       # average\n",
        "print(torch.std(d))        # standard deviation\n",
        "print(torch.prod(d))       # product of all numbers\n",
        "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
        "\n",
        "# vector and linear algebra operations\n",
        "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
        "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
        "m1 = torch.rand(2, 2)                   # random matrix\n",
        "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
        "\n",
        "print('\\nVectors & Matrices:')\n",
        "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
        "print(m1)\n",
        "m3 = torch.matmul(m1, m2)\n",
        "print(m3)                  # 3 times m1\n",
        "print(torch.svd(m3))       # singular value decomposition\n",
        "\n",
        "################################################################################\n",
        "# This is a small sample of operations. \n",
        "# For more details and the full inventory of math functions, have a look at the\n",
        "# `documentation <https://pytorch.org/docs/stable/torch.html#math-operations>`.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Altering Tensors in Place\n",
        "# \n",
        "# Most binary operations on tensors will return a third, new tensor. \n",
        "# When we say ``c = a * b`` (where ``a`` and ``b`` are tensors), \n",
        "# the new tensor ``c`` will occupy a region of memory distinct \n",
        "# from the other tensors.\n",
        "# \n",
        "# There are times, though, that you may wish to alter a tensor in place -\n",
        "# for example, if you’re doing an element-wise computation \n",
        "# where you can discard intermediate values. \n",
        "# For this, most of the math functions have a version \n",
        "# with an appended underscore (``_``) that will alter a tensor in place.\n",
        "# \n",
        "# For example:\n",
        "################################################################################\n",
        "\n",
        "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
        "print('a:')\n",
        "print(a)\n",
        "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
        "print(a)              # a has not changed\n",
        "\n",
        "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
        "print('\\nb:')\n",
        "print(b)\n",
        "print(torch.sin_(b))  # note the underscore\n",
        "print(b)              # b has changed\n",
        "\n",
        "################################################################################\n",
        "# For arithmetic operations, there are functions that behave similarly:\n",
        "################################################################################\n",
        "\n",
        "a = torch.ones(2, 2)\n",
        "b = torch.rand(2, 2)\n",
        "\n",
        "print('Before:')\n",
        "print(a)\n",
        "print(b)\n",
        "print('\\nAfter adding:')\n",
        "print(a.add_(b))\n",
        "print(a)\n",
        "print(b)\n",
        "print('\\nAfter multiplying')\n",
        "print(b.mul_(b))\n",
        "print(b)\n",
        "\n",
        "################################################################################\n",
        "# Note that these in-place arithmetic functions are methods \n",
        "# on the ``torch.Tensor`` object, \n",
        "# not attached to the ``torch`` module \n",
        "# like many other functions (e.g., ``torch.sin()``). \n",
        "# As you can see from ``a.add_(b)``, \n",
        "# *the calling tensor is the one that gets changed in place.*\n",
        "# \n",
        "# There is another option for placing the result of a computation \n",
        "# in an existing, allocated tensor. \n",
        "# Many of the methods and functions we’ve seen so far - \n",
        "# including creation methods! - \n",
        "# have an ``out`` argument that lets you specify a tensor to receive the output. \n",
        "# If the ``out`` tensor is the correct shape and ``dtype``, \n",
        "# this can happen without a new memory allocation:\n",
        "################################################################################\n",
        "\n",
        "a = torch.rand(2, 2)\n",
        "b = torch.rand(2, 2)\n",
        "c = torch.zeros(2, 2)\n",
        "old_id = id(c)\n",
        "\n",
        "print(c)\n",
        "d = torch.matmul(a, b, out=c)\n",
        "print(c)                # contents of c have changed\n",
        "\n",
        "assert c is d           # test c & d are same object, not just containing equal values\n",
        "assert id(c), old_id    # make sure that our new c is the same object as the old one\n",
        "\n",
        "torch.rand(2, 2, out=c) # works for creation too!\n",
        "print(c)                # c has changed again\n",
        "assert id(c), old_id    # still the same object!\n",
        "\n",
        "################################################################################\n",
        "# Copying Tensors\n",
        "# \n",
        "# As with any object in Python, \n",
        "# assigning a tensor to a variable makes the variable a *label* of the tensor, \n",
        "# and does not copy it. \n",
        "# For example:\n",
        "################################################################################\n",
        "\n",
        "a = torch.ones(2, 2)\n",
        "b = a\n",
        "\n",
        "a[0][1] = 561  # we change a...\n",
        "print(b)       # ...and b is also altered\n",
        "\n",
        "################################################################################\n",
        "# But what if you want a separate copy of the data to work on? \n",
        "# The ``clone()`` method is there for you:\n",
        "################################################################################\n",
        "\n",
        "a = torch.ones(2, 2)\n",
        "b = a.clone()\n",
        "\n",
        "assert b is not a      # different objects in memory...\n",
        "print(torch.eq(a, b))  # ...but still with the same contents!\n",
        "\n",
        "a[0][1] = 561          # a changes...\n",
        "print(b)               # ...but b is still all ones\n",
        "\n",
        "################################################################################\n",
        "# **There is an important thing to be aware of when using ``clone()``.**\n",
        "# If your source tensor has autograd, enabled then so will the clone.\n",
        "# **This will be covered more deeply in the video on autograd,** \n",
        "# but if you want the light version of the details, continue on.\n",
        "# \n",
        "# *In many cases, this will be what you want.* \n",
        "# For example, if your model\n",
        "# has multiple computation paths in its ``forward()`` method, \n",
        "# and *both* the original tensor and its clone contribute to the model’s output, \n",
        "# then to enable model learning you want autograd turned on for both tensors.\n",
        "# If your source tensor has autograd enabled \n",
        "# (which it generally will if\n",
        "# it’s a set of learning weights or derived from a computation involving\n",
        "# the weights), \n",
        "# then you’ll get the result you want.\n",
        "# \n",
        "# On the other hand, if you’re doing a computation where *neither* the\n",
        "# original tensor nor its clone need to track gradients, then as long as\n",
        "# the source tensor has autograd turned off, you’re good to go.\n",
        "# \n",
        "# *There is a third case,* though: \n",
        "# Imagine you’re performing a computation\n",
        "# in your model’s ``forward()`` function, \n",
        "# where gradients are turned on\n",
        "# for everything by default, but you want to pull out some values\n",
        "# mid-stream to generate some metrics. \n",
        "# In this case, you *don’t* want the cloned copy of your source tensor \n",
        "# to track gradients - \n",
        "# performance is improved with autograd’s history tracking turned off. \n",
        "# For this, you can use the ``.detach()`` method on the source tensor:\n",
        "################################################################################\n",
        "\n",
        "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
        "print(a)\n",
        "\n",
        "b = a.clone()\n",
        "print(b)\n",
        "\n",
        "c = a.detach().clone()\n",
        "print(c)\n",
        "\n",
        "print(a)\n",
        "\n",
        "################################################################################\n",
        "# What’s happening here?\n",
        "# \n",
        "# -  We create ``a`` with ``requires_grad=True`` turned on. \n",
        "#    **We haven’t covered this optional argument yet, \n",
        "#    but will during the unit on autograd.**\n",
        "# -  When we print ``a``, \n",
        "#    it informs us that the property ``requires_grad=True`` - \n",
        "#    this means that autograd and computation history tracking are turned on.\n",
        "# -  We clone ``a`` and label it ``b``. \n",
        "#    When we print ``b``, \n",
        "#    we can see that it’s tracking its computation history - \n",
        "#    it has inherited ``a``\\ ’s autograd settings, \n",
        "#    and added to the computation history.\n",
        "# -  We clone ``a`` into ``c``, but we call ``detach()`` first.\n",
        "# -  Printing ``c``, \n",
        "#    we see no computation history, and no ``requires_grad=True``.\n",
        "# \n",
        "# The ``detach()`` method *detaches the tensor from its computation history.* \n",
        "# It says, “do whatever comes next as if autograd was off.” \n",
        "# It does this *without* changing ``a`` - \n",
        "# you can see that when we print ``a`` again at the end, \n",
        "# it retains its ``requires_grad=True`` property.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Moving to GPU\n",
        "# \n",
        "# One of the major advantages of PyTorch is its robust acceleration on\n",
        "# CUDA-compatible Nvidia GPUs. \n",
        "# (“CUDA” stands for *Compute Unified Device\n",
        "# Architecture*, which is Nvidia’s platform for parallel computing.) \n",
        "# So far, everything we’ve done has been on CPU. \n",
        "# How do we move to the faster hardware?\n",
        "# \n",
        "# First, we should check whether a GPU is available, \n",
        "# with the ``is_available()`` method.\n",
        "# \n",
        "# .. note::\n",
        "#      If you do not have a CUDA-compatible GPU and CUDA drivers installed, \n",
        "#      the executable cells in this section will not execute \n",
        "#      any GPU-related code.\n",
        "################################################################################ \n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('We have a GPU!')\n",
        "else:\n",
        "    print('Sorry, CPU only.')\n",
        "\n",
        "################################################################################\n",
        "# Once we’ve determined that one or more GPUs is available, \n",
        "# we need to put our data someplace where the GPU can see it. \n",
        "# Your CPU does computation on data in your computer’s RAM. \n",
        "# Your GPU has dedicated memory attached to it. \n",
        "# Whenever you want to perform a computation on a device, \n",
        "# you must move *all* the data needed for that computation \n",
        "# to memory accessible by that device. \n",
        "# (Colloquially, “moving the data to memory accessible by the\n",
        "# GPU” is shorted to, “moving the data to the GPU”.)\n",
        "# \n",
        "# There are multiple ways to get your data onto your target device. \n",
        "# You may do it at creation time:\n",
        "################################################################################\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
        "    print(gpu_rand)\n",
        "else:\n",
        "    print('Sorry, CPU only.')\n",
        "\n",
        "################################################################################\n",
        "# By default, new tensors are created on the CPU, \n",
        "# so we have to specify when we want to create our tensor on the GPU \n",
        "# with the optional ``device`` argument. \n",
        "# You can see when we print the new tensor, \n",
        "# PyTorch informs us which device it’s on (if it’s not on CPU).\n",
        "# \n",
        "# You can query the number of GPUs with ``torch.cuda.device_count()``. \n",
        "# If you have more than one GPU, you can specify them by index:\n",
        "# ``device='cuda:0'``, ``device='cuda:1'``, etc.\n",
        "# \n",
        "# As a coding practice, \n",
        "# specifying our devices everywhere with string constants is pretty fragile. \n",
        "# In an ideal world, your code would perform\n",
        "# robustly whether you’re on CPU or GPU hardware. \n",
        "# You can do this by creating a device handle \n",
        "# that can be passed to your tensors instead of a string:\n",
        "################################################################################ \n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    my_device = torch.device('cuda')\n",
        "else:\n",
        "    my_device = torch.device('cpu')\n",
        "print('Device: {}'.format(my_device))\n",
        "\n",
        "x = torch.rand(2, 2, device=my_device)\n",
        "print(x)\n",
        "\n",
        "################################################################################\n",
        "# If you have an existing tensor living on one device, \n",
        "# you can move it to another with the ``to()`` method. \n",
        "# The following line of code creates a tensor on CPU, \n",
        "# and moves it to whichever device handle you acquired in the previous cell.\n",
        "################################################################################\n",
        "\n",
        "y = torch.rand(2, 2)\n",
        "y = y.to(my_device)\n",
        "\n",
        "################################################################################\n",
        "# It is important to know that \n",
        "# in order to do computation involving two or more tensors, \n",
        "# *all of the tensors must be on the same device*. \n",
        "# The following code will throw a runtime error, \n",
        "# regardless of whether you have a GPU device available:\n",
        "# \n",
        "#    x = torch.rand(2, 2)\n",
        "#    y = torch.rand(2, 2, device='gpu')\n",
        "#    z = x + y  # exception will be thrown\n",
        "################################################################################ \n",
        "\n",
        "################################################################################\n",
        "# Manipulating Tensor Shapes\n",
        "# \n",
        "# Sometimes, you’ll need to change the shape of your tensor. \n",
        "# Below, we’ll look at a few common cases, and how to handle them.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Changing the Number of Dimensions\n",
        "# \n",
        "# One case where you might need to change the number of dimensions is\n",
        "# passing a single instance of input to your model. \n",
        "# PyTorch models generally expect *batches* of input.\n",
        "# \n",
        "# For example, imagine having a model that works on 3 x 226 x 226 images -\n",
        "# a 226-pixel square with 3 color channels. \n",
        "# When you load and transform it, \n",
        "# you’ll get a tensor of shape ``(3, 226, 226)``. \n",
        "# Your model, though, is expecting input of shape ``(N, 3, 226, 226)``, \n",
        "# where ``N`` is the number of images in the batch. \n",
        "# So how do you make a batch of one?\n",
        "################################################################################ \n",
        "\n",
        "a = torch.rand(3, 226, 226)\n",
        "b = a.unsqueeze(0)\n",
        "\n",
        "print(a.shape)\n",
        "print(b.shape)\n",
        "\n",
        "################################################################################\n",
        "# The ``unsqueeze()`` method adds a dimension of extent 1.\n",
        "# ``unsqueeze(0)`` adds it as a new zeroth dimension - \n",
        "# now you have a batch of one!\n",
        "# \n",
        "# So if that’s *un*\\ squeezing? \n",
        "# What do we mean by squeezing? \n",
        "# We’re taking\n",
        "# advantage of the fact that any dimension of extent 1 *does not* change\n",
        "# the number of elements in the tensor.\n",
        "################################################################################\n",
        "\n",
        "c = torch.rand(1, 1, 1, 1, 1)\n",
        "print(c)\n",
        "\n",
        "################################################################################\n",
        "# Continuing the example above, \n",
        "# let’s say the model’s output is a 20-element vector for each input. \n",
        "# You would then expect the output to have shape ``(N, 20)``, \n",
        "# where ``N`` is the number of instances in the input batch. \n",
        "# That means that for our single-input batch, \n",
        "# we’ll get an output of shape ``(1, 20)``.\n",
        "# \n",
        "# What if you want to do some *non-batched* computation with that output -\n",
        "# something that’s just expecting a 20-element vector?\n",
        "################################################################################\n",
        "\n",
        "a = torch.rand(1, 20)\n",
        "print(a.shape)\n",
        "print(a)\n",
        "\n",
        "b = a.squeeze(0)\n",
        "print(b.shape)\n",
        "print(b)\n",
        "\n",
        "c = torch.rand(2, 2)\n",
        "print(c.shape)\n",
        "\n",
        "d = c.squeeze(0)\n",
        "print(d.shape)\n",
        "\n",
        "################################################################################\n",
        "# You can see from the shapes that \n",
        "# our 2-dimensional tensor is now 1-dimensional, \n",
        "# and if you look closely at the output of the cell above\n",
        "# you’ll see that printing ``a`` shows an “extra” set of square brackets\n",
        "# ``[]`` due to having an extra dimension.\n",
        "# \n",
        "# You may only ``squeeze()`` dimensions of extent 1. \n",
        "# See above where we try to squeeze a dimension of size 2 in ``c``, \n",
        "# and get back the same shape we started with. \n",
        "# Calls to ``squeeze()`` and ``unsqueeze()`` can\n",
        "# only act on dimensions of extent 1 \n",
        "# because to do otherwise would change the number of elements in the tensor.\n",
        "# \n",
        "# Another place you might use ``unsqueeze()`` is to ease broadcasting.\n",
        "# Recall the example above where we had the following code:\n",
        "# \n",
        "#    a =     torch.ones(4, 3, 2)\n",
        "# \n",
        "#    c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
        "#    print(c)\n",
        "# \n",
        "# The net effect of that was \n",
        "# to broadcast the operation over dimensions 0 and 2, \n",
        "# causing the random, 3 x 1 tensor to be multiplied \n",
        "# element-wise by every 3-element column in ``a``.\n",
        "# \n",
        "# What if the random vector had just been 3-element vector? \n",
        "# We’d lose the ability to do the broadcast, \n",
        "# because the final dimensions would not \n",
        "# match up according to the broadcasting rules. \n",
        "# ``unsqueeze()`` comes to the rescue:\n",
        "################################################################################\n",
        "\n",
        "a = torch.ones(4, 3, 2)\n",
        "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
        "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
        "print(c.shape)\n",
        "print(a * c)             # broadcasting works again!\n",
        "\n",
        "################################################################################\n",
        "# The ``squeeze()`` and ``unsqueeze()`` methods also have in-place versions, \n",
        "# ``squeeze_()`` and ``unsqueeze_()``:\n",
        "################################################################################\n",
        "\n",
        "batch_me = torch.rand(3, 226, 226)\n",
        "print(batch_me.shape)\n",
        "batch_me.unsqueeze_(0)\n",
        "print(batch_me.shape)\n",
        "\n",
        "################################################################################\n",
        "# Sometimes you’ll want to change the shape of a tensor more radically,\n",
        "# while still preserving the number of elements and their contents. \n",
        "# One case where this happens is at the interface \n",
        "# between a convolutional layer of a model and a linear layer of the model - \n",
        "# this is common in image classification models. \n",
        "# A convolution kernel will yield an output tensor \n",
        "# of shape *features x width x height,* \n",
        "# but the following linear layer expects a 1-dimensional input. \n",
        "# ``reshape()`` will do this for you,\n",
        "# provided that the dimensions you request yield \n",
        "# the same number of elements as the input tensor has:\n",
        "################################################################################ \n",
        "\n",
        "output3d = torch.rand(6, 20, 20)\n",
        "print(output3d.shape)\n",
        "\n",
        "input1d = output3d.reshape(6 * 20 * 20)\n",
        "print(input1d.shape)\n",
        "\n",
        "# can also call it as a method on the torch module:\n",
        "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)\n",
        "\n",
        "################################################################################\n",
        "# .. note::\n",
        "#      The ``(6 * 20 * 20,)`` argument in the final line of the cell above \n",
        "#      is because PyTorch expects a **tuple** when specifying a tensor shape - \n",
        "#      but when the shape is the first argument of a method, \n",
        "#      it lets us cheat and just use a series of integers. \n",
        "#      Here, we had to add the parentheses and comma \n",
        "#      to convince the method that this is really a one-element tuple.\n",
        "# \n",
        "# When it can, ``reshape()`` will return a *view* on the tensor to be changed - \n",
        "# that is, a separate tensor object \n",
        "# looking at the same underlying region of memory.\n",
        "# *This is important:* \n",
        "# That means any change\n",
        "# made to the source tensor will be reflected in the view on that tensor,\n",
        "# unless you ``clone()`` it.\n",
        "# \n",
        "# There *are* conditions, beyond the scope of this introduction, where\n",
        "# ``reshape()`` has to return a tensor carrying a copy of the data. \n",
        "# For more information, see the\n",
        "# `docs <https://pytorch.org/docs/stable/torch.html#torch.reshape>`.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# NumPy Bridge\n",
        "# \n",
        "# In the section above on broadcasting, it was mentioned that  \n",
        "# PyTorch’s broadcast semantics are compatible with NumPy’s - \n",
        "# but the kinship between PyTorch and NumPy goes even deeper than that.\n",
        "# \n",
        "# If you have existing ML or scientific code with data stored in NumPy ndarrays, \n",
        "# you may wish to express that same data as PyTorch tensors,\n",
        "# whether to take advantage of PyTorch’s GPU acceleration, \n",
        "# or its efficient abstractions for building ML models. \n",
        "# It’s easy to switch between ndarrays and PyTorch tensors:\n",
        "################################################################################\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "numpy_array = np.ones((2, 3))\n",
        "print(numpy_array)\n",
        "\n",
        "pytorch_tensor = torch.from_numpy(numpy_array)\n",
        "print(pytorch_tensor)\n",
        "\n",
        "################################################################################\n",
        "# PyTorch creates a tensor of the same shape and containing the same data\n",
        "# as the NumPy array, \n",
        "# going so far as to keep NumPy’s default 64-bit float data type.\n",
        "# \n",
        "# The conversion can just as easily go the other way:\n",
        "################################################################################\n",
        "\n",
        "pytorch_rand = torch.rand(2, 3)\n",
        "print(pytorch_rand)\n",
        "\n",
        "numpy_rand = pytorch_rand.numpy()\n",
        "print(numpy_rand)\n",
        "\n",
        "################################################################################\n",
        "# It is important to know that \n",
        "# these converted objects are \n",
        "# using *the same underlying memory* as their source objects, \n",
        "# meaning that changes to one are reflected in the other:\n",
        "################################################################################\n",
        "\n",
        "numpy_array[1, 1] = 23\n",
        "print(pytorch_tensor)\n",
        "\n",
        "pytorch_rand[1, 1] = 17\n",
        "print(numpy_rand)"
      ],
      "metadata": {
        "id": "Cif8PTC_ohmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Tensors on both CPU and GPU [beginner_source/basics/tensorqs_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/tensorqs_tutorial.py) [한국어](https://www.youtube.com/watch?v=UZ5FRdYdh3g)"
      ],
      "metadata": {
        "id": "hANDWaR4FwnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# 1 https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/tensorqs_tutorial.py \n",
        "# 2 https://github.com/pytorch/tutorials/blob/main/beginner_source/blitz/tensor_tutorial.py (basically same as 1)\n",
        "\"\"\"\n",
        "Tensors\n",
        "\n",
        "Tensors are a specialized data structure \n",
        "that are very similar to arrays and matrices.\n",
        "In PyTorch, we use tensors to encode the inputs and outputs of a model, \n",
        "as well as the model’s parameters.\n",
        "\n",
        "Tensors are similar to NumPy’s ndarrays, \n",
        "except that tensors can run on \n",
        "GPUs or other specialized hardware to accelerate computing.\n",
        "In fact, tensors and NumPy arrays can often share the same underlying memory, \n",
        "eliminating the need to copy data (see :ref:`bridge-to-np-label`). \n",
        "Tensors are also optimized for automatic differentiation \n",
        "(we'll see more about that later in the `Autograd <autogradqs_tutorial.html>`). \n",
        "If you’re familiar with ndarrays, \n",
        "you’ll be right at home with the Tensor API. \n",
        "If not, follow along in this quick API walkthrough.!\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "################################################################################\n",
        "# Initializing a Tensor\n",
        "#\n",
        "# Tensors can be initialized in various ways. \n",
        "# Take a look at the following examples:\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# **Directly from data**\n",
        "#\n",
        "# Tensors can be created directly from data. \n",
        "# The data type is automatically inferred.\n",
        "################################################################################\n",
        "\n",
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "\n",
        "################################################################################\n",
        "# **From a NumPy array**\n",
        "#\n",
        "# Tensors can be created from NumPy arrays \n",
        "# (and vice versa - see :ref:`bridge-to-np-label`).\n",
        "################################################################################\n",
        "\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "\n",
        "################################################################################\n",
        "# **From another tensor:**\n",
        "#\n",
        "# The new tensor retains the properties (shape, datatype) \n",
        "# of the argument tensor, unless explicitly overridden.\n",
        "################################################################################\n",
        "\n",
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "################################################################################\n",
        "# **With random or constant values:**\n",
        "#\n",
        "# ``shape`` is a tuple of tensor dimensions. \n",
        "# In the functions below, it determines the dimensionality of the output tensor.\n",
        "################################################################################\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
        "\n",
        "################################################################################\n",
        "# Attributes of a Tensor\n",
        "#\n",
        "# Tensor attributes describe their shape, datatype, and \n",
        "# the device on which they are stored.\n",
        "################################################################################\n",
        "\n",
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "\n",
        "################################################################################\n",
        "# Operations on Tensors\n",
        "#\n",
        "# Over 100 tensor operations, including transposing, indexing, slicing,\n",
        "# mathematical operations, linear algebra, random sampling, and more are\n",
        "# comprehensively described `here <https://pytorch.org/docs/stable/torch.html>`.\n",
        "#\n",
        "# Each of these operations can be run on the GPU \n",
        "# (at typically higher speeds than on a CPU). \n",
        "# If you’re using Colab, \n",
        "# allocate a GPU by going to Runtime > Change runtime type > GPU.\n",
        "#\n",
        "# By default, tensors are created on the CPU. \n",
        "# We need to explicitly move tensors to the GPU using\n",
        "# ``.to`` method (after checking for GPU availability). \n",
        "# Keep in mind that copying large tensors\n",
        "# across devices can be expensive in terms of time and memory!\n",
        "################################################################################\n",
        "\n",
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to(\"cuda\")\n",
        "print(f\"Tensor is stored on: {tensor.device}\")\n",
        "\n",
        "################################################################################\n",
        "# Try out some of the operations from the list.\n",
        "# If you're familiar with the NumPy API, \n",
        "# you'll find the Tensor API a breeze to use.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# **Standard numpy-like indexing and slicing:**\n",
        "################################################################################\n",
        "\n",
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "print(tensor)\n",
        "\n",
        "################################################################################\n",
        "# **Joining tensors** \n",
        "#\n",
        "# You can use ``torch.cat`` \n",
        "# to concatenate a sequence of tensors along a given dimension.\n",
        "# See also \n",
        "# `torch.stack <https://pytorch.org/docs/stable/generated/torch.stack.html>`,\n",
        "# another tensor joining op that is subtly different from ``torch.cat``.\n",
        "################################################################################\n",
        "\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)\n",
        "\n",
        "################################################################################\n",
        "# **Arithmetic operations**\n",
        "################################################################################\n",
        "\n",
        "# This computes the matrix multiplication between two tensors. \n",
        "# y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)\n",
        "\n",
        "################################################################################\n",
        "# **Single-element tensors** \n",
        "#\n",
        "# If you have a one-element tensor, \n",
        "# for example by aggregating all values of a tensor into one value, \n",
        "# you can convert it to a Python numerical value using ``item()``:\n",
        "################################################################################\n",
        "\n",
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))\n",
        "\n",
        "################################################################################\n",
        "# **In-place operations**\n",
        "#\n",
        "# Operations that store the result into the operand are called in-place. \n",
        "# They are denoted by a ``_`` suffix.\n",
        "# For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n",
        "################################################################################\n",
        "\n",
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)\n",
        "\n",
        "################################################################################\n",
        "# note::\n",
        "# In-place operations save some memory, but can be problematic \n",
        "# when computing derivatives because of an immediate loss of history. \n",
        "# Hence, their use is discouraged.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Bridge with NumPy\n",
        "# \n",
        "# Tensors on the CPU and NumPy arrays \n",
        "# can share their underlying memory locations, and \n",
        "# changing one will change the other.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Tensor to NumPy array\n",
        "################################################################################\n",
        "\n",
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "################################################################################\n",
        "# A change in the tensor reflects in the NumPy array.\n",
        "################################################################################\n",
        "\n",
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "################################################################################\n",
        "# NumPy array to Tensor\n",
        "################################################################################\n",
        "\n",
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)\n",
        "\n",
        "################################################################################\n",
        "# Changes in the NumPy array reflects in the tensor.\n",
        "################################################################################\n",
        "\n",
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uub9QO5cFwxo",
        "outputId": "c90eb9e4-2d11-41d8-95cc-52588d3df7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.7619, 0.8111],\n",
            "        [0.2297, 0.7903]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.8389, 0.6357, 0.8340],\n",
            "        [0.6429, 0.6732, 0.8929]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n",
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
            "12.0 <class 'float'>\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n",
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n",
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Tensors support Auto Differentiation [beginner_source/basics/autogradqs_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/autogradqs_tutorial.py) [한국어](https://www.youtube.com/watch?v=TpVyrkhyt_A)"
      ],
      "metadata": {
        "id": "hv3kKzmmW1cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/autogradqs_tutorial.py\n",
        "\"\"\"\n",
        "Automatic Differentiation with ``torch.autograd``\n",
        "\n",
        "When training neural networks, \n",
        "the most frequently used algorithm is **back propagation**. \n",
        "In this algorithm, \n",
        "parameters (model weights) are adjusted according to \n",
        "the **gradient** of the loss function with respect to the given parameter.\n",
        "\n",
        "To compute those gradients, \n",
        "PyTorch has a built-in differentiation engine called ``torch.autograd``. \n",
        "It supports automatic computation of gradient for any computational graph.\n",
        "\n",
        "Consider the simplest one-layer neural network, \n",
        "with input ``x``, parameters ``w`` and ``b``, and some loss function. \n",
        "It can be defined in PyTorch in the following manner:\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "\n",
        "################################################################################\n",
        "# Tensors, Functions and Computational graph\n",
        "#\n",
        "# This code defines the following **computational graph**:\n",
        "#\n",
        "# .. figure:: /_static/img/basics/comp-graph.png\n",
        "#    :alt:\n",
        "#\n",
        "# In this network, \n",
        "# ``w`` and ``b`` are **parameters**, which we need to optimize. \n",
        "# Thus, we need to be able to compute the gradients of loss function \n",
        "# with respect to those variables. \n",
        "# In order to do that, we set the ``requires_grad`` property of those tensors.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# .. note:: You can set the value of ``requires_grad`` \n",
        "#           when creating a tensor, \n",
        "#           or later by using ``x.requires_grad_(True)`` method.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# A function that we apply to tensors to construct computational graph is\n",
        "# in fact an object of class ``Function``. \n",
        "# This object knows how to compute the function in the *forward* direction, \n",
        "# and also how to compute its derivative during the *backward propagation* step. \n",
        "# A reference to the backward propagation function is \n",
        "# stored in ``grad_fn`` property of a tensor. \n",
        "# You can find more information of ``Function`` `in the documentation \n",
        "# <https://pytorch.org/docs/stable/autograd.html#function>`.\n",
        "################################################################################\n",
        "\n",
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
        "\n",
        "################################################################################\n",
        "# Computing Gradients\n",
        "#\n",
        "# To optimize weights of parameters in the neural network, \n",
        "# we need to compute the derivatives of our loss function \n",
        "# with respect to parameters,\n",
        "# namely, we need :math:`\\frac{\\partial loss}{\\partial w}` and\n",
        "# :math:`\\frac{\\partial loss}{\\partial b}` \n",
        "# under some fixed values of ``x`` and ``y``. \n",
        "# To compute those derivatives, \n",
        "# we call ``loss.backward()``, and then \n",
        "# retrieve the values from ``w.grad`` and ``b.grad``:\n",
        "################################################################################\n",
        "\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "\n",
        "################################################################################\n",
        "# .. note::\n",
        "#   - We can only obtain the ``grad`` properties \n",
        "#     for the leaf nodes of the computational graph, \n",
        "#     which have ``requires_grad`` property set to ``True``. \n",
        "#     For all other nodes in our graph, gradients will not be available.\n",
        "#   - We can only perform gradient calculations \n",
        "#     using ``backward`` once on a given graph, for performance reasons. \n",
        "#     If we need to do several ``backward`` calls on the same graph, \n",
        "#     we need to pass ``retain_graph=True`` to the ``backward`` call.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Disabling Gradient Tracking\n",
        "#\n",
        "# By default, all tensors with ``requires_grad=True`` \n",
        "# are tracking their computational history and support gradient computation. \n",
        "# However, there are some cases when we do not need to do that, for example, \n",
        "# when we have trained the model and just want to apply it to some input data, \n",
        "# i.e. we only want to do *forward* computations through the network. \n",
        "# We can stop tracking computations by surrounding our computation code \n",
        "# with ``torch.no_grad()`` block:\n",
        "################################################################################\n",
        "\n",
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "################################################################################\n",
        "# Another way to achieve the same result is \n",
        "# to use the ``detach()`` method on the tensor:\n",
        "################################################################################\n",
        "\n",
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)\n",
        "\n",
        "################################################################################\n",
        "# There are reasons you might want to disable gradient tracking:\n",
        "#   - To mark some parameters in your neural network as **frozen parameters**. \n",
        "#     This is a very common scenario for\n",
        "#     `finetuning a pretrained network \n",
        "#     <https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html>`\n",
        "#   - To **speed up computations** when you are only doing forward pass, \n",
        "#     because computations on tensors \n",
        "#     that do not track gradients would be more efficient.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# More on Computational Graphs\n",
        "# \n",
        "# Conceptually, autograd keeps a record of data (tensors) and \n",
        "# all executed operations (along with the resulting new tensors) \n",
        "# in a directed acyclic graph (DAG) consisting of `Function \n",
        "# <https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`\n",
        "# objects. \n",
        "# In this DAG, leaves are the input tensors, roots are the output tensors. \n",
        "# By tracing this graph from roots to leaves, you can\n",
        "# automatically compute the gradients using the chain rule.\n",
        "#\n",
        "# In a forward pass, autograd does two things simultaneously:\n",
        "#\n",
        "# - run the requested operation to compute a resulting tensor\n",
        "# - maintain the operation’s *gradient function* in the DAG.\n",
        "#\n",
        "# The backward pass kicks off when ``.backward()`` is called on the DAG root. \n",
        "# ``autograd`` then:\n",
        "#\n",
        "# - computes the gradients from each ``.grad_fn``,\n",
        "# - accumulates them in the respective tensor’s ``.grad`` attribute\n",
        "# - using the chain rule, propagates all the way to the leaf tensors.\n",
        "#\n",
        "# .. note::\n",
        "#   **DAGs are dynamic in PyTorch**\n",
        "#   An important thing to note is that the graph is recreated from scratch; \n",
        "#   after each ``.backward()`` call, autograd starts populating a new graph. \n",
        "#   This is \n",
        "#   exactly what allows you to use control flow statements in your model;\n",
        "#   you can change the shape, size and operations at every iteration if needed.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Optional Reading: Tensor Gradients and Jacobian Products\n",
        "#\n",
        "# In many cases, we have a scalar loss function, and \n",
        "# we need to compute the gradient with respect to some parameters. \n",
        "# However, there are cases when the output function is an arbitrary tensor. \n",
        "# In this case, PyTorch allows you to compute so-called **Jacobian product**, \n",
        "# and not the actual gradient.\n",
        "#\n",
        "# For a vector function :math:`\\vec{y}=f(\\vec{x})`, where\n",
        "# :math:`\\vec{x}=\\langle x_1,\\dots,x_n\\rangle` and\n",
        "# :math:`\\vec{y}=\\langle y_1,\\dots,y_m\\rangle`, \n",
        "# a gradient of :math:`\\vec{y}` with respect to :math:`\\vec{x}` is given by \n",
        "# **Jacobian matrix**:\n",
        "#\n",
        "# .. math::\n",
        "#    J=\\left(\\begin{array}{ccc}\n",
        "#       \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "#       \\vdots & \\ddots & \\vdots\\\\\n",
        "#       \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "#       \\end{array}\\right)\n",
        "#\n",
        "# Instead of computing the Jacobian matrix itself, \n",
        "# PyTorch allows you to compute **Jacobian Product** :math:`v^T\\cdot J` \n",
        "# for a given input vector :math:`v=(v_1 \\dots v_m)`. \n",
        "# This is achieved by calling ``backward`` with :math:`v` as an argument. \n",
        "# The size of :math:`v` should be the same as the size of the original tensor, \n",
        "# with respect to which we want to compute the product:\n",
        "################################################################################\n",
        "\n",
        "inp = torch.eye(4, 5, requires_grad=True)\n",
        "out = (inp+1).pow(2).t()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"First call\\n{inp.grad}\")\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nSecond call\\n{inp.grad}\")\n",
        "inp.grad.zero_()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")\n",
        "\n",
        "################################################################################\n",
        "# Notice that when we call ``backward`` for the second time \n",
        "# with the same argument,\n",
        "# the value of the gradient is different. \n",
        "# This happens because when doing ``backward`` propagation, \n",
        "# PyTorch **accumulates the gradients**, i.e. \n",
        "# the value of computed gradients is added \n",
        "# to the ``grad`` property of all leaf nodes of computational graph. \n",
        "# If you want to compute the proper gradients, \n",
        "# you need to zero out the ``grad`` property before. \n",
        "# In real-life training an *optimizer* helps us to do this.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# .. note:: Previously we were calling ``backward()`` function \n",
        "#           without parameters. \n",
        "#           This is essentially equivalent to calling \n",
        "#           ``backward(torch.tensor(1.0))``, which is a useful way \n",
        "#           to compute the gradients in case of a scalar-valued function, \n",
        "#           such as loss during neural network training.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Further Reading\n",
        "# \n",
        "# `Autograd Mechanics <https://pytorch.org/docs/stable/notes/autograd.html>`\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "TEG1G2FlW1jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 2 Datasets & DataLoaders***"
      ],
      "metadata": {
        "id": "pyensvkbh4aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Datasets & DataLoaders [beginner_source/basics/data_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/data_tutorial.py)"
      ],
      "metadata": {
        "id": "E10tP-KdJScL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/data_tutorial.py\n",
        "\"\"\"\n",
        "Datasets & DataLoaders\n",
        "\"\"\"\n",
        "\n",
        "################################################################################\n",
        "# Code for processing data samples can get messy and hard to maintain; \n",
        "# we ideally want our dataset code\n",
        "# to be decoupled from our model training code \n",
        "# for better readability and modularity.\n",
        "# PyTorch provides two data primitives: \n",
        "# ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``\n",
        "# that allow you to use pre-loaded datasets as well as your own data.\n",
        "# ``Dataset`` stores the samples and their corresponding labels, and \n",
        "# ``DataLoader`` wraps an iterable around\n",
        "# the ``Dataset`` to enable easy access to the samples.\n",
        "#\n",
        "# PyTorch domain libraries provide a number of pre-loaded datasets \n",
        "# (such as FashionMNIST) that\n",
        "# subclass ``torch.utils.data.Dataset`` and implement functions specific \n",
        "# to the particular data.\n",
        "# They can be used to prototype and benchmark your model. \n",
        "# You can find them here: \n",
        "# `Image Datasets <https://pytorch.org/vision/stable/datasets.html>`,\n",
        "# `Text Datasets  <https://pytorch.org/text/stable/datasets.html>`, and\n",
        "# `Audio Datasets <https://pytorch.org/audio/stable/datasets.html>`\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Loading a Dataset\n",
        "#\n",
        "# Here is an example of how to load the \n",
        "# `Fashion-MNIST \n",
        "# <https://research.zalando.com/project/fashion_mnist/fashion_mnist/>`\n",
        "# dataset from TorchVision.\n",
        "# Fashion-MNIST is a dataset of Zalando’s article images consisting \n",
        "# of 60,000 training examples and 10,000 test examples.\n",
        "# Each example comprises a 28×28 grayscale image and \n",
        "# an associated label from one of 10 classes.\n",
        "#\n",
        "# We load the \n",
        "# `FashionMNIST Dataset \n",
        "# <https://pytorch.org/vision/stable/datasets.html#fashion-mnist>`\n",
        "# with the following parameters:\n",
        "#  - ``root`` is the path where the train/test data is stored,\n",
        "#  - ``train`` specifies training or test dataset,\n",
        "#  - ``download=True`` downloads the data from the internet \n",
        "#    if it's not available at ``root``.\n",
        "#  - ``transform`` and ``target_transform`` \n",
        "#    specify the feature and label transformations\n",
        "################################################################################\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "################################################################################\n",
        "# Iterating and Visualizing the Dataset\n",
        "#\n",
        "# We can index ``Datasets`` manually like a list: ``training_data[index]``.\n",
        "# We use ``matplotlib`` to visualize some samples in our training data.\n",
        "################################################################################\n",
        "\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "################################################################################\n",
        "# Creating a Custom Dataset for your files\n",
        "#\n",
        "# A custom Dataset class must implement three functions: \n",
        "# `__init__`, `__len__`, and `__getitem__`.\n",
        "# Take a look at this implementation; \n",
        "# the FashionMNIST images are stored in a directory ``img_dir``, and \n",
        "# their labels are stored separately in a CSV file ``annotations_file``.\n",
        "#\n",
        "# In the next sections, we'll break down what's happening \n",
        "# in each of these functions.\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "################################################################################\n",
        "# __init__\n",
        "#\n",
        "# The __init__ function is run once when instantiating the Dataset object. \n",
        "# We initialize\n",
        "# the directory containing the images, the annotations file, and \n",
        "# both transforms (covered in more detail in the next section).\n",
        "#\n",
        "# The labels.csv file looks like: ::\n",
        "#\n",
        "#     tshirt1.jpg, 0\n",
        "#     tshirt2.jpg, 0\n",
        "#     ......\n",
        "#     ankleboot999.jpg, 9\n",
        "################################################################################\n",
        "\n",
        "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# __len__\n",
        "#\n",
        "# The __len__ function returns the number of samples in our dataset.\n",
        "################################################################################\n",
        "\n",
        "def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "################################################################################\n",
        "# __getitem__\n",
        "#\n",
        "# The __getitem__ function loads and returns a sample \n",
        "# from the dataset at the given index ``idx``.\n",
        "# Based on the index, it identifies the image's location on disk, \n",
        "# converts that to a tensor using ``read_image``, retrieves the\n",
        "# corresponding label from the csv data in ``self.img_labels``, \n",
        "# calls the transform functions on them (if applicable), and returns the\n",
        "# tensor image and corresponding label in a tuple.\n",
        "################################################################################\n",
        "\n",
        "def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path)\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    return image, label\n",
        "\n",
        "################################################################################\n",
        "# Preparing your data for training with DataLoaders\n",
        "# \n",
        "# The ``Dataset`` retrieves our dataset's features and labels \n",
        "# one sample  at a time. \n",
        "# While training a model, we typically want to pass samples in \"minibatches\", \n",
        "# reshuffle the data at every epoch to reduce model overfitting,  \n",
        "# and use Python's ``multiprocessing`` to speed up data retrieval.\n",
        "#\n",
        "# ``DataLoader`` is an iterable \n",
        "# that abstracts this complexity for us in an easy API.\n",
        "################################################################################\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "################################################################################\n",
        "# Iterate through the DataLoader\n",
        "#\n",
        "# We have loaded that dataset into the ``DataLoader`` and \n",
        "# can iterate through the dataset as needed.\n",
        "# Each iteration below returns \n",
        "# a batch of ``train_features`` and ``train_labels`` \n",
        "# (containing ``batch_size=64`` features and labels respectively).\n",
        "# Because we specified ``shuffle=True``, \n",
        "# after we iterate over all batches the data is shuffled \n",
        "# (for finer-grained control over the data loading order, take a look at \n",
        "# `Samplers \n",
        "# <https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler>`).\n",
        "################################################################################\n",
        "\n",
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")\n",
        "\n",
        "################################################################################\n",
        "# Further Reading\n",
        "# \n",
        "# - `torch.utils.data API <https://pytorch.org/docs/stable/data.html>`\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "L4QnEDFGJSmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Transforms [beginner_source/basics/transforms_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/transforms_tutorial.py)"
      ],
      "metadata": {
        "id": "GuKtnPK_VIS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/transforms_tutorial.py\n",
        "\"\"\"\n",
        "Transforms\n",
        "===================\n",
        "\n",
        "Data does not always come in its final processed form \n",
        "that is required for training machine learning algorithms. \n",
        "We use **transforms** to perform some\n",
        "manipulation of the data and make it suitable for training.\n",
        "\n",
        "All TorchVision datasets have two parameters -\n",
        "``transform`` to modify the features and\n",
        "``target_transform`` to modify the labels - \n",
        "that accept callables containing the transformation logic.\n",
        "The `torchvision.transforms <https://pytorch.org/vision/stable/transforms.html>`\n",
        "module offers\n",
        "several commonly-used transforms out of the box.\n",
        "\n",
        "The FashionMNIST features are in PIL Image format, and the labels are integers.\n",
        "For training, \n",
        "we need the features as normalized tensors, and \n",
        "the labels as one-hot encoded tensors.\n",
        "To make these transformations, we use ``ToTensor`` and ``Lambda``.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    # https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor\n",
        "    # CLASS torchvision.transforms.ToTensor[SOURCE]\n",
        "    # Convert a PIL Image or numpy.ndarray to tensor. \n",
        "    # This transform does not support torchscript.\n",
        "    # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] \n",
        "    # to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \n",
        "    # if the PIL Image belongs to \n",
        "    # one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or \n",
        "    # if the numpy.ndarray has dtype = np.uint8\n",
        "    # In the other cases, tensors are returned without scaling.\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")\n",
        "\n",
        "################################################################################\n",
        "# ToTensor()\n",
        "#\n",
        "# `ToTensor \n",
        "# <https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor>`\n",
        "# converts a PIL image or NumPy ``ndarray`` into a ``FloatTensor``. and scales\n",
        "# the image's pixel intensity values in the range [0., 1.]\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Lambda Transforms\n",
        "#\n",
        "# Lambda transforms apply any user-defined lambda function. \n",
        "# Here, we define a function\n",
        "# to turn the integer into a one-hot encoded tensor.\n",
        "# It first creates a zero tensor \n",
        "# of size 10 (the number of labels in our dataset) and calls\n",
        "# `scatter_ \n",
        "# <https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html>` \n",
        "# which assigns a\n",
        "# ``value=1`` on the index as given by the label ``y``.\n",
        "################################################################################\n",
        "\n",
        "target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
        "\n",
        "################################################################################\n",
        "# Further Reading\n",
        "# \n",
        "# - `torchvision.transforms API \n",
        "#   <https://pytorch.org/vision/stable/transforms.html>`\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "w9Oc_cssVIfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Writing Custom Datasets, DataLoaders and Transforms [beginner_source/data_loading_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/data_loading_tutorial.py)"
      ],
      "metadata": {
        "id": "RNL7O-h8yIxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Writing Custom Datasets, DataLoaders and Transforms\n",
        "===================================================\n",
        "**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_\n",
        "\n",
        "A lot of effort in solving any machine learning problem goes into\n",
        "preparing the data. PyTorch provides many tools to make data loading\n",
        "easy and hopefully, to make your code more readable. In this tutorial,\n",
        "we will see how to load and preprocess/augment data from a non trivial\n",
        "dataset.\n",
        "\n",
        "To run this tutorial, please make sure the following packages are\n",
        "installed:\n",
        "\n",
        "-  ``scikit-image``: For image io and transforms\n",
        "-  ``pandas``: For easier csv parsing\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "######################################################################\n",
        "# The dataset we are going to deal with is that of facial pose.\n",
        "# This means that a face is annotated like this:\n",
        "#\n",
        "# .. figure:: /_static/img/landmarked_face2.png\n",
        "#    :width: 400\n",
        "#\n",
        "# Over all, 68 different landmark points are annotated for each face.\n",
        "#\n",
        "# .. note::\n",
        "#     Download the dataset from `here <https://download.pytorch.org/tutorial/faces.zip>`_\n",
        "#     so that the images are in a directory named 'data/faces/'.\n",
        "#     This dataset was actually\n",
        "#     generated by applying excellent `dlib's pose\n",
        "#     estimation <https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`__\n",
        "#     on a few images from imagenet tagged as 'face'.\n",
        "#\n",
        "# Dataset comes with a csv file with annotations which looks like this:\n",
        "#\n",
        "# ::\n",
        "#\n",
        "#     image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n",
        "#     0805personali01.jpg,27,83,27,98, ... 84,134\n",
        "#     1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312\n",
        "#\n",
        "# Let's take a single image name and its annotations from the CSV, in this case row index number 65\n",
        "# for person-7.jpg just as an example. Read it, store the image name in ``img_name`` and store its\n",
        "# annotations in an (L, 2) array ``landmarks`` where L is the number of landmarks in that row.\n",
        "#\n",
        "\n",
        "landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n",
        "\n",
        "n = 65\n",
        "img_name = landmarks_frame.iloc[n, 0]\n",
        "landmarks = landmarks_frame.iloc[n, 1:]\n",
        "landmarks = np.asarray(landmarks)\n",
        "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "\n",
        "print('Image name: {}'.format(img_name))\n",
        "print('Landmarks shape: {}'.format(landmarks.shape))\n",
        "print('First 4 Landmarks: {}'.format(landmarks[:4]))\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Let's write a simple helper function to show an image and its landmarks\n",
        "# and use it to show a sample.\n",
        "#\n",
        "\n",
        "def show_landmarks(image, landmarks):\n",
        "    \"\"\"Show image with landmarks\"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "plt.figure()\n",
        "show_landmarks(io.imread(os.path.join('data/faces/', img_name)),\n",
        "               landmarks)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Dataset class\n",
        "# -------------\n",
        "#\n",
        "# ``torch.utils.data.Dataset`` is an abstract class representing a\n",
        "# dataset.\n",
        "# Your custom dataset should inherit ``Dataset`` and override the following\n",
        "# methods:\n",
        "#\n",
        "# -  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
        "# -  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
        "#    be used to get :math:`i`\\ th sample.\n",
        "#\n",
        "# Let's create a dataset class for our face landmarks dataset. We will\n",
        "# read the csv in ``__init__`` but leave the reading of images to\n",
        "# ``__getitem__``. This is memory efficient because all the images are not\n",
        "# stored in the memory at once but read as required.\n",
        "#\n",
        "# Sample of our dataset will be a dict\n",
        "# ``{'image': image, 'landmarks': landmarks}``. Our dataset will take an\n",
        "# optional argument ``transform`` so that any required processing can be\n",
        "# applied on the sample. We will see the usefulness of ``transform`` in the\n",
        "# next section.\n",
        "#\n",
        "\n",
        "class FaceLandmarksDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks])\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Let's instantiate this class and iterate through the data samples. We\n",
        "# will print the sizes of first 4 samples and show their landmarks.\n",
        "#\n",
        "\n",
        "face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
        "                                    root_dir='data/faces/')\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "for i in range(len(face_dataset)):\n",
        "    sample = face_dataset[i]\n",
        "\n",
        "    print(i, sample['image'].shape, sample['landmarks'].shape)\n",
        "\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title('Sample #{}'.format(i))\n",
        "    ax.axis('off')\n",
        "    show_landmarks(**sample)\n",
        "\n",
        "    if i == 3:\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Transforms\n",
        "# ----------\n",
        "#\n",
        "# One issue we can see from the above is that the samples are not of the\n",
        "# same size. Most neural networks expect the images of a fixed size.\n",
        "# Therefore, we will need to write some preprocessing code.\n",
        "# Let's create three transforms:\n",
        "#\n",
        "# -  ``Rescale``: to scale the image\n",
        "# -  ``RandomCrop``: to crop from image randomly. This is data\n",
        "#    augmentation.\n",
        "# -  ``ToTensor``: to convert the numpy images to torch images (we need to\n",
        "#    swap axes).\n",
        "#\n",
        "# We will write them as callable classes instead of simple functions so\n",
        "# that parameters of the transform need not be passed everytime it's\n",
        "# called. For this, we just need to implement ``__call__`` method and\n",
        "# if required, ``__init__`` method. We can then use a transform like this:\n",
        "#\n",
        "# ::\n",
        "#\n",
        "#     tsfm = Transform(params)\n",
        "#     transformed_sample = tsfm(sample)\n",
        "#\n",
        "# Observe below how these transforms had to be applied both on the image and\n",
        "# landmarks.\n",
        "#\n",
        "\n",
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "\n",
        "        # h and w are swapped for landmarks because for images,\n",
        "        # x and y axes are axis 1 and 0 respectively\n",
        "        landmarks = landmarks * [new_w / w, new_h / h]\n",
        "\n",
        "        return {'image': img, 'landmarks': landmarks}\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        top = np.random.randint(0, h - new_h)\n",
        "        left = np.random.randint(0, w - new_w)\n",
        "\n",
        "        image = image[top: top + new_h,\n",
        "                      left: left + new_w]\n",
        "\n",
        "        landmarks = landmarks - [left, top]\n",
        "\n",
        "        return {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'landmarks': torch.from_numpy(landmarks)}\n",
        "    \n",
        "######################################################################\n",
        "# .. note::\n",
        "#     In the example above, `RandomCrop` uses an external library's random number generator \n",
        "#     (in this case, Numpy's `np.random.int`). This can result in unexpected behavior with `DataLoader` \n",
        "#     (see https://pytorch.org/docs/stable/notes/faq.html#my-data-loader-workers-return-identical-random-numbers). \n",
        "#     In practice, it is safer to stick to PyTorch's random number generator, e.g. by using `torch.randint` instead.\n",
        "\n",
        "######################################################################\n",
        "# Compose transforms\n",
        "# ~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# Now, we apply the transforms on a sample.\n",
        "#\n",
        "# Let's say we want to rescale the shorter side of the image to 256 and\n",
        "# then randomly crop a square of size 224 from it. i.e, we want to compose\n",
        "# ``Rescale`` and ``RandomCrop`` transforms.\n",
        "# ``torchvision.transforms.Compose`` is a simple callable class which allows us\n",
        "# to do this.\n",
        "#\n",
        "\n",
        "scale = Rescale(256)\n",
        "crop = RandomCrop(128)\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "\n",
        "# Apply each of the above transforms on sample.\n",
        "fig = plt.figure()\n",
        "sample = face_dataset[65]\n",
        "for i, tsfrm in enumerate([scale, crop, composed]):\n",
        "    transformed_sample = tsfrm(sample)\n",
        "\n",
        "    ax = plt.subplot(1, 3, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title(type(tsfrm).__name__)\n",
        "    show_landmarks(**transformed_sample)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Iterating through the dataset\n",
        "# -----------------------------\n",
        "#\n",
        "# Let's put this all together to create a dataset with composed\n",
        "# transforms.\n",
        "# To summarize, every time this dataset is sampled:\n",
        "#\n",
        "# -  An image is read from the file on the fly\n",
        "# -  Transforms are applied on the read image\n",
        "# -  Since one of the transforms is random, data is augmented on\n",
        "#    sampling\n",
        "#\n",
        "# We can iterate over the created dataset with a ``for i in range``\n",
        "# loop as before.\n",
        "#\n",
        "\n",
        "transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
        "                                           root_dir='data/faces/',\n",
        "                                           transform=transforms.Compose([\n",
        "                                               Rescale(256),\n",
        "                                               RandomCrop(224),\n",
        "                                               ToTensor()\n",
        "                                           ]))\n",
        "\n",
        "for i in range(len(transformed_dataset)):\n",
        "    sample = transformed_dataset[i]\n",
        "\n",
        "    print(i, sample['image'].size(), sample['landmarks'].size())\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# However, we are losing a lot of features by using a simple ``for`` loop to\n",
        "# iterate over the data. In particular, we are missing out on:\n",
        "#\n",
        "# -  Batching the data\n",
        "# -  Shuffling the data\n",
        "# -  Load the data in parallel using ``multiprocessing`` workers.\n",
        "#\n",
        "# ``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
        "# features. Parameters used below should be clear. One parameter of\n",
        "# interest is ``collate_fn``. You can specify how exactly the samples need\n",
        "# to be batched using ``collate_fn``. However, default collate should work\n",
        "# fine for most use cases.\n",
        "#\n",
        "\n",
        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "# Helper function to show a batch\n",
        "def show_landmarks_batch(sample_batched):\n",
        "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
        "    images_batch, landmarks_batch = \\\n",
        "            sample_batched['image'], sample_batched['landmarks']\n",
        "    batch_size = len(images_batch)\n",
        "    im_size = images_batch.size(2)\n",
        "    grid_border_size = 2\n",
        "\n",
        "    grid = utils.make_grid(images_batch)\n",
        "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,\n",
        "                    landmarks_batch[i, :, 1].numpy() + grid_border_size,\n",
        "                    s=10, marker='.', c='r')\n",
        "\n",
        "        plt.title('Batch from dataloader')\n",
        "\n",
        "# if you are using Windows, uncomment the next line and indent the for loop.\n",
        "# you might need to go back and change \"num_workers\" to 0. \n",
        "\n",
        "# if __name__ == '__main__':\n",
        "for i_batch, sample_batched in enumerate(dataloader):\n",
        "    print(i_batch, sample_batched['image'].size(),\n",
        "          sample_batched['landmarks'].size())\n",
        "\n",
        "    # observe 4th batch and stop.\n",
        "    if i_batch == 3:\n",
        "        plt.figure()\n",
        "        show_landmarks_batch(sample_batched)\n",
        "        plt.axis('off')\n",
        "        plt.ioff()\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "######################################################################\n",
        "# Afterword: torchvision\n",
        "# ----------------------\n",
        "#\n",
        "# In this tutorial, we have seen how to write and use datasets, transforms\n",
        "# and dataloader. ``torchvision`` package provides some common datasets and\n",
        "# transforms. You might not even have to write custom classes. One of the\n",
        "# more generic datasets available in torchvision is ``ImageFolder``.\n",
        "# It assumes that images are organized in the following way: ::\n",
        "#\n",
        "#     root/ants/xxx.png\n",
        "#     root/ants/xxy.jpeg\n",
        "#     root/ants/xxz.png\n",
        "#     .\n",
        "#     .\n",
        "#     .\n",
        "#     root/bees/123.jpg\n",
        "#     root/bees/nsdf3.png\n",
        "#     root/bees/asd932_.png\n",
        "#\n",
        "# where 'ants', 'bees' etc. are class labels. Similarly generic transforms\n",
        "# which operate on ``PIL.Image`` like  ``RandomHorizontalFlip``, ``Scale``,\n",
        "# are also available. You can use these to write a dataloader like this: ::\n",
        "#\n",
        "#   import torch\n",
        "#   from torchvision import transforms, datasets\n",
        "#\n",
        "#   data_transform = transforms.Compose([\n",
        "#           transforms.RandomSizedCrop(224),\n",
        "#           transforms.RandomHorizontalFlip(),\n",
        "#           transforms.ToTensor(),\n",
        "#           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                std=[0.229, 0.224, 0.225])\n",
        "#       ])\n",
        "#   hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train',\n",
        "#                                              transform=data_transform)\n",
        "#   dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,\n",
        "#                                                batch_size=4, shuffle=True,\n",
        "#                                                num_workers=4)\n",
        "#\n",
        "# For an example with training code, please see\n",
        "# :doc:`transfer_learning_tutorial`."
      ],
      "metadata": {
        "id": "_et5AjFUyI9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 3 Gradient Descent***"
      ],
      "metadata": {
        "id": "OFaydrarvCFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Gradient Descent [한국어](https://youtu.be/9a4rIVRL9eA)"
      ],
      "metadata": {
        "id": "XepwuIUxvqed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\theta_{n+1}=\\theta_n-\\lambda \\frac{\\partial l}{\\partial\\theta}\n",
        "$$"
      ],
      "metadata": {
        "id": "BTmSDEtfvUHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "lr = 2e-2\n",
        "epoch = int(1e2)\n",
        "theta = 1.2\n",
        "\n",
        "compute_loss = lambda theta : theta**2\n",
        "compute_gradient = lambda theta : 2*theta\n",
        "apply_gradient_descent = lambda theta, grad, lr: theta - lr * grad\n",
        "\n",
        "theta_trace = [theta]\n",
        "loss_trace = [compute_loss(theta)]\n",
        "for i in range(epoch): \n",
        "    grad = compute_gradient(theta_trace[-1])\n",
        "    theta = apply_gradient_descent(theta_trace[-1], grad, lr)\n",
        "    theta_trace.append(theta)\n",
        "    loss_trace.append(compute_loss(theta))  \n",
        "    \n",
        "fig, (ax0, ax1) = plt.subplots(1,2,figsize=(12,3))\n",
        "\n",
        "theta_fig = np.linspace(-1.5, 1.5)\n",
        "loss_fig = compute_loss(theta_fig)\n",
        "ax0.plot(theta_fig,loss_fig)\n",
        "ax0.plot(theta_trace,loss_trace,'--*r',label='parameter')\n",
        "\n",
        "ax1.plot(loss_trace,'-*',label=\"loss\")\n",
        "\n",
        "for ax in (ax0, ax1):\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "7e981ef2-b7b1-4901-c71a-31c1351f0535",
        "id": "UmbWC7UVvjW_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Polynomial Fit to Sine Curve (Numpy Implementation) [beginner_source/examples_tensor/polynomial_numpy.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_tensor/polynomial_numpy.py)"
      ],
      "metadata": {
        "id": "RX6rJ79RlJNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_tensor/polynomial_numpy.py\n",
        "\"\"\"\n",
        "Warm-up: numpy\n",
        "--------------\n",
        "\n",
        "A third order polynomial, trained to predict :math:`y=\\sin(x)` from :math:`-\\pi`\n",
        "to :math:`pi` by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses numpy to manually compute the forward pass, loss, and\n",
        "backward pass.\n",
        "\n",
        "A numpy array is a generic n-dimensional array; it does not know anything about\n",
        "deep learning or gradients or computational graphs, and is just a way to perform\n",
        "generic numeric computations.\n",
        "\"\"\"\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np; np.random.seed(0)\n",
        "\n",
        "# Create random input and output data\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "y = np.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n",
        "\n",
        "y_pred = a + b * x + c * x**2 + d * x**3\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,3))\n",
        "ax.plot(x,y,\"-k\",label=\"original\")\n",
        "ax.plot(x,y_pred,\"--r\",label=\"predicted\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rGubeEdllJWA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "4b1d8e8f-8a14-4bb1-9bd3-1cbe9f4f51a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1942.5981197543472\n",
            "199 1357.8620988723726\n",
            "299 950.6459714997519\n",
            "399 666.8234472515073\n",
            "499 468.84881983428795\n",
            "599 330.65138961169055\n",
            "699 234.11186628462443\n",
            "799 166.62610469569785\n",
            "899 119.4189117419541\n",
            "999 86.37582610337255\n",
            "1099 63.23296761101651\n",
            "1199 47.01467134962145\n",
            "1299 35.642761530587755\n",
            "1399 27.664836288357098\n",
            "1499 22.065150545580426\n",
            "1599 18.132878187057905\n",
            "1699 15.370270373477139\n",
            "1799 13.428577985505335\n",
            "1899 12.06331164944789\n",
            "1999 11.10298121439451\n",
            "Result: y = 0.04878583756373356 + 0.8443864408312384 x + -0.008416368779585175 x^2 + -0.09157308290467793 x^3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Polynomial Fit to Sine Curve (PyTorch Implementation) [beginner_source/examples_tensor/polynomial_tensor.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_tensor/polynomial_tensor.py)"
      ],
      "metadata": {
        "id": "xgO6UO3VlJfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_tensor/polynomial_tensor.py\n",
        "\"\"\"\n",
        "PyTorch: Tensors\n",
        "----------------\n",
        "\n",
        "A third order polynomial, trained to predict :math:`y=\\sin(x)` from :math:`-\\pi`\n",
        "to :math:`pi` by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass,\n",
        "loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know\n",
        "anything about deep learning or computational graphs or gradients, and is just\n",
        "a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that\n",
        "a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,\n",
        "just cast the Tensor to a cuda datatype.\n",
        "\"\"\"\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch; torch.manual_seed(0)\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
        "\n",
        "y_pred = a + b * x + c * x**2 + d * x**3\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,3))\n",
        "ax.plot(x,y,\"-k\",label=\"original\")\n",
        "ax.plot(x,y_pred,\"--r\",label=\"predicted\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mZe9EXmplJoA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "149d6cfe-df31-4b40-b7ea-caa3307a71df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 3238.300537109375\n",
            "199 2245.867431640625\n",
            "299 1559.936279296875\n",
            "399 1085.354248046875\n",
            "499 756.6680908203125\n",
            "599 528.8026733398438\n",
            "699 370.6806335449219\n",
            "799 260.8537292480469\n",
            "899 184.50198364257812\n",
            "999 131.3756866455078\n",
            "1099 94.37876892089844\n",
            "1199 68.59319305419922\n",
            "1299 50.60741424560547\n",
            "1399 38.052650451660156\n",
            "1499 29.282575607299805\n",
            "1599 23.151979446411133\n",
            "1699 18.863677978515625\n",
            "1799 15.862088203430176\n",
            "1899 13.759873390197754\n",
            "1999 12.286673545837402\n",
            "Result: y = 0.058623336255550385 + 0.8372474908828735 x + -0.01011350192129612 x^2 + -0.09055762737989426 x^3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Polynomial Fit to Sine Curve (Linear Layer) [beginner_source/examples_nn/polynomial_nn.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/polynomial_nn.py)"
      ],
      "metadata": {
        "id": "YB-StOuYkAMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PyTorch: nn\n",
        "-----------\n",
        "\n",
        "A third order polynomial, trained to predict :math:`y=\\sin(x)` from :math:`-\\pi`\n",
        "to :math:`pi` by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses the nn package from PyTorch to build the network.\n",
        "PyTorch autograd makes it easy to define computational graphs and take gradients,\n",
        "but raw autograd can be a bit too low-level for defining complex neural networks;\n",
        "this is where the nn package can help. The nn package defines a set of Modules,\n",
        "which you can think of as a neural network layer that produces output from\n",
        "input and may have some trainable weights.\n",
        "\"\"\"\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
        "# we can consider it as a linear layer neural network. Let's prepare the\n",
        "# tensor (x, x^2, x^3).\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
        "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
        "# of shape (2000, 3) \n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. The Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
        "# to match the shape of `y`.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "# You can access the first layer of `model` like accessing the first item of a list\n",
        "linear_layer = model[0]\n",
        "\n",
        "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "id": "vARYrk9SkAVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Polynomial Fit to Sine Curve (Custom Module) [beginner_source/examples_nn/polynomial_module.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/polynomial_module.py)"
      ],
      "metadata": {
        "id": "AMYUBmOwj_8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PyTorch: Custom nn Modules\n",
        "--------------------------\n",
        "\n",
        "A third order polynomial, trained to predict :math:`y=\\sin(x)` from :math:`-\\pi`\n",
        "to :math:`\\pi` by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation defines the model as a custom Module subclass. Whenever you\n",
        "want a model more complex than a simple sequence of existing Modules you will\n",
        "need to define your model this way.\n",
        "\"\"\"\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class Polynomial3(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate four parameters and assign them as\n",
        "        member parameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = Polynomial3()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters (defined \n",
        "# with torch.nn.Parameter) which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
        "for t in range(2000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "metadata": {
        "id": "3PXAY0bykAEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 4 Autograd***"
      ],
      "metadata": {
        "id": "uWWc9z9_xQHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Fundamentals of Autograd [beginner_source/introyt/autogradyt_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/autogradyt_tutorial.py) [english](https://www.youtube.com/watch?v=M0fX15_-xrY)"
      ],
      "metadata": {
        "id": "StAOraZ3no8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/autogradyt_tutorial.py\n",
        "\"\"\"\n",
        "The Fundamentals of Autograd\n",
        "============================\n",
        "\n",
        "Follow along with the video below or on \n",
        "`youtube <https://www.youtube.com/watch?v=M0fX15_-xrY>`.\n",
        "\n",
        "PyTorch’s *Autograd* feature is part of what make PyTorch flexible and\n",
        "fast for building machine learning projects. \n",
        "It allows for the rapid and\n",
        "easy computation of multiple partial derivatives (also referred to as\n",
        "*gradients)* over a complex computation. \n",
        "This operation is central to backpropagation-based neural network learning.\n",
        "\n",
        "The power of autograd comes from the fact that it traces your\n",
        "computation dynamically *at runtime,* meaning that if your model has\n",
        "decision branches, or loops whose lengths are not known until runtime,\n",
        "the computation will still be traced correctly, and you’ll get correct\n",
        "gradients to drive learning. \n",
        "This, combined with the fact that your\n",
        "models are built in Python, offers far more flexibility than frameworks\n",
        "that rely on static analysis of a more rigidly-structured model for\n",
        "computing gradients.\n",
        "\n",
        "What Do We Need Autograd For?\n",
        "-----------------------------\n",
        "\"\"\"\n",
        "\n",
        "################################################################################\n",
        "# A machine learning model is a *function*, with inputs and outputs. \n",
        "# For this discussion, we’ll treat the inputs as an *i*-dimensional vector\n",
        "# :math:`\\vec{x}`, with elements :math:`x_{i}`. \n",
        "# We can then express the model, *M*, \n",
        "# as a vector-valued function of the input: :math:`\\vec{y} = \\vec{M}(\\vec{x})`. \n",
        "# (We treat the value of M’s output as\n",
        "# a vector because in general, a model may have any number of outputs.)\n",
        "#\n",
        "# Since we’ll mostly be discussing autograd in the context of training,\n",
        "# our output of interest will be the model’s loss. \n",
        "# The *loss function* L(:math:`\\vec{y}`) = L(:math:`\\vec{M}`\\ (:math:`\\vec{x}`)) \n",
        "# is a single-valued scalar function of the model’s output. \n",
        "# This function expresses how far off our model’s prediction was \n",
        "# from a particular input’s *ideal* output. \n",
        "# *Note: After this point, we will often omit the vector sign \n",
        "# where it should be contextually clear - e.g.,* :math:`y`\n",
        "# instead of :math:`\\vec y`.\n",
        "#\n",
        "# In training a model, we want to minimize the loss. \n",
        "# In the idealized case of a perfect model, \n",
        "# that means adjusting its learning weights - that is,\n",
        "# the adjustable parameters of the function - \n",
        "# such that loss is zero for all inputs. \n",
        "# In the real world, it means an iterative process of \n",
        "# nudging the learning weights  \n",
        "# until we see that we get a tolerable loss for a wide variety of inputs.\n",
        "#\n",
        "# How do we decide how far and in which direction to nudge the weights? \n",
        "# We want to *minimize* the loss, which means making its first derivative\n",
        "# with respect to the input equal to 0:\n",
        "# :math:`\\frac{\\partial L}{\\partial x} = 0`.\n",
        "#\n",
        "# Recall, though, that the loss is not *directly* derived from the input,\n",
        "# but a function of the model’s output \n",
        "# (which is a function of the input directly), \n",
        "# :math:`\\frac{\\partial L}{\\partial x}` =\n",
        "# :math:`\\frac{\\partial {L({\\vec y})}}{\\partial x}`. \n",
        "# By the chain rule of differential calculus, we have\n",
        "# :math:`\\frac{\\partial {L({\\vec y})}}{\\partial x}` =\n",
        "# :math:`\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}` =\n",
        "# :math:`\\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}`.\n",
        "#\n",
        "# :math:`\\frac{\\partial M(x)}{\\partial x}` is where things get complex.\n",
        "# The partial derivatives of the model’s outputs \n",
        "# with respect to its inputs, \n",
        "# if we were to expand the expression using the chain rule again,\n",
        "# would involve many local partial derivatives \n",
        "# over every multiplied learning weight, every activation function, \n",
        "# and every other mathematical transformation in the model. \n",
        "# The full expression for each such partial derivative \n",
        "# is the sum of the products of the local gradient of *every\n",
        "# possible path* through the computation graph \n",
        "# that ends with the variable whose gradient we are trying to measure.\n",
        "#\n",
        "# In particular, \n",
        "# the gradients over the learning weights are of interest to us - \n",
        "# they tell us *what direction to change each weight* \n",
        "# to get the loss function closer to zero.\n",
        "#\n",
        "# Since the number of such local derivatives (each corresponding to a\n",
        "# separate path through the model’s computation graph) will tend to go up\n",
        "# exponentially with the depth of a neural network, \n",
        "# so does the complexity in computing them. \n",
        "# This is where autograd comes in: \n",
        "# It tracks the history of every computation. \n",
        "# Every computed tensor in your PyTorch model \n",
        "# carries a history of its input tensors and the function used to create it. \n",
        "# Combined with the fact that PyTorch functions meant to act on tensors \n",
        "# each have a built-in implementation for computing their own derivatives, \n",
        "# this greatly speeds the computation of \n",
        "# the local derivatives needed for learning.\n",
        "#\n",
        "# A Simple Example\n",
        "# ----------------\n",
        "#\n",
        "# That was a lot of theory - \n",
        "# but what does it look like to use autograd in practice?\n",
        "#\n",
        "# Let’s start with a straightforward example. \n",
        "# First, we’ll do some imports to let us graph our results:\n",
        "################################################################################\n",
        "\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import math\n",
        "\n",
        "################################################################################\n",
        "# Next, we’ll create \n",
        "# an input tensor full of evenly spaced values on the interval \n",
        "# :math:`[0, 2{\\pi}]`, and specify ``requires_grad=True``. \n",
        "# (Like most functions that create tensors, ``torch.linspace()`` accepts an\n",
        "# optional ``requires_grad`` option.) \n",
        "# Setting this flag means that in every computation that follows, \n",
        "# autograd will be accumulating the history of \n",
        "# the computation in the output tensors of that computation.\n",
        "################################################################################\n",
        "\n",
        "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "print(a)\n",
        "\n",
        "################################################################################\n",
        "# Next, we’ll perform a computation, and \n",
        "# plot its output in terms of its inputs:\n",
        "################################################################################\n",
        "\n",
        "b = torch.sin(a)\n",
        "plt.plot(a.detach(), b.detach())\n",
        "\n",
        "################################################################################\n",
        "# Let’s have a closer look at the tensor ``b``. \n",
        "# When we print it, \n",
        "# we see an indicator that it is tracking its computation history:\n",
        "################################################################################\n",
        "\n",
        "print(b)\n",
        "\n",
        "################################################################################\n",
        "# This ``grad_fn`` gives us a hint that \n",
        "# when we execute the backpropagation step and compute gradients, \n",
        "# we’ll need to compute the derivative of :math:`sin(x)` \n",
        "# for all this tensor’s inputs.\n",
        "# \n",
        "# Let’s perform some more computations:\n",
        "################################################################################ \n",
        "\n",
        "c = 2 * b\n",
        "print(c)\n",
        "\n",
        "d = c + 1\n",
        "print(d)\n",
        "\n",
        "################################################################################\n",
        "# Finally, let’s compute a single-element output. \n",
        "# When you call ``.backward()`` on a tensor with no arguments, \n",
        "# it expects the calling tensor to contain only a single element,\n",
        "# as is the case when computing a loss function.\n",
        "################################################################################\n",
        "\n",
        "out = d.sum()\n",
        "print(out)\n",
        "\n",
        "################################################################################\n",
        "# Each ``grad_fn`` stored with our tensors allows \n",
        "# you to walk the computation all the way back to its inputs \n",
        "# with its ``next_functions`` property. \n",
        "# We can see below that drilling down on this property on ``d``\n",
        "# shows us the gradient functions for all the prior tensors. \n",
        "# Note that ``a.grad_fn`` is reported as ``None``, \n",
        "# indicating that this was an input to the function with no history of its own.\n",
        "################################################################################\n",
        "\n",
        "print('d:')\n",
        "print(d.grad_fn)\n",
        "print(d.grad_fn.next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print('\\nc:')\n",
        "print(c.grad_fn)\n",
        "print('\\nb:')\n",
        "print(b.grad_fn)\n",
        "print('\\na:')\n",
        "print(a.grad_fn)\n",
        "\n",
        "################################################################################\n",
        "# With all this machinery in place, \n",
        "# how do we get derivatives out? \n",
        "# You call the ``backward()`` method on the output, \n",
        "# and check the input’s ``grad`` property to inspect the gradients:\n",
        "################################################################################\n",
        "\n",
        "out.backward()\n",
        "print(a.grad)\n",
        "plt.plot(a.detach(), a.grad.detach())\n",
        "\n",
        "################################################################################\n",
        "# Recall the computation steps we took to get here:\n",
        "# \n",
        "#    a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "#    b = torch.sin(a)\n",
        "#    c = 2 * b\n",
        "#    d = c + 1\n",
        "#    out = d.sum()\n",
        "# \n",
        "# Adding a constant, as we did to compute ``d``, does not change the derivative. \n",
        "# That leaves :math:`c = 2 * b = 2 * sin(a)`, \n",
        "# the derivative of which should be :math:`2 * cos(a)`. \n",
        "# Looking at the graph above, that’s just what we see.\n",
        "# \n",
        "# Be aware that \n",
        "# only *leaf nodes* of the computation have their gradients computed. \n",
        "# If you tried, for example, ``print(c.grad)`` you’d get back ``None``. \n",
        "# In this simple example, only the input is a leaf node, \n",
        "# so only it has gradients computed.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Autograd in Training\n",
        "# \n",
        "# We’ve had a brief look at how autograd works, \n",
        "# but how does it look when it’s used for its intended purpose? \n",
        "# Let’s define a small model and examine how it changes \n",
        "# after a single training batch. \n",
        "# First, define a few constants, our model, \n",
        "# and some stand-ins for inputs and outputs:\n",
        "################################################################################\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "DIM_IN = 1000\n",
        "HIDDEN_SIZE = 100\n",
        "DIM_OUT = 10\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "        \n",
        "        self.layer1 = torch.nn.Linear(1000, 100)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer2 = torch.nn.Linear(100, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "    \n",
        "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
        "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
        "\n",
        "model = TinyModel()\n",
        "\n",
        "################################################################################\n",
        "# One thing you might notice is that \n",
        "# we never specify ``requires_grad=True`` for the model’s layers. \n",
        "# Within a subclass of ``torch.nn.Module``, \n",
        "# it’s assumed that \n",
        "# we want to track gradients on the layers’ weights for learning.\n",
        "# \n",
        "# If we look at the layers of the model, \n",
        "# we can examine the values of the weights, \n",
        "# and verify that no gradients have been computed yet:\n",
        "################################################################################\n",
        "\n",
        "print(model.layer2.weight[0][0:10]) # just a small slice\n",
        "print(model.layer2.weight.grad)\n",
        "\n",
        "################################################################################\n",
        "# Let’s see how this changes when we run through one training batch. \n",
        "# For a loss function, we’ll just use the square of the Euclidean distance\n",
        "# between our ``prediction`` and the ``ideal_output``, \n",
        "# and we’ll use a basic stochastic gradient descent optimizer.\n",
        "################################################################################\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "prediction = model(some_input)\n",
        "\n",
        "loss = (ideal_output - prediction).pow(2).sum()\n",
        "print(loss)\n",
        "\n",
        "################################################################################\n",
        "# Now, let’s call ``loss.backward()`` and see what happens:\n",
        "################################################################################\n",
        "\n",
        "loss.backward()\n",
        "print(model.layer2.weight[0][0:10])\n",
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "################################################################################\n",
        "# We can see that the gradients have been computed for each learning weight, \n",
        "# but the weights remain unchanged, because we haven’t run the optimizer yet. \n",
        "# The optimizer is responsible for updating model weights\n",
        "# based on the computed gradients.\n",
        "################################################################################\n",
        "\n",
        "optimizer.step()\n",
        "print(model.layer2.weight[0][0:10])\n",
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "################################################################################\n",
        "# You should see that ``layer2``\\ ’s weights have changed.\n",
        "# \n",
        "# One important thing about the process: \n",
        "# After calling ``optimizer.step()``, \n",
        "# you need to call ``optimizer.zero_grad()``, or\n",
        "# else every time you run ``loss.backward()``, \n",
        "# the gradients on the learning weights will accumulate:\n",
        "################################################################################\n",
        "\n",
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "for i in range(0, 5):\n",
        "    prediction = model(some_input)\n",
        "    loss = (ideal_output - prediction).pow(2).sum()\n",
        "    loss.backward()\n",
        "    \n",
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "################################################################################\n",
        "# After running the cell above, \n",
        "# you should see that after running ``loss.backward()`` multiple times, \n",
        "# the magnitudes of most of the gradients will be much larger. \n",
        "# Failing to zero the gradients before running your next training batch\n",
        "# will cause the gradients to blow up in\n",
        "# this manner, causing incorrect and unpredictable learning results.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Turning Autograd Off and On\n",
        "# \n",
        "# There are situations \n",
        "# where you will need fine-grained control over whether autograd is enabled. \n",
        "# There are multiple ways to do this,\n",
        "# depending on the situation.\n",
        "# \n",
        "# The simplest is to change the ``requires_grad`` flag on a tensor directly:\n",
        "################################################################################\n",
        "\n",
        "a = torch.ones(2, 3, requires_grad=True)\n",
        "print(a)\n",
        "\n",
        "b1 = 2 * a\n",
        "print(b1)\n",
        "\n",
        "a.requires_grad = False\n",
        "b2 = 2 * a\n",
        "print(b2)\n",
        "\n",
        "################################################################################\n",
        "# In the cell above, we see that \n",
        "# ``b1`` has a ``grad_fn`` (i.e., a traced computation history), \n",
        "# which is what we expect, since it was derived from a tensor, ``a``, \n",
        "# that had autograd turned on. \n",
        "# When we turn off autograd explicitly with ``a.requires_grad = False``,\n",
        "#  computation history is no longer tracked, as we see when we compute ``b2``.\n",
        "# \n",
        "# If you only need autograd turned off temporarily, \n",
        "# a better way is to use the ``torch.no_grad()``:\n",
        "################################################################################\n",
        "\n",
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = a + b\n",
        "print(c1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    c2 = a + b\n",
        "\n",
        "print(c2)\n",
        "\n",
        "c3 = a * b\n",
        "print(c3)\n",
        "\n",
        "################################################################################\n",
        "# ``torch.no_grad()`` can also be used as a function or method decorator:\n",
        "################################################################################\n",
        "\n",
        "def add_tensors1(x, y):\n",
        "    return x + y\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_tensors2(x, y):\n",
        "    return x + y\n",
        "\n",
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = add_tensors1(a, b)\n",
        "print(c1)\n",
        "\n",
        "c2 = add_tensors2(a, b)\n",
        "print(c2)\n",
        "\n",
        "################################################################################\n",
        "# There’s a corresponding context manager, ``torch.enable_grad()``, \n",
        "# for turning autograd on when it isn’t already. \n",
        "# It may also be used as a decorator.\n",
        "# \n",
        "# Finally, you may have a tensor that requires gradient tracking, \n",
        "# but you want a copy that does not.\n",
        "# For this we have the ``Tensor`` object’s ``detach()`` method - \n",
        "# it creates a copy of the tensor that is *detached*\n",
        "# from the computation history:\n",
        "################################################################################\n",
        "\n",
        "x = torch.rand(5, requires_grad=True)\n",
        "y = x.detach()\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "################################################################################\n",
        "# We did this above when we wanted to graph some of our tensors. \n",
        "# This is because ``matplotlib`` expects a NumPy array as input, \n",
        "# and the implicit conversion from a PyTorch tensor to a NumPy array \n",
        "# is not enabled for tensors with requires_grad=True. \n",
        "# Making a detached copy lets us move forward.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Autograd and In-place Operations\n",
        "# \n",
        "# In every example in this notebook so far, \n",
        "# we’ve used variables to capture the intermediate values of a computation. \n",
        "# Autograd needs these intermediate values to perform gradient computations. \n",
        "# *For this reason, you must be careful about using in-place operations \n",
        "# when using autograd.* \n",
        "# Doing so can destroy information you need to compute\n",
        "# derivatives in the ``backward()`` call. \n",
        "# PyTorch will even stop you if\n",
        "# you attempt an in-place operation on leaf variable that requires autograd, \n",
        "# as shown below.\n",
        "# \n",
        "# .. note::\n",
        "#     The following code cell throws a runtime error. This is expected.\n",
        "#\n",
        "#    a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "#    torch.sin_(a)\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Autograd Profiler\n",
        "# \n",
        "# Autograd tracks every step of your computation in detail. \n",
        "# Such a computation history, combined with timing information, would make a\n",
        "# handy profiler - and autograd has that feature baked in. \n",
        "# Here’s a quick example usage:\n",
        "################################################################################\n",
        "\n",
        "device = torch.device('cpu')\n",
        "run_on_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    run_on_gpu = True\n",
        "    \n",
        "x = torch.randn(2, 3, requires_grad=True)\n",
        "y = torch.rand(2, 3, requires_grad=True)\n",
        "z = torch.ones(2, 3, requires_grad=True)\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
        "    for _ in range(1000):\n",
        "        z = (z / x) * y\n",
        "        \n",
        "print(prf.key_averages().table(sort_by='self_cpu_time_total'))\n",
        "\n",
        "################################################################################\n",
        "# The profiler can also label individual sub-blocks of code, \n",
        "# break out the data by input tensor shape, \n",
        "# and export data as a Chrome tracing tools file. \n",
        "# For full details of the API, see the\n",
        "# `documentation <https://pytorch.org/docs/stable/autograd.html#profiler>`.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Advanced Topic: More Autograd Detail and the High-Level API\n",
        "# \n",
        "# If you have a function with an n-dimensional input and m-dimensional output, \n",
        "# :math:`\\vec{y}=f(\\vec{x})`, the complete gradient is a matrix of\n",
        "# the derivative of every output with respect to every input, \n",
        "# called the *Jacobian:*\n",
        "# \n",
        "# .. math::\n",
        "#\n",
        "#      J\n",
        "#      =\n",
        "#      \\left(\\begin{array}{ccc}\n",
        "#      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "#      \\vdots & \\ddots & \\vdots\\\\\n",
        "#      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "#      \\end{array}\\right)\n",
        "# \n",
        "# If you have a second function, :math:`l=g\\left(\\vec{y}\\right)` that\n",
        "# takes m-dimensional input (that is, the same dimensionality as the\n",
        "# output above), and returns a scalar output, you can express its\n",
        "# gradients with respect to :math:`\\vec{y}` as a column vector,\n",
        "# :math:`v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}`\n",
        "# - which is really just a one-column Jacobian.\n",
        "# \n",
        "# More concretely, imagine the first function as your PyTorch model (with\n",
        "# potentially many inputs and many outputs) and the second function as a\n",
        "# loss function (with the model’s output as input, and the loss value as\n",
        "# the scalar output).\n",
        "# \n",
        "# If we multiply the first function’s Jacobian by the gradient of the\n",
        "# second function, and apply the chain rule, we get:\n",
        "# \n",
        "# .. math::\n",
        "#\n",
        "#    J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
        "#    \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        "#    \\vdots & \\ddots & \\vdots\\\\\n",
        "#    \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "#    \\end{array}\\right)\\left(\\begin{array}{c}\n",
        "#    \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        "#    \\vdots\\\\\n",
        "#    \\frac{\\partial l}{\\partial y_{m}}\n",
        "#    \\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "#    \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        "#    \\vdots\\\\\n",
        "#    \\frac{\\partial l}{\\partial x_{n}}\n",
        "#    \\end{array}\\right)\n",
        "# \n",
        "# Note: You could also use the equivalent operation :math:`v^{T}\\cdot J`,\n",
        "# and get back a row vector.\n",
        "# \n",
        "# The resulting column vector is the *gradient of the second function with\n",
        "# respect to the inputs of the first* - or in the case of our model and\n",
        "# loss function, the gradient of the loss with respect to the model\n",
        "# inputs.\n",
        "# \n",
        "# **``torch.autograd`` is an engine for computing these products.** This\n",
        "# is how we accumulate the gradients over the learning weights during the\n",
        "# backward pass.\n",
        "# \n",
        "# For this reason, the ``backward()`` call can *also* take an optional\n",
        "# vector input. This vector represents a set of gradients over the tensor,\n",
        "# which are multiplied by the Jacobian of the autograd-traced tensor that\n",
        "# precedes it. Let’s try a specific example with a small vector:\n",
        "################################################################################\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)\n",
        "\n",
        "################################################################################\n",
        "# If we tried to call ``y.backward()`` now, we’d get a runtime error and a\n",
        "# message that gradients can only be *implicitly* computed for scalar\n",
        "# outputs. For a multi-dimensional output, autograd expects us to provide\n",
        "# gradients for those three outputs that it can multiply into the\n",
        "# Jacobian:\n",
        "################################################################################\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)\n",
        "\n",
        "################################################################################\n",
        "# (Note that the output gradients are all related to powers of two - which\n",
        "# we’d expect from a repeated doubling operation.)\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# The High-Level API\n",
        "# \n",
        "# There is an API on autograd that gives you direct access to important\n",
        "# differential matrix and vector operations. In particular, it allows you\n",
        "# to calculate the Jacobian and the *Hessian* matrices of a particular\n",
        "# function for particular inputs. (The Hessian is like the Jacobian, but\n",
        "# expresses all partial *second* derivatives.) It also provides methods\n",
        "# for taking vector products with these matrices.\n",
        "# \n",
        "# Let’s take the Jacobian of a simple function, evaluated for a 2\n",
        "# single-element inputs:\n",
        "################################################################################\n",
        "\n",
        "def exp_adder(x, y):\n",
        "    return 2 * x.exp() + 3 * y\n",
        "\n",
        "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)\n",
        "\n",
        "################################################################################\n",
        "# If you look closely, the first output should equal :math:`2e^x` (since\n",
        "# the derivative of :math:`e^x` is :math:`e^x`), and the second value\n",
        "# should be 3.\n",
        "# \n",
        "# You can, of course, do this with higher-order tensors:\n",
        "################################################################################\n",
        "\n",
        "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)\n",
        "\n",
        "################################################################################\n",
        "# The ``torch.autograd.functional.hessian()`` method works identically\n",
        "# (assuming your function is twice differentiable), but returns a matrix\n",
        "# of all second derivatives.\n",
        "# \n",
        "# There is also a function to directly compute the vector-Jacobian\n",
        "# product, if you provide the vector:\n",
        "################################################################################\n",
        "\n",
        "def do_some_doubling(x):\n",
        "    y = x * 2\n",
        "    while y.data.norm() < 1000:\n",
        "        y = y * 2\n",
        "    return y\n",
        "\n",
        "inputs = torch.randn(3)\n",
        "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
        "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)\n",
        "\n",
        "################################################################################\n",
        "# The ``torch.autograd.functional.jvp()`` method performs the same matrix\n",
        "# multiplication as ``vjp()`` with the operands reversed. The ``vhp()``\n",
        "# and ``hvp()`` methods do the same for a vector-Hessian product.\n",
        "# \n",
        "# For more information, including performance notes on the `docs for the\n",
        "# functional API \n",
        "# <https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api>`\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "pVkdkwoWnpII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Polynomial Fit to Sine Curve (Built-In Functions with Forward and Backward) [beginner_source/examples_autograd/polynomial_autograd.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_autograd/polynomial_autograd.py)"
      ],
      "metadata": {
        "id": "3msMlPmbjW3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_autograd/polynomial_autograd.py\n",
        "\"\"\"\n",
        "PyTorch: Tensors and autograd\n",
        "\n",
        "A third order polynomial, trained to predict :math:`y=\\sin(x)` from :math:`-\\pi`\n",
        "to :math:`\\pi` by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation computes the forward pass using operations on PyTorch\n",
        "Tensors, and uses PyTorch autograd to compute gradients.\n",
        "\n",
        "A PyTorch Tensor represents a node in a computational graph. If ``x`` is a\n",
        "Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor\n",
        "holding the gradient of ``x`` with respect to some scalar value.\n",
        "\"\"\"\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "# By default, requires_grad=False, which indicates that we do not need to\n",
        "# compute gradients with respect to these Tensors during the backward pass.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Create random Tensors for weights. For a third order polynomial, we need\n",
        "# 4 weights: y = a + b x + c x^2 + d x^3\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y using operations on Tensors.\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the scalar value held in the loss.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
        "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = a + b * x + c * x**2 + d * x**3\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,3))\n",
        "ax.plot(x,y,\"-k\",label=\"original\")\n",
        "ax.plot(x,y_pred,\"--r\",label=\"predicted\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VkvBOy6JjXBg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "13a06d4e-4347-4d6a-9e05-eb572b5fde4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 3272.46044921875\n",
            "199 2184.318603515625\n",
            "299 1459.694091796875\n",
            "399 976.9326782226562\n",
            "499 655.1564331054688\n",
            "599 440.5772399902344\n",
            "699 297.4096984863281\n",
            "799 201.8365020751953\n",
            "899 137.99900817871094\n",
            "999 95.33416748046875\n",
            "1099 66.80167388916016\n",
            "1199 47.70795822143555\n",
            "1299 34.9218864440918\n",
            "1399 26.353607177734375\n",
            "1499 20.607515335083008\n",
            "1599 16.751129150390625\n",
            "1699 14.160844802856445\n",
            "1799 12.419572830200195\n",
            "1899 11.24796199798584\n",
            "1999 10.45901870727539\n",
            "Result: y = -0.024277467280626297 + 0.8242697715759277 x + 0.004188267979770899 x^2 + -0.08871166408061981 x^3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Polynomial Fit to Sine Curve (Custom Functions with Forward and Backward) [beginner_source/examples_autograd/polynomial_custom_function.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_autograd/polynomial_custom_function.py)"
      ],
      "metadata": {
        "id": "cXMnwDefjXLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_autograd/polynomial_custom_function.py\n",
        "\"\"\"\n",
        "PyTorch: Defining New autograd Functions\n",
        "\n",
        "A third order polynomial, trained to predict :math:`y=\\sin(x)` from :math:`-\\pi`\n",
        "to :math:`\\pi` by minimizing squared Euclidean distance. Instead of writing the\n",
        "polynomial as :math:`y=a+bx+cx^2+dx^3`, we write the polynomial as\n",
        ":math:`y=a+b P_3(c+dx)` where :math:`P_3(x)=\\\\frac{1}{2}\\\\left(5x^3-3x\\\\right)` is\n",
        "the `Legendre polynomial`_ of degree three.\n",
        "\n",
        ".. _Legendre polynomial:\n",
        "    https://en.wikipedia.org/wiki/Legendre_polynomials\n",
        "\n",
        "This implementation computes the forward pass using operations on PyTorch\n",
        "Tensors, and uses PyTorch autograd to compute gradients.\n",
        "\n",
        "In this implementation we implement our own custom autograd function to perform\n",
        ":math:`P_3'(x)`. By mathematics, :math:`P_3'(x)=\\\\frac{3}{2}\\\\left(5x^2-1\\\\right)`\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "class LegendrePolynomial3(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "# By default, requires_grad=False, which indicates that we do not need to\n",
        "# compute gradients with respect to these Tensors during the backward pass.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Create random Tensors for weights. For this example, we need\n",
        "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
        "# not too far from the correct result to ensure convergence.\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 5e-6\n",
        "for t in range(2000):\n",
        "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
        "    P3 = LegendrePolynomial3.apply\n",
        "\n",
        "    # Forward pass: compute predicted y using operations; we compute\n",
        "    # P3 using our custom autograd operation.\n",
        "    y_pred = a + b * P3(c + d * x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = a + b * P3(c + d * x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,3))\n",
        "ax.plot(x,y,\"-k\",label=\"original\")\n",
        "ax.plot(x,y_pred,\"--r\",label=\"predicted\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i5d9quIijXVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "e59c5e50-33de-4645-ac35-f1d3793e41a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 209.95834350585938\n",
            "199 144.66018676757812\n",
            "299 100.70249938964844\n",
            "399 71.03519439697266\n",
            "499 50.97850799560547\n",
            "599 37.403133392333984\n",
            "699 28.206867218017578\n",
            "799 21.97318458557129\n",
            "899 17.7457275390625\n",
            "999 14.877889633178711\n",
            "1099 12.93176555633545\n",
            "1199 11.610918998718262\n",
            "1299 10.71425724029541\n",
            "1399 10.10548210144043\n",
            "1499 9.692106246948242\n",
            "1599 9.411375045776367\n",
            "1699 9.220745086669922\n",
            "1799 9.091285705566406\n",
            "1899 9.003360748291016\n",
            "1999 8.943639755249023\n",
            "Result: y = -5.394172664097141e-09 + -2.208526849746704 * P3(1.367587154632588e-09 + 0.2554861009120941 x)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 5 Optimization***"
      ],
      "metadata": {
        "id": "t2WH4YGnizQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Optimizing Model Parameters [beginner_source/basics/optimization_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/optimization_tutorial.py)"
      ],
      "metadata": {
        "id": "ihXyJN6IW1sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/optimization_tutorial.py\n",
        "\"\"\"\n",
        "Optimizing Model Parameters\n",
        "===========================\n",
        "\n",
        "Now that we have a model and data it's time to train, \n",
        "validate and test our model by optimizing its parameters on\n",
        "our data. Training a model is an iterative process; \n",
        "in each iteration the model makes a guess about the output, calculates\n",
        "the error in its guess (*loss*), collects the derivatives of the error \n",
        "with respect to its parameters (as we saw in\n",
        "the `previous section  <autograd_tutorial.html>`_), and \n",
        "**optimizes** these parameters using gradient descent. For a more\n",
        "detailed walkthrough of this process, \n",
        "check out this video on \n",
        "`backpropagation from 3Blue1Brown \n",
        "<https://www.youtube.com/watch?v=tIeHLnjs5U8>`.\n",
        "\n",
        "Prerequisite Code\n",
        "-----------------\n",
        "We load the code from the previous sections \n",
        "on `Datasets & DataLoaders <data_tutorial.html>`\n",
        "and `Build Model  <buildmodel_tutorial.html>`.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Hyperparameters\n",
        "#\n",
        "# Hyperparameters are adjustable parameters \n",
        "# that let you control the model optimization process.\n",
        "# Different hyperparameter values \n",
        "# can impact model training and convergence rates\n",
        "# (`read more \n",
        "# <https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html>`\n",
        "# about hyperparameter tuning)\n",
        "#\n",
        "# We define the following hyperparameters for training:\n",
        "#  - **Number of Epochs** - the number times to iterate over the dataset\n",
        "#  - **Batch Size** - the number of data samples propagated \n",
        "#    through the network before the parameters are updated\n",
        "#  - **Learning Rate** - \n",
        "#    how much to update models parameters at each batch/epoch. \n",
        "#    Smaller values yield slow learning speed, \n",
        "#    while large values may result in unpredictable behavior during training.\n",
        "################################################################################\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "################################################################################\n",
        "# Optimization Loop\n",
        "#\n",
        "# Once we set our hyperparameters, \n",
        "# we can then train and optimize our model with an optimization loop. \n",
        "# Each iteration of the optimization loop is called an **epoch**.\n",
        "#\n",
        "# Each epoch consists of two main parts:\n",
        "#  - **The Train Loop** - iterate over the training dataset and \n",
        "#    try to converge to optimal parameters.\n",
        "#  - **The Validation/Test Loop** - iterate over the test dataset \n",
        "#    to check if model performance is improving.\n",
        "#\n",
        "# Let's briefly familiarize ourselves with some of the concepts \n",
        "# used in the training loop. \n",
        "# Jump ahead to see the :ref:`full-impl-label` of the optimization loop.\n",
        "#\n",
        "# Loss Function\n",
        "# ~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# When presented with some training data, \n",
        "# our untrained network is likely not to give the correct answer. \n",
        "# **Loss function** measures the degree of dissimilarity \n",
        "# of obtained result to the target value,\n",
        "# and it is the loss function that we want to minimize during training. \n",
        "# To calculate the loss we make a\n",
        "# prediction using the inputs of our given data sample \n",
        "# and compare it against the true data label value.\n",
        "#\n",
        "# Common loss functions include \n",
        "# `nn.MSELoss <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss>` \n",
        "# (Mean Square Error) for regression tasks, and\n",
        "# `nn.NLLLoss <https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>` \n",
        "# (Negative Log Likelihood) for classification.\n",
        "# `nn.CrossEntropyLoss <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss>` \n",
        "# combines ``nn.LogSoftmax`` and ``nn.NLLLoss``.\n",
        "#\n",
        "# We pass our model's output logits to ``nn.CrossEntropyLoss``, \n",
        "# which will normalize the logits and compute the prediction error.\n",
        "################################################################################\n",
        "\n",
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "################################################################################\n",
        "# Optimizer\n",
        "#\n",
        "# Optimization is the process of adjusting model parameters \n",
        "# to reduce model error in each training step. \n",
        "# **Optimization algorithms** define how this process is performed \n",
        "# (in this example we use Stochastic Gradient Descent).\n",
        "# All optimization logic is encapsulated in  the ``optimizer`` object. \n",
        "# Here, we use the SGD optimizer; additionally, there are many \n",
        "# `different optimizers <https://pytorch.org/docs/stable/optim.html>`_\n",
        "# available in PyTorch such as ADAM and RMSProp, \n",
        "# that work better for different kinds of models and data.\n",
        "#\n",
        "# We initialize the optimizer by registering the model's parameters \n",
        "# that need to be trained, and passing in the learning rate hyperparameter.\n",
        "################################################################################\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "################################################################################\n",
        "# Inside the training loop, optimization happens in three steps:\n",
        "#  * Call ``optimizer.zero_grad()`` to reset the gradients of model parameters. \n",
        "# Gradients by default add up; to prevent double-counting, \n",
        "# we explicitly zero them at each iteration.\n",
        "#  * Backpropagate the prediction loss with a call to ``loss.backward()``. \n",
        "# PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "#  * Once we have our gradients, we call ``optimizer.step()`` \n",
        "# to adjust the parameters by the gradients collected in the backward pass.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# .. _full-impl-label:\n",
        "#\n",
        "# Full Implementation\n",
        "# -----------------------\n",
        "# We define ``train_loop`` that loops over our optimization code, and \n",
        "# ``test_loop`` that evaluates the model's performance against our test data.\n",
        "################################################################################\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "################################################################################\n",
        "# We initialize the loss function and optimizer, \n",
        "# and pass it to ``train_loop`` and ``test_loop``.\n",
        "# Feel free to increase the number of epochs to track \n",
        "# the model's improving performance.\n",
        "################################################################################\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "################################################################################\n",
        "# Further Reading\n",
        "# \n",
        "# - `Loss Functions <https://pytorch.org/docs/stable/nn.html#loss-functions>`\n",
        "# - `torch.optim <https://pytorch.org/docs/stable/optim.html>`\n",
        "# - `Warmstart Training a Model <https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html>`\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "hwdAsXGvW10L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Training with PyTorch [beginner_source/introyt/trainingyt.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/trainingyt.py) [english](https://www.youtube.com/watch?v=jF43_wj_DCQ)"
      ],
      "metadata": {
        "id": "cjkx0hPlpCAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/trainingyt.py\n",
        "\"\"\"\n",
        "Training with PyTorch\n",
        "=====================\n",
        "\n",
        "Follow along with the video below or on \n",
        "`youtube <https://www.youtube.com/watch?v=jF43_wj_DCQ>`.\n",
        "\n",
        "Introduction\n",
        "------------\n",
        "\n",
        "In past videos, we’ve discussed and demonstrated:\n",
        "\n",
        "- Building models with the neural network layers and functions of the torch.nn module\n",
        "- The mechanics of automated gradient computation, which is central to\n",
        "  gradient-based model training \n",
        "- Using TensorBoard to visualize training progress and other activities\n",
        "\n",
        "In this video, we’ll be adding some new tools to your inventory:\n",
        "\n",
        "- We’ll get familiar with the dataset and dataloader abstractions, and how\n",
        "  they ease the process of feeding data to your model during a training loop \n",
        "- We’ll discuss specific loss functions and when to use them\n",
        "- We’ll look at PyTorch optimizers, which implement algorithms to adjust\n",
        "  model weights based on the outcome of a loss function\n",
        "\n",
        "Finally, we’ll pull all of these together and see a full PyTorch\n",
        "training loop in action.\n",
        "\n",
        "\n",
        "Dataset and DataLoader\n",
        "----------------------\n",
        " \n",
        "The ``Dataset`` and ``DataLoader`` classes encapsulate the process of\n",
        "pulling your data from storage and exposing it to your training loop in\n",
        "batches.\n",
        "\n",
        "The ``Dataset`` is responsible for accessing and processing single\n",
        "instances of data.\n",
        " \n",
        "The ``DataLoader`` pulls instances of data from the ``Dataset`` (either\n",
        "automatically or with a sampler that you define), collects them in\n",
        "batches, and returns them for consumption by your training loop. The\n",
        "``DataLoader`` works with all kinds of datasets, regardless of the type\n",
        "of data they contain.\n",
        " \n",
        "For this tutorial, we’ll be using the Fashion-MNIST dataset provided by\n",
        "TorchVision. We use ``torchvision.transforms.Normalize()`` to\n",
        "zero-center and normalize the distribution of the image tile content,\n",
        "and download both training and validation data splits.\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Create datasets for training & validation, download if necessary\n",
        "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
        "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Create data loaders for our datasets; shuffle for training, not for validation\n",
        "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Class labels\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# Report split sizes\n",
        "print('Training set has {} instances'.format(len(training_set)))\n",
        "print('Validation set has {} instances'.format(len(validation_set)))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# As always, let’s visualize the data as a sanity check:\n",
        "################################################################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Create a grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)\n",
        "print('  '.join(classes[labels[j]] for j in range(4)))\n",
        "\n",
        "################################################################################\n",
        "# The Model\n",
        "# ---------\n",
        "# \n",
        "# The model we’ll use in this example is a variant of LeNet-5 - it should\n",
        "# be familiar if you’ve watched the previous videos in this series.\n",
        "################################################################################ \n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch models inherit from torch.nn.Module\n",
        "class GarmentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GarmentClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "model = GarmentClassifier()\n",
        "\n",
        "################################################################################\n",
        "# Loss Function\n",
        "# -------------\n",
        "# \n",
        "# For this example, we’ll be using a cross-entropy loss. For demonstration\n",
        "# purposes, we’ll create batches of dummy output and label values, run\n",
        "# them through the loss function, and examine the result.\n",
        "################################################################################\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
        "# Represents the model's confidence in each of the 10 classes for a given input\n",
        "dummy_outputs = torch.rand(4, 10)\n",
        "# Represents the correct class among the 10 being tested\n",
        "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
        "    \n",
        "print(dummy_outputs)\n",
        "print(dummy_labels)\n",
        "\n",
        "loss = loss_fn(dummy_outputs, dummy_labels)\n",
        "print('Total loss for this batch: {}'.format(loss.item()))\n",
        "\n",
        "################################################################################\n",
        "# Optimizer\n",
        "# ---------\n",
        "# \n",
        "# For this example, we’ll be using simple `stochastic gradient\n",
        "# descent <https://pytorch.org/docs/stable/optim.html>`__ with momentum.\n",
        "# \n",
        "# It can be instructive to try some variations on this optimization\n",
        "# scheme:\n",
        "# \n",
        "# - Learning rate determines the size of the steps the optimizer\n",
        "#   takes. What does a different learning rate do to the your training\n",
        "#   results, in terms of accuracy and convergence time?\n",
        "# - Momentum nudges the optimizer in the direction of strongest gradient over\n",
        "#   multiple steps. What does changing this value do to your results? \n",
        "# - Try some different optimization algorithms, such as averaged SGD, Adagrad, \n",
        "#   or Adam. How do your results differ?\n",
        "################################################################################\n",
        "\n",
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "################################################################################\n",
        "# The Training Loop\n",
        "# -----------------\n",
        "# \n",
        "# Below, we have a function that performs one training epoch. It\n",
        "# enumerates data from the DataLoader, and on each pass of the loop does\n",
        "# the following:\n",
        "# \n",
        "# - Gets a batch of training data from the DataLoader\n",
        "# - Zeros the optimizer’s gradients \n",
        "# - Performs an inference - that is, gets predictions from the model \n",
        "#   for an input batch\n",
        "# - Calculates the loss for \n",
        "#   that set of predictions vs. the labels on the dataset\n",
        "# - Calculates the backward gradients over the learning weights\n",
        "# - Tells the optimizer to perform one learning step - \n",
        "#   that is, adjust the model’s learning weights \n",
        "#   based on the observed gradients for this batch, \n",
        "#   according to the optimization algorithm we chose\n",
        "# - It reports on the loss for every 1000 batches.\n",
        "# - Finally, it reports the average per-batch loss for the last\n",
        "#   1000 batches, for comparison with a validation run\n",
        "################################################################################\n",
        "\n",
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    \n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        \n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "            \n",
        "    return last_loss\n",
        "\n",
        "################################################################################\n",
        "# Per-Epoch Activity\n",
        "# ~~~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# There are a couple of things we’ll want to do once per epoch: \n",
        "#\n",
        "# - Perform validation by checking our relative loss on a set of data that was \n",
        "#   not used for training, and report this \n",
        "# - Save a copy of the model\n",
        "# \n",
        "# Here, we’ll do our reporting in TensorBoard. This will require going to\n",
        "# the command line to start TensorBoard, and opening it in another browser\n",
        "# tab.\n",
        "################################################################################\n",
        "\n",
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "    \n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "    \n",
        "    # We don't need gradients on to do reporting\n",
        "    model.train(False)\n",
        "    \n",
        "    running_vloss = 0.0\n",
        "    for i, vdata in enumerate(validation_loader):\n",
        "        vinputs, vlabels = vdata\n",
        "        voutputs = model(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        running_vloss += vloss\n",
        "    \n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "    \n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "    \n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    epoch_number += 1\n",
        "\n",
        "################################################################################\n",
        "# To load a saved version of the model:\n",
        "# \n",
        "# ::\n",
        "# \n",
        "#    saved_model = GarmentClassifier()\n",
        "#    saved_model.load_state_dict(torch.load(PATH))\n",
        "# \n",
        "# Once you’ve loaded the model, it’s ready for whatever you need it for -\n",
        "# more training, inference, or analysis.\n",
        "# \n",
        "# Note that if your model has constructor parameters that affect model\n",
        "# structure, you’ll need to provide them and configure the model\n",
        "# identically to the state in which it was saved.\n",
        "# \n",
        "# Other Resources\n",
        "# ---------------\n",
        "# \n",
        "# -  Docs on the `data\n",
        "#    utilities <https://pytorch.org/docs/stable/data.html>`__, including\n",
        "#    Dataset and DataLoader, at pytorch.org\n",
        "# -  A `note on the use of pinned memory \n",
        "#    <https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning>`\n",
        "#    for GPU training\n",
        "# -  Documentation on the datasets available in\n",
        "#    `TorchVision <https://pytorch.org/vision/stable/datasets.html>`,\n",
        "#    `TorchText <https://pytorch.org/text/stable/datasets.html>`, and\n",
        "#    `TorchAudio <https://pytorch.org/audio/stable/datasets.html>`\n",
        "# -  Documentation on the `loss\n",
        "#    functions <https://pytorch.org/docs/stable/nn.html#loss-functions>`\n",
        "#    available in PyTorch\n",
        "# -  Documentation on the `torch.optim\n",
        "#    package <https://pytorch.org/docs/stable/optim.html>`, which\n",
        "#    includes optimizers and related tools, such as learning rate\n",
        "#    scheduling\n",
        "# -  A detailed `tutorial on saving and loading\n",
        "#    models <https://pytorch.org/tutorials/beginner/saving_loading_models.html>`\n",
        "# -  The `Tutorials section of\n",
        "#    pytorch.org <https://pytorch.org/tutorials/>` contains tutorials on\n",
        "#    a broad variety of training tasks, including classification in\n",
        "#    different domains, generative adversarial networks, reinforcement\n",
        "#    learning, and more \n",
        "################################################################################"
      ],
      "metadata": {
        "id": "6a7g1K5HpCMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Polynomial Fit to Sine Curve (Optimizer) [beginner_source/examples_nn/polynomial_optim.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/polynomial_optim.py)"
      ],
      "metadata": {
        "id": "vd_vfQf1kAeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PyTorch: optim\n",
        "--------------\n",
        "\n",
        "A third order polynomial, trained to predict :math:`y=\\sin(x)` from :math:`-\\pi`\n",
        "to :math:`pi` by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses the nn package from PyTorch to build the network.\n",
        "\n",
        "Rather than manually updating the weights of the model as we have been doing,\n",
        "we use the optim package to define an Optimizer that will update the weights\n",
        "for us. The optim package defines many optimization algorithms that are commonly\n",
        "used for deep learning, including SGD+momentum, RMSProp, Adam, etc.\n",
        "\"\"\"\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Prepare the input tensor (x, x^2, x^3).\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# Use the nn package to define our model and loss function.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
        "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "linear_layer = model[0]\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "id": "5aqCx2VbkAmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 6 Linear Regression***"
      ],
      "metadata": {
        "id": "N6xvUP2diArJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Linear Regression without Optimizor [한국어](https://www.youtube.com/watch?v=Pj1wOWWM8xM&list=PLXziV1DL41oiG39s82XHjPHbOIJKWdiNa&index=3)"
      ],
      "metadata": {
        "id": "qEseDDRkmOi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np; np.random.seed(0)\n",
        "import torch; torch.manual_seed(0)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    x = np.random.uniform(size=(100,1))\n",
        "    y = 1 + 2*x + np.random.normal(scale=0.1,size=(100,1))\n",
        "    \n",
        "    inputs = torch.tensor(x, dtype=torch.float32) # (100, 1)\n",
        "    targets = torch.tensor(y, dtype=torch.float32) # (100, 1)\n",
        "        \n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "def initialize_weight():\n",
        "    w = torch.randn((1,), requires_grad=True) # (1,)\n",
        "    b = torch.randn((1,), requires_grad=True) # (1,)\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def train(inputs, targets, w, b, epochs=600, lr=1e-1):\n",
        "    w_trace = []\n",
        "    b_trace = []\n",
        "    loss_trace = []\n",
        "    for i in range(epochs):\n",
        "        preds = model(inputs, w, b) # (100, 1)\n",
        "        loss = mse(preds, targets) # ()\n",
        "        w_trace.append(w.item())\n",
        "        b_trace.append(b.item())\n",
        "        loss_trace.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "        # without : with torch.no_grad():\n",
        "        # RuntimeError: a leaf Variable that requires grad is being used \n",
        "        # in an in-place operation.\n",
        "            w -= w.grad * lr # in-place operation\n",
        "            b -= b.grad * lr # in-place operation\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "            \n",
        "    return w, b, w_trace, b_trace, loss_trace \n",
        "\n",
        "\n",
        "def model(inputs, w, b):\n",
        "    return inputs * w + b # (100, 1) * (1,) + (1,) = (100, 1)\n",
        "\n",
        "\n",
        "def mse(preds, targets):\n",
        "    return torch.sum((preds-targets)**2) / targets.numel()\n",
        "\n",
        "\n",
        "def main():\n",
        "    inputs, targets = load_data()\n",
        "    \n",
        "    w, b = initialize_weight()\n",
        "    \n",
        "    w, b, w_trace, b_trace, loss_trace = train(inputs, targets, w, b) \n",
        "\n",
        "    preds = model(inputs, w, b)\n",
        "\n",
        "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1,4,figsize=(15,4))\n",
        "    ax0.plot(loss_trace,label=\"loss\")\n",
        "    ax1.plot(w_trace,label=\"slope\")\n",
        "    ax2.plot(b_trace,label=\"intercept\")\n",
        "    ax3.plot(inputs,targets,'k.',label=\"data\")\n",
        "    ax3.plot(inputs,preds.detach().numpy(),'r-',label=\"linear regression\")\n",
        "    for ax in (ax0, ax1, ax2, ax3):\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "HcnRRzQ9mPx9",
        "outputId": "d543d3db-4ce8-4cf2-934e-922b75485c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Linear Regression with Optimizor"
      ],
      "metadata": {
        "id": "elIsT8K-mP_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np; np.random.seed(0)\n",
        "import torch; torch.manual_seed(0)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    x = np.random.uniform(size=(100,1))\n",
        "    y = 1 + 2*x + np.random.normal(scale=0.1,size=(100,1))\n",
        "    \n",
        "    inputs = torch.tensor(x, dtype=torch.float32) # (100, 1)\n",
        "    targets = torch.tensor(y, dtype=torch.float32) # (100, 1)\n",
        "        \n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "def train(inputs, targets, model, loss_fn, optimizor, epochs=600):\n",
        "    w_trace = []\n",
        "    b_trace = []\n",
        "    loss_trace = []\n",
        "    for epoch in range(epochs):\n",
        "        preds = model(inputs)\n",
        "        loss = loss_fn(preds,targets)\n",
        "        w_trace.append(model.weight.item())\n",
        "        b_trace.append(model.bias.item())\n",
        "        loss_trace.append(loss.item())\n",
        " \n",
        "        loss.backward()\n",
        "\n",
        "        optimizor.step()\n",
        "        optimizor.zero_grad()\n",
        "            \n",
        "    return w_trace, b_trace, loss_trace \n",
        "\n",
        "\n",
        "def main():\n",
        "    inputs, targets = load_data()\n",
        "    \n",
        "    model = torch.nn.Linear(in_features=1,out_features=1)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    optimizor = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "    \n",
        "    w_trace, b_trace, loss_trace = train(inputs, targets, model, loss_fn, optimizor) \n",
        "    \n",
        "    preds = model(inputs)\n",
        "\n",
        "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1,4,figsize=(15,4))\n",
        "    ax0.plot(loss_trace,label=\"loss\")\n",
        "    ax1.plot(w_trace,label=\"slope\")\n",
        "    ax2.plot(b_trace,label=\"intercept\")\n",
        "    ax3.plot(inputs,targets,'k.',label=\"data\")\n",
        "    ax3.plot(inputs,preds.detach().numpy(),'r-',label=\"linear regression\")\n",
        "    for ax in (ax0, ax1, ax2, ax3):\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "C9E4A4yEk6de",
        "outputId": "1341bb20-89a8-4cb2-f52d-a6492ee86303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Linear Regression without DataLoader"
      ],
      "metadata": {
        "id": "qkrJ7Ay6yDL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np; np.random.seed(0)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_data():\n",
        "    x = np.random.uniform(size=(100,1))\n",
        "    y = 1 + 2*x + np.random.normal(scale=0.1,size=(100,1))\n",
        "    \n",
        "    inputs = torch.tensor(x, dtype=torch.float32) # (100, 1)\n",
        "    targets = torch.tensor(y, dtype=torch.float32) # (100, 1)\n",
        "        \n",
        "    return inputs, targets\n",
        "\n",
        "class Model(nn.Module):        \n",
        "        \n",
        "    def __init__(self, inputs, targets):\n",
        "        super().__init__()\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "        \n",
        "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
        "        self.loss = F.mse_loss\n",
        "            \n",
        "    def fit(self, epochs, lr, opt_func=torch.optim.SGD):\n",
        "        opt = opt_func(self.parameters(), lr)\n",
        "\n",
        "        w_trace = []\n",
        "        b_trace = []\n",
        "        loss_trace = []\n",
        "        for i in range(epochs):\n",
        "            preds = self(self.inputs) # 1. Generate predictions\n",
        "            loss = self.loss(preds, self.targets) # 2. Calculate loss\n",
        "            loss.backward() # 3. Compute gradients\n",
        "            opt.step() # 4. Update parameters using gradients\n",
        "            opt.zero_grad() # 5. Reset the gradients to zero\n",
        "\n",
        "            w_trace.append(self.linear.weight.item())\n",
        "            b_trace.append(self.linear.bias.item())\n",
        "            loss_trace.append(loss.item())\n",
        "\n",
        "        return {\"w_trace\": w_trace, \"b_trace\": b_trace, \"loss_trace\": loss_trace}\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        xb = xb.view((-1, 1))\n",
        "        return self.linear(xb)\n",
        "    \n",
        "def main():\n",
        "    inputs, targets = load_data() \n",
        "    \n",
        "    model = Model(inputs, targets)\n",
        "    history = model.fit(epochs=600, lr=1e-1, opt_func=torch.optim.SGD)\n",
        "\n",
        "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1,4,figsize=(15,4))\n",
        "    ax0.plot(history[\"loss_trace\"],label=\"loss\")\n",
        "    ax1.plot(history[\"w_trace\"],label=\"slope\")\n",
        "    ax2.plot(history[\"b_trace\"],label=\"intercept\")\n",
        "    ax3.plot(inputs,targets,'k.',label=\"data\")\n",
        "    ax3.plot(inputs,model(inputs).detach().numpy(),'r-',label=\"linear regression\")\n",
        "    for ax in (ax0, ax1, ax2, ax3):\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "2qfFB2vcyDV8",
        "outputId": "93a65085-7cce-4469-bff3-8c10d4d54b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x288 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABC8AAAEYCAYAAAB89tyPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU1fnH8c/JZIewmLDvKqDIpgQk4hJFxYW64IK0al0qtW7FnytWAYVKra2iUqVYEbEWbV2oC7YoEhEYRUBQBIuoiAGUECAQQjKZmfP7YzJhErJnkrmTfN+vV16z3DP3PsnAzJ1nzvMcY61FRERERERERMSpYiIdgIiIiIiIiIhIVZS8EBERERERERFHU/JCRERERERERBxNyQsRERERERERcTQlL0RERERERETE0WIjdeC0tDTbs2fPSB1eRKqwevXqXdbadpGOA/RaIeJkTnqtAL1eiDiZk14v9Foh4lxVvVZELHnRs2dPVq1aFanDi0gVjDHfRzqGIL1WiDiXk14rQK8XIk7mpNcLvVaIOFdVrxUqGxERERERERERR1PyQkREREREREQcTckLEREREREREXG0iPW8EIkmxcXFZGdnU1hYGOlQwioxMZGuXbsSFxcX6VBqpak+H9WJ1udLRCSaNNf3mHCJ1vcqPe9NV7T+m5TDKXkhUgPZ2dmkpKTQs2dPjDGRDicsrLXk5uaSnZ1Nr169Ih1OrTTF56M60fx8iYhEk+b4HhMu0fxepee9aYrmf5NyOJWNiNRAYWEhqampTerNzBhDampq2L5hMMZ0M8YsMcZsMMZ8aYz5bQVjjDHmSWPMZmPM58aYE+pyrKb4fFQn3M+XiIhUrDm+x4RLNL9X6XlvmqL536QcTjMvRGqoKb6Zhfl38gJ3WGvXGGNSgNXGmPestRtCxpwL9C75ORF4puSy1pri81Gd5vg7i4hEgl5v6y6a/3bRHLtUTs9r06GZFyISFtbaHdbaNSXX9wMbgS7lhl0IzLMBHwNtjDGdGjlUERERERGJMo5OXlhr8fr8+P020qGIRFzLli0jHUKNGWN6AscDn5Tb1AX4IeR2NocnODDGjDfGrDLGrMrJyWmoMMMuMzOTVatWRToMkRqx1mKtxe8P/Pj8gfdcr8+PT++7IhF10kknVTtmxowZFBQUNEI0h9u7dy9PP/10RI7dXEyZMoU//elPlW5fsGABGzZsqHS7SKS53W6mT5+O2+0O2z4dXTZiLRz9u3e5/cw+/PbM3pEOR0RqwBjTEngNmGCt3VeXfVhrZwOzAdLT0/UpSg7j8fo56PFRUOzF4/VT5PWXu/Th8frx+PwUFQcuPSHbin0lH9b9Fp/fX3JZcttX9n6/tXjLjLd4/f7S234L2MCl31psyKXl0P2UXPqD9/tLEgiE3G8D9/mD9/uD+ym7T6D0/sCN4EVgTMhdpccIPqYmzhvQkad/MaSez1L0+HJ7HtfPXUVyvIsnxx1P/y6tIx2SNHMrVqyodsyMGTO48sorSU5OrvF+fT4fLperPqEBh5IXN910U733JXWzYMECRo8eTb9+/SIdishh3G43I0eOxOPxEB8fz+LFi8nIyKj3fh2dvAg6dNolItZa7r77bt59912MMdx///2MHTuWHTt2MHbsWPbt24fX6+WZZ57hpJNO4vrrr2fVqlUYY7juuuu4/fbbGyw2Y0wcgcTFS9ba1ysYsg3oFnK7a8l9UefAgQNcfvnlZGdn4/P5eOCBB8psnz9/Pg8//DDWWs4//3weeeQRIDCD5oYbbmDRokV07NiRl19+mXbt2vHNN99w8803k5OTQ3JyMs8++yzHHHNMJH61BlVY7GNfYTH7DhaTF/pTUEzeQS95B4vZX1hMgcfHAY+XAo+PguBlUeD6weJA8qG+XDEGV4whtsxlTOnt0G2uGEOsq+z2OFcMiXEGYwwxBmKMwUDpbVNyX4wxUHodDCVjjSkZE7wd8liC+zCl+zFATEygbre0erdkLASOF9x26PqhbYceY6odd1T76JnpFQ4PvrWBQq8PgGueX8nbt55Cx9aJEY5KmrOWLVuSn59PVlYWU6ZMIS0tjfXr1zNkyBD+/ve/89RTT7F9+3ZOP/100tLSWLJkCYsWLWLy5MkUFRVx1FFH8fzzz9OyZUt69uzJ2LFjee+997j77rtp06YN9913Hz6fj7S0NBYvXsyBAwe49dZbWb9+PcXFxUyZMoULL7yQuXPn8sYbb5CXl8e2bdu48sormTx5Mvfeey/ffPMNgwcP5qyzzuLRRx+N9J8sItxuN1lZWWRmZoblg9nvf/97XnjhBdq3b0+3bt0YMmQIzz77LLNnz8bj8XD00Ufz4osvsnbtWt58800+/PBDpk2bxmuvvcYHH3xw2LjaJLZEwikrKwuPx4PP58Pj8ZCVldX0kxfBk6qaflMk0hgefOtLNmyv04SCSvXr3IrJPzuuRmNff/111q5dy7p169i1axdDhw7l1FNP5R//+AejRo3id7/7HT6fj4KCAtauXcu2bdtYv349EPimpKGYQDek54CN1trHKhn2JnCLMeZlAo0686y1O+pz3Eg9H//5z3/o3Lkz77zzDgB5eXk888wzAGzfvp177rmH1atX07ZtW84++2wWLFjARRddxIEDB0hPT+fxxx/noYce4sEHH2TmzJmMHz+eWbNm0bt3bz755BNuuukmPvjgg7D+Xg3FWsu+g1627T3I9r0HyckvImd/EbvyAz+B6x5y9heRX+Stcl8t4l2kJMbRIsFFi4RYkuJctE9JJCneRYt4F8nxsSTHH9qWFO8iITaG+NgYEmJdxMfGEO8K3o4p3Ra8PyHORbwrhjiXUQMvAWDn/kJWfrebO8/uwzn9O3LBzOVMeOUz5t8wXP9GJOLv+QCfffYZX375JZ07d2bEiBEsX76c2267jccee4wlS5aQlpbGrl27mDZtGu+//z4tWrTgkUce4bHHHmPSpEkApKamsmbNGnJycjjhhBNYunQpvXr1Yvfu3UDgQ/MZZ5zBnDlz2Lt3L8OGDePMM88EYOXKlaxfv57k5GSGDh3K+eefzx/+8AfWr1/P2rVrw/q3iSbh/mZ59erVvPzyy6xduxav18sJJ5zAkCFDGDNmDDfccAMA999/P8899xy33norF1xwAaNHj+bSSy8FoE2bNhWOE4mEzMxM4uPjS/9/ZGZmhmW/Dk9eBE4alLsQOWTZsmWMGzcOl8tFhw4dOO200/j0008ZOnQo1113HcXFxVx00UUMHjyYI488km+//ZZbb72V888/n7PPPrshQxsBXAV8YYwJns3cB3QHsNbOAhYC5wGbgQLg2oYMqCENGDCAO+64g3vuuYfRo0dzyimnlG779NNPyczMpF27dgD84he/YOnSpVx00UXExMQwduxYAK688krGjBlDfn4+K1as4LLLLivdR1FRUeP+QtUo9vnZuruAb3MO8G1OPj/sKWDbnoNs31vItr0HK0xKtE6Ko11KAmkt4zmucyvSWgaut06Op1ViLK2T4sr8tEqKI87l6FZM0gSt+T6Q1D25dzuObp/CpNH9uPf1L/jXqmwuH9qtmkeLNLxhw4bRtWtXAAYPHsyWLVs4+eSTy4z5+OOP2bBhAyNGjADA4/GU+SAdfN/5+OOPOfXUU+nVqxcARxxxBACLFi3izTffLO2xUFhYyNatWwE466yzSE1NBWDMmDEsW7aMiy66qKF+3agR7m+WP/roIy6++OLS2RIXXHABAOvXr+f+++9n79695OfnM2rUqAofX9NxIo0hIyODxYsXh3VmEjg8eVFKUy/EQWrzbUljOvXUU1m6dCnvvPMO11xzDf/3f//H1Vdfzbp16/jvf//LrFmz+Oc//8mcOXMa5PjW2mWEzEyvZIwFbg7ncSP1fPTp04c1a9awcOFC7r//fkaOHFmn/Rhj8Pv9tGnTxhHfYPn9lu9yD/Dl9n1s2L6PzTv3823OAbbuLsAb0sSxTXIcXdok0SM1mYyjUunaNokubZLo3CaJ9q0SSG2RQHysEhHifN/k5APQu6RU5vL0bry+Zhu/X7iRM45tT1rLhEiGJxHmhPf8hIRD/wZdLhde7+HJYmstZ511FvPnz69wHy1atKjyGNZaXnvtNfr27Vvm/k8++eSwGUiakRTQUN8sl3fNNdewYMECBg0axNy5c8nKyqrXOJHGkpGREbakRZDjzyyN0cwLkVCnnHIKr7zyCj6fj5ycHJYuXcqwYcP4/vvv6dChAzfccAO/+tWvWLNmDbt27cLv93PJJZcwbdo01qxZE+nwm4zt27eTnJzMlVdeyV133VXmbzts2DA+/PBDdu3ahc/nY/78+Zx22mkA+P1+Xn31VQD+8Y9/cPLJJ9OqVSt69erFv/71LyBwErlu3boG/x2stfywu4A3Pstm0r/Xc8kzK+g/5b+M/POH3Db/M55b9i1bdxfQt2MKvz7tSP582SDeuOkk1k0+m7WTzuad205h9tXpTLngOH51ypGcO6ATg7q1oVPrJCUupErGmDnGmJ3GmPWVbP+FMeZzY8wXxpgVxphBDRXLNzn5dGyVSIuEwPc5MTGGh8f0p8DjZerb6uQvzpWSksL+/fsBGD58OMuXL2fz5s1AoC/Tpk2bDnvM8OHDWbp0Kd999x1AadnIqFGjeOqpp7AlXxh+9tlnpY9577332L17NwcPHmTBggWMGDGizLGbq+A3y1OnTg1LM8JTTz2VBQsWcPDgQfbv389bb70FwP79++nUqRPFxcW89NJLpePLPweVjRNpShw/88KgiRcioS6++GLcbjeDBg3CGMMf//hHOnbsyAsvvMCjjz5KXFwcLVu2ZN68eWzbto1rr70Wv98PwPTp0yMcfdPxxRdfcNdddxETE0NcXBzPPPMMd955JwCdOnXiD3/4A6effnppw84LL7wQCHz7tXLlSqZNm0b79u155ZVXAHjppZf4zW9+w7Rp0yguLuaKK65g0KDwfl6z1rJ5Zz7LN+/i0+/3sHrLHn7cVxiIK95Fv86tuDy9G/06taJf51b06ZCiJIQ0lLnATGBeJdu/A06z1u4xxpxLYPWhExsikG9zDnBku7LfSh/dPoWbMo/micVfc/HxXcjs274hDi1SL+PHj+ecc86hc+fOLFmyhLlz5zJu3LjSssNp06bRp0+fMo9p164ds2fPZsyYMfj9ftq3b897773HAw88wIQJExg4cCB+v59evXrx9ttvA4GE/CWXXEJ2djZXXnkl6enpAIwYMYL+/ftz7rnnNtuGneH8ZvmEE05g7NixDBo0iPbt2zN06FAApk6dyoknnki7du048cQTSxMWV1xxBTfccANPPvkkr776aqXjRJoSYyOUGUhPT7erVq2qdtxR9y3kxtOO5K5RTa/rvkSPjRs3cuyxx0Y6jAZR0e9mjFltrU2PUEhlVPRaEc3PR7CDfF3V9nffV1jMis27+HBTDh/+L4fteYFkRefWiaT3PIKhPduS3vMI+nRIwRWjqcBSO/V5rTDG9ATettb2r2ZcW2C9tbZLdfus6blFqIFT/ssFgzsz7aIBZe4v8vo474mPKPL6WXT7qSTHO/77HgmTaH6PCbe5c+eyatUqZs6cWavH6dxCnEbPb/So6rXC8e/EmnkhIlI7eQXFLNrwI+98sYPlm3dR7LOkJMQy4ug0bh3ZjpOPTqPbEVo+TaLG9cC7lW00xowHxgN07969VjsOLN3rpVPrpMO2JcS6mD5mIJf/1c1jizZx/+h+tYtaREREwsr5yQv1vBCRJqQ+sy6qUuT18d6Gn3htdTbLShIWXdokcc1JPTmrX0eO795GK3lI1DHGnE4geXFyZWOstbMJlJWQnp5eq1OGvQXFALRNjq9w+7BeRzBuWHfmLP+OCwZ3ZmDXNrXZvUjUu+aaa7jmmmsiHYaICBANyQuMZl6II1hrm1yH7UiVjYVDU3w+qlPR8/W/H/fz8qdbWfDZNvYUFNOlTRLXjejFeQM6MbBr62b3N5KmwxgzEPgbcK61NrchjrH7gAeAtslxlY6599xjWLzxJ+597QsW3DxCfWCaieb4HhMu0XxuISLO5vjkBQas5l5IhCUmJpKbm0tqamqTOZmx1pKbm0tiYmKkQ6m1pvh8VCf0+bLWkrUph2eXfsuKb3KJd8Vw1nEdGJvejRFHp6l3hUQ9Y0x34HXgKmvt4UsmhMmegpLkRYuKZ14AtE6K46EL+3Pj31fz5OKvuXNU30rHStPQHN9jwiWazy1ExPkcn7wwoLoRibiuXbuSnZ1NTk5OpEMJq8TERLp27RrpMGqtqT4f1YlPSGDtbhfj31jK1zsDyzvee+4xXJ7ejSOq+PAl4jTGmPlAJpBmjMkGJgNxANbaWcAkIBV4uuTDo7chGv2VJi8qKRsJOqd/Ry4b0pWnszaT2bcd6T2PCHco4iDN9T0mXKL13EJEnM/5yQslvMUB4uLi6NWrV6TDkBLN7fnw+S1vrdvOjLc3sSW3gGM7teLxsYM4f0BnTWGXqGStHVfN9l8Bv2roOPYEy0ZaVF42EjT5guP45Lvd3P7PtSy87RRSEqt/jESn5vYe42TGmERgKZBA4HPLq9bayeXGJBBYdnkIkAuMtdZuaeRQRaQRRMVZryZeiEhzteR/Ozn3iaVMeGUtiXEu/nZ1OgtvO5mLj++qxIVIPe0padjZJqn6mUstE2J5fOwgtu05yOQ3v1Rdv0jjKALOsNYOAgYD5xhjhpcbcz2wx1p7NPA48Egjxxg2LVu2BGD79u1ceumlEY4m8mbNmsW8efMiHYY4iPNnXmB0giAizc73uQeY+vYG3t+4k15pLXhq3PGcP6ATMepnIRI2uw94SEmIrXEicEiPI7j1jN48sfhrhvYMrEQiIg3HBj4EBJfpiiv5Kf/B4EJgSsn1V4GZxhhjo/gDROfOnXn11Vcb9Bher5fY2Io/Cla1rSastVhriYmp35csN954Y70eLw5ibVhKKhz/tZ0xaLUREWk2irw+HntvE2c9vhT3N7lMPPcY/jvhVH42qLMSFyJhtrfAQ5salIyEum1kb07t047J//6SdT/sbaDIRCTIGOMyxqwFdgLvWWs/KTekC/ADgLXWC+QR6JlTfj/jjTGrjDGrnN7PZMuWLfTv3x+AuXPnMmbMGM455xx69+7N3XffXTpu0aJFZGRkcMIJJ3DZZZeVLsf+0EMPMXToUPr378/48eNLvwjOzMxkwoQJpKen88QTT5Q55pQpU7jqqqsYMWIEV111FTk5OVxyySUMHTqUoUOHsnz5cgBycnI466yzOO644/jVr35Fjx492LVrF1u2bKFv375cffXV9O/fnx9++IFHH32UoUOHMnDgQCZPDlT7HDhwgPPPP59BgwbRv39/XnnlFQDuvfde+vXrx8CBA7nzzjtLY/rTn/4EwNq1axk+fDgDBw7k4osvZs+ePaW/0z333MOwYcPo06cPH330UYM8J1IP7dtDTAzk5dV7V1Ew80JlIyLSPHyRnccd/1rLpp/yuWBQZ+4771g6tlbHdpGGsr/QS0pC7ZIXrhjDE2MH87OZy/jN31fz1q0nk9oyoYEiFBFrrQ8YbIxpA7xhjOlvrV1fh/3MBmYDpKenV/3xYsIEWLu2LuFWbvBgmDGjTg9du3Ytn332GQkJCfTt25dbb72VpKQkpk2bxvvvv0+LFi145JFHeOyxx5g0aRK33HILkyZNAuCqq67i7bff5mc/+xkAHo+HVatWVXicDRs2sGzZMpKSkvj5z3/O7bffzsknn8zWrVsZNWoUGzdu5MEHH+SMM85g4sSJ/Oc//+G5554rffzXX3/NCy+8wPDhw1m0aBFff/01K1euxFrLBRdcwNKlS8nJyaFz58688847AOTl5ZGbm8sbb7zBV199hTGGvXsPTwxfffXVPPXUU5x22mlMmjSJBx98kBklf0+v18vKlStZuHAhDz74IO+//36d/s4SZtaCy3VoJkKrVvXepfOTF8Zo5oWINGnFPj9PfbCZvyzZTGqLeOZck84Zx3SIdFgiTd7BYh/J8a5aP65ti3hmXTmEMc+s4KaX1jDv+mEkxNZ+PyJSc9bavcaYJcA5QGjyYhvQDcg2xsQCrQk07mwyRo4cSevWrQHo168f33//PXv37mXDhg2MGDECCCQlMjIyAFiyZAl//OMfKSgoYPfu3Rx33HGlyYuxY8dWepwLLriApKQkAN5//302bNhQum3fvn3k5+ezbNky3njjDQDOOecc2rZtWzqmR48eDB8eaEmyaNEiFi1axPHHHw9Afn4+X3/9Naeccgp33HEH99xzD6NHj+aUU07B6/WSmJjI9ddfz+jRoxk9enSZuPLy8ti7dy+nnXYaAL/85S+57LLLSrePGTMGgCFDhrBly5ba/Gmlofj9gcRFUFFRWMpGnJ+8AKzmXohIE/VjXiG3zl/Dp1v2cPHxXZj8s360qWbZRhEJjwKPj5TEup0K9e/SmkcvHchvX17LPa9+zuNjB2O0RJpIWBlj2gHFJYmLJOAsDm/I+SbwS8ANXAp8UO9+F3WcIdFQEhIOze5yuVx4vV6stZx11lnMnz+/zNjCwkJuuukmVq1aRbdu3ZgyZQqFhYWl21u0aFHpcUK3+f1+Pv74YxITaz4DNPTx1lomTpzIr3/968PGrVmzhoULF3L//fczcuRIJk2axMqVK1m8eDGvvvoqM2fO5IMPPqjxcYN/n+DfRhqO2+0mKyuLzMxMMjIyDrsNgMcDIf9mKS6GevRQCeX4nheo54WINFHLvt7F+U9+xJfb9zFj7GAeHztYiQuRRlRY7CMpru4zJi4c3IW7RvVlwdrt/HnRpjBGJiIlOgFLjDGfA58S6HnxtjHmIWPMBSVjngNSjTGbgf8D7o1QrI1q+PDhLF++nM2bNwOBXhKbNm0qTVSkpaWRn59f58afZ599Nk899VTp7bUlZTQjRozgn//8JxCYXRHsPVHeqFGjmDNnTmkfjm3btrFz5062b99OcnIyV155JXfddRdr1qwhPz+fvLw8zjvvPB5//HHWrVtXZl+tW7embdu2pf0sXnzxxdJZGNJ43G43I0eO5IEHHmDkyJHMnj27zG232w35+WUSF3986CHcn34athiiYuaFiEhTYq3l2Y++Zfq7X3F0u5Y8c+UJHN0+JdJhiTQ7B4t9JNWhbCTUTZlH8cPuAmYu2UyHVglcldEzPMGJCNbaz4HjK7h/Usj1QuCy8mOaunbt2jF37lzGjRtHUVERANOmTaNPnz7ccMMN9O/fn44dOzJ06NA67f/JJ5/k5ptvZuDAgXi9Xk499VRmzZrF5MmTGTduHC+++CIZGRl07NiRlJSU0iRF0Nlnn83GjRtLv41v2bIlf//739m8eTN33XUXMTExxMXF8cwzz7B//34uvPBCCgsLsdby2GOPHRbPCy+8wI033khBQQFHHnkkzz//fJ1+L6m7rKwsPB4PPp8Pj8fDa6+9Vub2yrfeIuOkk0rHt0xMpPDBB4mfPp3FixcfmplRDyZSqwilp6fbyprFhBr04CIuGtyZBy/s3whRiQiAMWa1tTY90nFAzV8rooXX52fSm1/yj0+2ct6AjvzpskEkxzs+jyxSISe9VkDtXy+G/v59zjy2PdPHDKzXcYt9fm58cTWLv9rJHy8ZyOVDu9VrfyJNkZNeLyp6rdi4cSPHHntshCKKHkVFRbhcLmJjY3G73fzmN78pnZXhZHp+6y8488Lj8RAfH8+MGTOYMGECHo+HvrGxfFmSRAOY/vvf88CkSfh8PlwuF1OnTmXixIk1Ok5VrxWOP2M2RquNiEjTsL+wmJv/8RlLN+Xwm8yjuOvsvlr+VCSCCj0+EutRNhIU54rhL784gRvmreKe1z8nIS6GCwd3CUOEIiLOsnXrVi6//HL8fj/x8fE8++yzkQ5JGklGRgaLFy8u0+NiwIABbHzpJa77y18ODbSWTLeb+Ph4PB4PsbGxbN26FbfbXe/ZF85PXqCeFyIS/fYWeLh6zkq+3L6PP4wZwBXDukc6JJFmr66rjVQkMc7F7KvSuXbuSm5/ZS1+a7n4+K5h2beIiFP07t2bzz77LNJhSIRkZGSUSUBkFBSQUS5xERy3ePFi5s2bx5w5c3j22Wd54YUX6l0+4viGncYYrTYiIlFtV34RV8z+mK9+3M+zVw9R4kLEATxeP16/rVfDzvKS4l0898uhnNgrldtfWcfzy78L275FpOFFqpxeGpae1/Byu91Mnz6d72+6Cc4889CGcn/njIwMunfvjs/nK+2LkZWVVa9jVzvzwhjTDZgHdCBQwTHbWvtEuTGZwL+B4Lv069bah+oVWXDf4diJiEiE7NxXyLhnP2bb3oPM+eVQTu6dFumQRITArAsgLGUjoVokxPL8tUO5bf5nPPjWBvYc8HD7WX20jKqIwyUmJpKbm0tqaqr+vzYh1lpyc3NrteSrVC7Y9+LPhYX0CE1WVJIgyszMLC0fiY+PJzMzs17Hr0nZiBe4w1q7xhiTAqw2xrxnrd1QbtxH1trR9YqmEkqWiUg02lvg4arnVrIjr5AXrh3GiUemRjokESlRWJK8aIiGuYlxLp7+xQlMfP0LnvxgM9/vLuCRSwaGPVEiIuHTtWtXsrOzycnJiXQoEmaJiYl07aoyvnDIysrijYMHGRV6ZyUf1t1uN1lZWcyYMYPc3NzSPhn1Ue07trV2B7Cj5Pp+Y8xGoAtQPnnRINSwU0Si0YEiL9fO/ZTvdh3g+WuHKnEh4jAFnkDyIim+YSpoY10x/PHSgfRITeZPizaxZdcBZl+dTodW+vZPxIni4uLo1atXpMMQcbR7f/e7MpUR7hUrqCgdUX5lknAtlVqrd2xjTE8Cay1/UsHmDGPMOmPMu8aY4yp5/HhjzCpjzKqaZzWNZl6ISFTxeP3c+PfVrPthL0/9/HhGHK1SERGnORhMXsQ1XO9yYwy3nNGbWVcO4eud+VwwcxmfbtndYMcTERFpMMZgQj6Yu1esqDQhkZWVhcfjCVuvi6AaJy+MMS2B14AJ1tp95TavAXpYawcBTwELKtqHtXa2tTbdWpverl27Gh4XNPdCRKKFtZbfvfEFH329i0cuGcio4zpGOiQRqUCw50VSmFYbqco5/bwEkwcAACAASURBVDvy2m9OIiHWxdi/unly8df4/Dq3ERGRhhVsrul2u+u3o/J9YKytciZFsNeFy+UKS6+LoBp93WCMiSOQuHjJWvt6+e2hyQxr7UJjzNPGmDRr7a76BqilUkUkmvzto+/41+psbjvjaC5L7xbpcESkEodmXjROH4pjO7XindtO5v4F63nsvU2s+GYXj10+mM5tkhrl+CIi0ryErXSjgsRFdYJLpWZlZYWl10VQtTMvTKDd7nPARmvtY5WM6VgyDmPMsJL95oYjQGOUvBCR6LB44088/O5GzhvQkQln9ol0OCJShYOlDTsbr4lmSmIcM8YO5k+XDeLz7DzOfnwpL378PX7NwhARkTALS+lGHRIXQRkZGUycODFsiQuoWdnICOAq4AxjzNqSn/OMMTcaY24sGXMpsN4Ysw54ErjChmlBXYPBqmxERBzuu10H+O3La+nfuTV/vmwwMTFaZk3EyRpqqdTqGGO4dEhX/jvhVAZ3a8MDC9ZzxeyP2bxzf6PGISIiTVu9SzdqkLgoX5YStjKVStRktZFlQJVn4dbamcDMcAUVSjMvRMTpCot93PTSGmJdhr9eNaRRauhFpH4KPY3X86Ii3Y5I5sXrh/Hq6mymvr2BUTM+4qrhPZhwZm/aJMdHJCYREWk66lW6UcPERWhZyowZM5gwYULYVxgJ1XAttsPEoHadIuJsU9/ewMYd+5hzTbrq10WihMfnByDOFblZUsYYLkvvxunHtOfx9zYxz72FNz7bxm9H9uYXw7uTEKtEqIiI1F1GRkbtEwg1LBUpX5by2muvHVamEu7kRcMsbh5GxmipVBFxrrfWbeelT7by69OO5IxjOkQ6HBGpIW9J8iLeFflTobSWCfz+4gEs/O0pDOjSmofe3kDmo1nMc2+hsKS8RUREpMHVosdF+bKUSy65pEFWGAnl+JkXgHpeiIgj7cg7yH2vf8GQHm258+y+kQ5HRGqh2Bc4t4h1QPIi6JiOrXjx+mEs35zLE4s3MenfX/KXJZu54ZQjuXxoN1olxkU6RBERaULcbvehspKTTiq7sZoZBBWVpQwYMCDsK4yEcnzywqhuREQcyFrLPa99gc9aHr98MHEO+gAkItVzQtlIRYwxnNw7jRFHp+L+JpcZi79m2jsbeey9TVw6pCu/PKknR7VrGekwRUQkyoX2rPD6ys7ym/7ww2S63dUmIMqXpdSpTKUWoiN5ISJRwRgzBxgN7LTW9q9ge2vg70B3Aq8/f7LWPt+4UYbHy5/+wNJNOUy98Di6pyZHOhwRqSVvycyLuBhnJh6NMZx0dBonHZ3G+m15PL98Cy+v/IF57u/JODKVMSd04dwBnWiZ4PhTORERcaBgz4ryiYvkpCQ8DzzQYE0368OZ79jlaOKFSNSYC5xTxfabgQ3W2kFAJvBnY0zUtdXP3lPAtLc3cNJRqfzixB6RDkdE6qDY58cVY6JiWeP+XVrz58sHsWLiGdx5dh925B3krlc/Z+i097n9lbVk/W8nHq8/0mGKiEgUyczMrHDGRfmmm07i+HS9wWDVsVMkKlhrlxpjelY1BEgxxhigJbAb8DZCaGFjreW+N9YD8MdLB0bFBx8ROVyxz++4kpHqpLVM4JYzenPz6UezZuseXluzjbfWbeeNz7aRkhBL5jHtGXVcB07r044U9ccQEZEqVNTjItPtJj4+vnS504Zoulkfzk9eGM28EGlCZgJvAtuBFGCstTaqvi58d/2PLN2Uw+Sf9aNrW5WLiESrYp+N2l41xhiG9DiCIT2OYNLofqz4Zhf/Xf8T72/8ibfWbSc2xnBC97aMODqNk3unMrBrm6j9XUVEpAFUsqpIRU04ncT5yQuqbXQqItFjFLAWOAM4CnjPGPORtXZf6CBjzHhgPED37t0bPcjK5Bd5eeitDfTr1IqrhqtcRCSaBWZeRP8H+sQ4F2cc04EzjumAz29Z/f0ePvhqJyu+2cWMxZt4/H1omRDLCT3acny3NhzfvQ3Hd2tL62TNzBARaZaqWQ61oZtu1ofzkxfGaOaFSNNxLfAHG6gF22yM+Q44BlgZOshaOxuYDZCenu6Yl4An3t/Ej/sKefrKExy1vKKI1F40lo1UxxVjGNbrCIb1OgKAvQUe3N/ksmzzLlZ/v4enPvgaf8kr6pHtWjCoaxuO6ZjCMZ1acWzHFNqlJGDUKV1EpOkKfY1v3Rr27o1cLHXg/OQFqOeFSNOxFRgJfGSM6QD0Bb6NbEg1s3nnfuYs38K4Yd04oXvbSIcjIvVU7LPEOnSlkXBpkxzPuQM6ce6ATkBg9tjn2Xv5bOte1ny/B/c3ubzx2bbS8W2T4zimYyuOat+Cnqkt6JHagl5pyXRtm0xinCtSv4aIiIRDaOJixAhYtixysdSR45MXqOeFSNQwxswnsIpImjEmG5gMxAFYa2cBU4G5xpgvCOQm77HW7opQuLXyh3e/IjnOxZ1n9410KCISBsU+P/GxTTt5UV7LhFhOOiqNk45KK71vzwEPX/24n//9uI+vftzPVz/u5611O8g7WFw6xhjo3DqJrm2T6NwmiY6tE+nUOpFOrZPo1DqRjq0TOSI5Xg2MRUScKjRxcf318Le/RS6WenB88sKAshciUcJaO66a7duBsxspnLBxf5PL+xt3cvc5fUltmRDpcEQkDJpi2UhdtG0RT8ZRqWQclVrm/r0FHrbkFrBl1wG25B5gy64DbNt7kE+37OanfYUU+8qenMXGGNq2iCe1RTxHlPwEridwRMt42ibHkZIYR0piLK0SY0uvJ8W5VKoiItKQQl9jf/97uO++yMVST85PXhiDVfZCRCLE77c8vHAjnVsnct2IXpEOR0TCpDmUjdRHm+R4BifHM7hbm8O2+f2WXQeK+DGvkB15hfyYV8hP+wrZfcBD7gEPuw94+HL7PnLzi9hXWPVq2LExhpaJsaQkxpKSEEfLhFgS410kxcWQFOciKd5FYlzgJ6nkJ7A98JMQG0NcbAxxLkO8K4ZY16Hrca6y2+JKf4wSJiLSPIS+1r34Ilx5ZeRiCQPnJy/QaiMiEjlvfb6dL7bl8fjYQar5FmlCin1+4ppZ2Ui4xMQY2qck0j4lkYFdqx5b7POz54CHPQXF7C8sZn+hl32FxeQXedlf6C29L3g9v8jLvoPF7Nzn42Cxj4OewGVhse+w2R71ERtjiHUZXMYQE2NwxYRcN4HbMTEcfl/pNoPLUOY+V0wgKWIIfF4IXB66DQZjIMaAKbluSq5TbnyMOXSd0PEcekwg93b4/cHbQZUlakLvNiGPKHt/JeNLbiTGufi/s/rU7o8vIg3PWghN0L/7LpxzTuTiCRPnJy+MkhciEhlen5/H39vEsZ1aceGgLpEOR0TCqNjnJ15lIw0uzhVD+1aJtG+VWO99Ffv8FBb7KCwOXIYmNYp9/tIfjzdw2+v3U+y1eEK2FfssHq+/ZLvFV/Ljt2UvfX4OXbcWf7lxPkvpfT4b2KfP2sBqLjYwZ9hasNjApaXkPlv2/uB9AOVu++2hxxJyf+jj/SUPPGxfBB8Tev3QjTKn1rbCq5WOD91n66Q4JS9EGpnb7SYrK4vMzMyKlzQtLISkpEO3P/0U0tMbL8AG5PzkBSobEZHIWLB2O1tyC5h91RA1ohNpYrwqG4k6wbKPlPrnQUREopLb7WbkyJF4PB7i4+NZvHhx2QRGTg60b3/o9ubNcNRRjR9oA3H8u7ZmXohIJBT7/Dy5+Gv6d2nFWf06RDockSbFGDPHGLPTGLO+ku3GGPOkMWazMeZzY8wJ4Y7Bo7IRERGJMvPmzaOwsBCfz4fH4yErK+vQxo0byyYudu1qUokLiILkhYhIJLyxZhtbdxcwYWQfNXYTCb+5QFXFt+cCvUt+xgPPhDsAlY2IiEg0cbvdPP/886UlXS6Xi8zMzMDGJUugX79DgwsLITX18J1EuahIXmjihYg0pmKfn6eWfM3Arq0ZeWz76h8gIrVirV0K7K5iyIXAPBvwMdDGGNMpnDGobERERKJJVlYWXm9gBSdjDNddd12gZGTOHDjjjEMD/X5ISMDtdjN9+nTcbneEIg4/5/e8MEZlIyLSqBZ+sYMfdh9k0ujjNOtCJDK6AD+E3M4uuW9HuA6g1UZERCSaZGZmEh8fX9rv4uqrr4a774ZHHz00qOSDc7W9MaKU85MXgOZeiEhjsdby1w+/5ah2LRh5jGZdiDidMWY8gdISunfvXuPHeXx+4tSIV0REokRGRgaLFy8+tNLIlCmwaNGhASHf+GdlZeHxeMr0xlDyohGoYaeINKblm3PZsGMfj1wyQCuMiETONqBbyO2uJfcdxlo7G5gNkJ6eXuMzBq/PEufSzAsREYkeGRkZgSREfDwUFx/aUO4Dc/lZGqW9MaJcdCQvIh2EiDQbf136De1SErjo+C6RDkWkOXsTuMUY8zJwIpBnrQ1byQgEy0aUoBQRkShTvqS5gm/6D5ul0QRmXUA0JC8wpR1VRUQa0pfb8/jo613cfU5fEmJdkQ5HpMkyxswHMoE0Y0w2MBmIA7DWzgIWAucBm4EC4Npwx+Dx+dWwU0REoksNEhdBpbM0mhDnJy8080JEGsnzy7eQHO/iFyf2iHQoIk2atXZcNdstcHNDxuD1WeLVsFNERKJFucSFe8UKmlZqonrOT16gnhci0vD2HPDw1rrtXJbeldZJcZEOR0QaWLHPT5xLZSMiIhIFyiUuYl0uplbRhNPtdpeWjABNpnzE8ckLjNHMCxFpcP9c9QNFXj9XDe8Z6VBEpIFZa/H6rcpGRETE+SpIXMTHx5Oamsr06dMPS0qELpPqcrkwxuD1epvEkqmOT14EZl4ofSEiDcfvt/z9k+8Z1usI+nZMiXQ4ItLAin2B8wqVjYiIiKNVUCoyNSuL1NRUJkyYULqaSGhSInSZVL/fDwQ+TzeFJVMd/65dvieJiEi4fbgphx92H+Sq4ep1IdIcFPsCJ3OxWg5ZREQixO12M336dNxud8UDKmjOmZGRwcSJE8nNzS1NUASTEkHBZVJdLhdxcXGl15vCkqlRMvMi0lGISFM2z72FdikJjDquY6RDEZFGEExexLkc/x2OSLNmjOkGzAM6EOjhP9ta+0S5MZnAv4HvSu563Vr7UGPGKVJboaUdFZZzVLOqSDBBEXx8aFKi/DKp0Ix6XtTwRcMATxBY1qwAuMZauyYcARpNvRCRBrRt70GyNuVwy+lHawq5SDMRLBtRw04Rx/MCd1hr1xhjUoDVxpj3rLUbyo37yFo7OgLxidRJaGnHYeUcNVgOtXyConxSovwyqdGetAiqycyLmrxonAv0Lvk5EXim5DIsrFp2ikgDeX11NtbC5endIh2KiDQSf8mJYIzKRkQczVq7A9hRcn2/MWYj0AUon7wQiSqVzpyoQeIiqHyCojmoNnlRwxeNC4F5Jeuyf2yMaWOM6VTy2HpR2YiINBRrLa+uyWb4kUfQ7YjkSIcjIo3E5w+cWKjnhUj0MMb0BI4HPqlgc4YxZh2wHbjTWvtlBY8fD4wH6N69e8MFKs1K6JKktUkkVDhzohaJi+aqVj0vqnjR6AL8EHI7u+S+MsmLurxoGKPnTUQaxsrvdvN9bgG3ndE70qGISCMKJi9iVJoqEhWMMS2B14AJ1tp95TavAXpYa/ONMecBCwjMBi/DWjsbmA2Qnp6uTxdSb7Nnz+bmm2/G7/eTkJBQ62VIy8ycqGBVkeY1p6JmalzgXc2LRo1Ya2dba9Ottent2rWr2XExKhsRkQbxr9XZtEyI5dwBatQp0pwEkxcuzbwQcTxjTByBzyAvWWtfL7/dWrvPWptfcn0hEGeMSWvkMKWZcbvd3HLLLXi9Xvx+P0VFRWVW/KiVcomLWJeLkSNHVr4KSQWxVLlqSRNSo5kX1b1oANuA0ILxriX31Z9mXohIAzhQ5GXhFzv42cDOJMc7fuElEQkjn1XyQiQalCwK8Byw0Vr7WCVjOgI/WWutMWYYgS9ncxsxTGmGsrKy8Pl8pbdjYmIqXIa02rKSChIXFTbxrES1q5Y0MTVZbaTaFw3gTeAWY8zLBBp15oWj3wWU9LwIx45EREK8u/5HCjw+LkvvGulQRKSR+VU2IhItRgBXAV8YY9aW3Hcf0B3AWjsLuBT4jTHGCxwErijpwyfSYDIzM0lISKCoqAiXy8XMmTMPSxrUdjlU94oVxIeMrygZUl6Vq5Y0QTX5urEmLxoLCSyTupnAUqnXhitAY8D6w7U3EZGAN9dtp9sRSQzp0TbSoYhII9PMC5HoYK1dRuC7zKrGzARmNk5EIgHVLVUKtV8ONQOq3Wd5la5a0kTVZLWRmrxoWODmcAUVKtDzQtkLEQmf3Pwilm/exa9PPRKjb15Fmh017BQRkfqqbqnS8omF1NRUpk+fzsT77is7MGSiUG2XP61JEqUpcXyht1YbEZFwe3f9j/j8lp8N6hzpUEQkAvwl34lo5oWIiDSU0MRCamoqEyZMoODgwbKDwvBBt7YJj2hW49VGIsUY9bwQkfB6a912jm7fkmM6pkQ6FBGJgENlIxEOREREmrSMjAwmTpxIbm5ugyQumhvHv20bDOq5IyLh8mNeISu37OZnAzurZESkmfKVTL1wxTj+NEhERJqA8qUi7hUrIhRJdHP8u7ZmXohIOL3zxQ6shdGDOkU6FBGJEF+wbEQJTBERaWjl3mtm//WvzabMI9wcn7wQEQmnt9Zt57jOrTiqXctIhyIiEVLasFNnQSIi0pBCEhd+INblIjc3N3LxRLmoeNtW1YiIhMNP+wpZ+8NezhugWRcizZk/2PNCMy9ERCQM3G4306dPx+12l14PTVx8EBNDvMtVZjnT0MdIzUTBaiNGZSMiEhbvbfgJgLP6dYhwJCISScGZF1ptRERE6svtdjNy5Eg8Hg8ulwsXUODxHBrwu9+RdP75TA1ZzjT0MfHx8SxevFilJDXg/OQFaOqFiITFext+okdqMr3bq2REpDkLrjYSo+SFiIjUU1ZWFh6PB5/PRyufj90h29644gounjaNDCiTnAh9jMfjISsrS8mLGnB82YgadopED2PMHGPMTmPM+irGZBpj1hpjvjTGfNhYseUXeXF/k8tZx3bQKiMizZzfr7IREREJj8zMTOLj4+kfE1MmcTEiIYGOt91W5WNc5UpJpGrOT16giRciUWQucE5lG40xbYCngQustccBlzVSXHz4vxw8Pr9KRkREZSMiIhK2nhMZGRm8ce21fFGyDDfAuFNP5ZdPPlnpbIqMjAwWL17M1KlTVTJSC84vGzEGq7kXIlHBWrvUGNOziiE/B1631m4tGb+zMeICeG/Dj7RNjmNIj7aNdUgRcajShp1KXoiINEu16TnhdruZN28eAFdfffVh47KvuYZRL7xQertVXBwFy5fz708/ZcCAAVUmMJS0qB3NvBCRxtQHaGuMyTLGrDbGXF3RIGPMeGPMKmPMqpycnHof1Ovzs+R/OZxxTAdiXY5/2RORBubVzAsRkWatop4TFXG73Zx++unMmjWLWbNmkZmZWXamxskn0zUkcRED5Hu91e5X6sbxZ/HGKHkh0oTEAkOA84FRwAPGmD7lB1lrZ1tr06216e3atav3Qddl55F3sJjTj6n/vkQk+gXLRmLU80JEpFmqac+JYJIjqLi4+FBCwhhYvrx0W6zLRXxCAnFxcepl0UAcXzYCWipVpAnJBnKttQeAA8aYpcAgYFNDHnTZ17swBkYcldaQhxGRKKGyERGR5i3YcyIrZPnSigSTHEVFRQDExcUFEhLlkt/uFStKl0IFqt2v1I3jkxeBmRdKX4g0Ef8GZhpjYoF44ETg8YY+6LLNOfTv3Jq2LeIb+lAiEgV8JT3VtNqIiEjzVZOeExkZGSxZsqRsz4uTTio7yNrDlkJV0qJhOD95EekARKTGjDHzgUwgzRiTDUwG4gCstbOstRuNMf8BPgf8wN+stZUuqxoO+UVePtu6l/GnHtmQhxGRKBJcKjXG8cWzIiLS0Nxud5UzJcokOconvfUle6NyfvJCPS9Eooa1dlwNxjwKPNoI4QDwybe5eP2Wk3urZEREAnwqGxEREWq36khNEhfVJUKkfpyfvNDcCxGph0++2028K4YTumuJVBEJCDbsVNmIiEjzVtGqI8GkQ5lERAWlIuXVKhEideL45AWAVctOEamj1d/vYUDX1iTGuSIdiog4RLBhZ4xmXoiINGvBhpzBhEOw4WZoIsLr85V9UCVlAVUlQiQ8HJ+8UNmIiNRVYbGPL7LzuHZEz0iHIiIO4vUFTixilbwQEWlyalO6UdmqI8FERE0TF1B5IkTCJzqSF5EOQkSi0pfb8/D4/JzQQyUjInKIZl6IiDRNdSndqGjVkczMzMMSF9MffphMt7vS/dV0+VWpO+cnLzBaKlVE6mTN93sB1O9CRMpQzwsRkaYpXKUb5XtcJCcl4XnggWoTIjVZflXqzvmLhGnmhYjU0cYd++jQKoF2KQmRDkVEHESrjYiINE3B0g2Xy1X30o1yie3pDz98WEJEIiMKZl6g7IWI1MlXP+6nb8dWkQ5DRBzGXzLzIkYzL0REmpR6l25UsBxqptutXhYO4fiZFzHGKHchIrXm9fnZnJPPMR1TIh2KiDiMzx+41MwLEZGmJyMjg4kTJwIwffp03G53zR5YQeIiuL/FixczdepULX8aYY6feRFjDjXWEhGpqS25B/B4/fTtoOSFiBMZY84BngBcwN+stX8ot7078ALQpmTMvdbaheE4drBsRLkLEZGmqdaNOytJXASpl4UzOH7mhTFGyQsRqbWvftwPQF/NvBBxHGOMC/gLcC7QDxhnjOlXbtj9wD+ttccDVwBPh+v4fr8lxgTOMUREpOmpqHFnpapJXIhzREHyQv9+RKT2Nv24nxgDR7dvGelQRORww4DN1tpvrbUe4GXgwnJjLBBsWtMa2B6ug/usJTbG8adAIiJSRzVp3Ol2u5W4iDJRUDZi9G9IRGrt+90FdGqdRGKcK9KhiMjhugA/hNzOBk4sN2YKsMgYcyvQAjizoh0ZY8YD4wG6d+9eo4P7/BblLkREmq7qGne63e7DlkPVh07nc3zywqCeFyJSe9l7DtLtiKRIhyEidTcOmGut/bMxJgN40RjT31rrDx1krZ0NzAZIT0+v0QmDz29xqWRERKRJq6pPRfnExfSHH2ZiYwQl9VLt9w7GmDnGmJ3GmPWVbM80xuQZY9aW/EwKa4CaeSEidfDD7gK6tU2OdBgiUrFtQLeQ211L7gt1PfBPAGutG0gE0sJx8MDMCyUvRESapXLJ6+SkpErLSmq1Wok0uJrMvJgLzATmVTHmI2vt6LBEVI7RaiMiUkuFxT527i+i2xFKXog41KdAb2NMLwJJiyuAn5cbsxUYCcw1xhxLIHmRE46D+63VMqkiIs1RucTF9IcfZnElZSW1Wq1EGkW1yQtr7VJjTM+GD6VixhiUuhCR2sjecxBAZSMiDmWt9RpjbgH+S2AZ1DnW2i+NMQ8Bq6y1bwJ3AM8aY24n0LzzGmvD822GykZERKKL2+2utH9FjVXQnLOyUpGKVitR8iLywtXzIsMYs45AJ/A7rbVfhmm/xBgI07mKiDQTP+YVAtCptZIXIk5lrV0ILCx336SQ6xuAEQ1xbL9V2YiISLQIyyyIWq4qElytJHjMispKpPGFI3mxBuhhrc03xpwHLAB6VzSwLh3BA2UjYYhSRJqNnPxA8qJdSkKEIxERJ9LMCxGR6FHvWRA1TFyUn91R1WolEhn1Tl5Ya/eFXF9ojHnaGJNmrd1VwdhadwQPNOxU9kJEam7Xfg+g5IWIVMznRz0vRESiRL1mQdQicVHR7A4lLZyl3qucG2M6GhP4V2GMGVayz9z67jcoxhjNvBCRWsnJLyI+NoaUBMevBi0iEeDz+5W8EBGJEsFZEFOnTq1dyUgtSkUqmt0hzlPtmb0xZj6QCaQZY7KByUAcgLV2FnAp8BtjjBc4CFwRroZaQVptRERqY9f+Itq1TMBoWriIVMBnNfNCRCSa1HoWROg5YOvWsHdvhcOCpSKpqanqcREFarLayLhqts8ksJRqg4gxBi03IiK1kZNfRJpKRkSkEn6/RbkLEZEmqKgIEhMP3b7mGnj++QqHli8VmTFjBrm5uepx4WCOn1MdaNip7IWI1FzO/iK6ttVKIyJSMZ/fauaFiEhTs20bdO166PZTT8Ett1Q6vHypSG5uLhMnVrZ4qjhBvXteNLQYrTYiIrW0K99DWkvNvBCRivmsDczsFBGRpuGjj8omLpYurTJxAYcagbpcLpWKRAnHz7yIMQaruhERqSFrLXkHPbRtER/pUETEofyaeSEiElHllyWtl5kz4dZbD93OzoYuXap9mJZDjT6OT16gmRciUguFxX6KfZaUROe/vIlIZPiskhci0cAY0w2YB3Qg0AVvtrX2iXJjDPAEcB5QAFxjrV3T2LFKzVW2LGmd/PznMH/+oduFhZBQs9m3YU2gSKNw/Nm9GnaKSG3sLywGoFViXIQjERGn8vlVNiISJbzAHdbaNcaYFGC1MeY9a+2GkDHnAr1Lfk4Enim5FIeqaFnSOiUPWreGffsO3a5Fn8SwJlCk0URJzwtlL0SkZvaVJC8080JEKuO3lljNvBBxPGvtjuAsCmvtfmAjUL4e4EJgng34GGhjjOnUyKFKLYT2moiNjWXr1q243W7cbjfTp0/H7XZXvxNj6py4gIoTKOJ8jj+7NxglL0SkxvYVegFolaSZFyJSMa/PEqPkhUhUMcb0BI4HPim3qQvwQ8jt7JL7dpR7/HhgPED37t0bKkypgWCviXnz5jFnzhyeffZZ5syZgzEGr9db/UyI8jPnavhZMbRMJJhACc68ULPO6OD45EWMqkZEpBb2HVTZiIhULTDzwvGTT0WkhDGmJfAaMMFau6+68RWx1s4GZgOkp6fr44UDfPvtt3i9Xvx+P36/Hwg0Xi9fSlKmZ1L4lQAAIABJREFUN8VJJ5XdSS0SF+XLRNSsM/o4PnlhjMHawD9ko/pUEalG6cwLlY2ISCV8fktCrM4pRKKBMSaOQOLiJWvt6xUM2QZ0C7ndteQ+cahgIqGoqAi/309MTAyxsbFlZl4EZ0KEJh28Pl/ZHdVidn5FZSITJ05U0iLKOP7sPpivsPbwGUIiIuWVNuxU2YiIVMJnUdmISBQoWUnkOWCjtfaxSoa9CdxijHmZQKPOPGvtjkrGigMEEwnBxMWZZ57JlClTSreFzoQIjq1P4gJQmUgT4fjkRbAbuOZ2iUhN7DsYmHmhhp0iUhlrLcpdiESFEcBVwBfGmLUl990HdAew1s4CFhJYJnUzgaVSr41AnFIL5RMJU6ZMKU1WlJ8JkZmZWafERfllUIN9NlQmEt0cf3YfPLfwW4sLnWmISNX2FxYTG2NIinNFOhQRcSi/tbg0nVPE8ay1y6DqDwDWWgvc3DgRSThUlUg4LOlQhx4XFfW3gMNndUj0cXzyIjitUwuOiDifMWYOMBrYaa3tX8W4oYAbuMJa+2o4Y9hXWExKYqx65IhIpfx+laKKiERScDZEqPJJh4KDB8s+qIYfCMv3t5g3bx4vvPBCmWSGEhjRyfGttoMnF1ouVSQqzAXOqWqAMcYFPAIsaogA9hd6SdFKIyJSBb+agIuIOE5o0qGuiQs4VJbicrmIj48HOKxZp0Qnx8+8MGjmhUi0sNYuLVmHvSq3EugaPrQhYjjo8ZEcr5IREamael6IiDhLMOlQn8QFHF6WApSZeaFmndHL8cmL4MmFVctOkahnjOkCXAycThXJC2PMeGA8QPfu3Wt1jEKvnwT1uxCRKvitLW0ILiIizpCRkVHvxEXovkJLQ9Sss2mIguRF4OTCr9yFSFMwA7jHWuuvasq2tXY2MBsgPT29Vv/7Cz0+kuIcXxEnIhHk1/LrIiLOU/6FOYxT7yvqsSHRx/HJC/W8EGlS0oGXSxIXacB5xhivtXZBuA5wsNhHWsv4cO1ORJogq54XIiLO0oCJC2k6oiB5oZ4XIk2FtbZX8LoxZi7wdjgTFxBIXiSp54WIVMFaVDYiItIIyi99WiElLqSGnJ+8KLm0+kcs4njGmPlAJpBmjMkGJgNx8P/t3X+UXHWZ5/HPU9XdSVAImgQFkgDOhHXR+IMJP8rhOI0sLuAecAd3JqAT9ODmjIJOlnFWM+6gQ2ZOdOYcJ3JgzWSRxTgL6Mqsm9W46MT0gSMFklF+uzARWAiLkxj5oZJ0ddV99o97b3V1d3XV7U513W91vV/n1Om6VbfrPt3V+ebWc5/v85XcfWs3YjhUqWkhPS8AtBD3vMg7CgCY3yYvfdp0iVISF5iB4JMX9Yad/B0DwXP3y2aw7wfnIobRKskLAK1FPn5xBAAwNxqXPk2XKJ2QvCBxgRkKvqtdoZA27OSPGUB7hyo1LSJ5AaAFF6uNAMBcS5c+LRaLU5coJXGBWQi+8iL9s2a1EQDtuHvc84LkBYAWokg07ASAOdLY56LpEqUZExeZ+mWgr4SfvEgbdorsBYDWKrVIkYuGnQBacnpeAMCcaNbnYuPGjeM7zCBx0bZfBvpO+NNGWG0EQEaHxyJJoucFgJYin3r+DAA4cs36XNTNYKpIy9dB3wo+eZH+jdPzAkA7h8dqksS0EQAt0fMCAObGtH0uZtjjIn2dQqGgQqGgJUuWzE3A6CnBJy9YbQRAVocqcfJi4WDwQxuAHMWVFyQvAKDTSqWSdu3apU2bNo1P9ZhFc85SqaQtW7aoUCioVqtpw4YNKpfLcxQ1ekX4PS/EaiMAsjlE5QWADOh5AQBzp1Qqzbg5ZzMHDx6UuyuKouZLraLvBH950qi8AJBRmrxYSMNOAC3Q8wIAOq9cLmvz5s3jFRJHuBxqy6VW0ZeCr7ygYSeArOh5ASCLuPKC7AUAdMrk1UFeOXRo/MlCQarVMr1G49Ko6RQUlktFKvjkBQ07AWRF8gJAFpGL5AUAdNDIyIhGR0cVRdHExMWFF0o7d7b9/umWRp0wBQV9r+20ETO72cz2m9kj0zxvZna9me01s4fM7PSOBphWXnTyRQHMS4cqLJUKoL3InWkjANBBS5Ys0WAUTfzM9hd/kSlxIbE0KrLJ0vPiFkkXtHj+Qkmrktt6SV868rDGUXkBIKtKUpK4YCD4dj4AcuQ+3hAcANDelH4Wk4w+9ZQON2x/4wMfkD796cyvT38LZNF22oi732VmJ7fY5RJJ293dJd1rZsea2fHu/nwnArR6zwuSFwBaG6vG48QgyQsALbDaCABkN92Ujrr77tPHPve5+ubpCxboxo9+dEbHoL8FsujEGf6Jkp5t2N6XPNYRBVYbAZBRpRZPGxnkUwkQNDO7wMweT6acfmqafX7PzB4zs0fN7NZOHj9yqcA4AaCPtaukaNRySsff/Z109tn1zS/82Z/pxt27Z5V8KJVK2rhxI4kLTKurDTvNbL3iqSVauXJltu9JyjojkhcA2hhLkxdFKi+AUJlZUdKNks5XfMHjfjPb4e6PNeyzStJGSb/t7i+Y2XGdjIGeFwD6WdtKiknSKR3p/vUpHddcI/3N34zvWK3qmiJ9xzB3OpG8eE7Siobt5cljU7j7NknbJGnNmjWZ0hH1ygtadgJoo568YNoIELIzJe119yclycxuVzwF9bGGff69pBvd/QVJcvf9nQyAnhcA+lmzSop21Q5XXHGFJGndunXxvmecIe3ZM74DZfLogk4kL3ZIujo5+ThL0kud6nchjfe8iKJOvSKA+WqslvS8KPKhBAhYs+mmZ03a51RJMrMfSCpK+qy7/+9mLzabqk4XPS8A9K9pKymamFylsW7dOk0pXSNxgS5pm7wws9skDUtaamb7JH1G0qAkuftWSTslXSRpr6RXJH2okwGy2giArCrVtOcFlRdAjxtQvIrZsOKKzrvMbLW7vzh5x9lUdUY+vhQ7APSbZs0xy+Vy02aZk6s0Su94x8QX4zMauijLaiOXtXneJV3VsYgmKdRXG5mrIwCYL6pRpIGC0YgPCFuW6ab7JN3n7mOSnjKzJxQnM+7vRAARq40A6HOlUqmepGjVA6OxSqOaLElfxwc0dFnwlycLVF4AyGis5jTrBMJ3v6RVZnaKmQ1JWqt4CmqjbyquupCZLVU8jeTJTgXgrqllzwDQp1qtJpJWaZC4QAiCP8tPr6DW+AcCoI1KNaLfBRA4d69KulrSnZJ+Iunr7v6omV1nZhcnu90p6aCZPSZpt6Q/cfeDHTq+JFF5AQCJtLqiWCw27YHBVBGEoqtLpc5Gsd6wk38kAFobq0UaYqURIHjuvlNxz6zGx65tuO+SrkluHZWeTtDzAgBizXpg1NGcEwEJP3mRVl6QvADQxlgtYtoIgJYiKi8AzCPTNdqcqcYeGHUkLhCY4JMX6ZURpo0AaIeeFwDaSZMXRuUFgB7XqtFmlu9tmfQgcYEABZ+8SCsvoijnQAAEr1Kj5wWA1tLzb3IXAHpds0abWZIXbZMeJC4QqOAvUaYXUam8ANDOWJVpIwBac3peAJgn2jXanE6r1UVIXCBkwVdepGWdLJUKoB0adgJoh54XAOaLlo02W0iTHmnlRT3p0SJx0aneGsCRCD55wWojALKi5wWAduo9L0T2AkDva9poM8P3TEl6tElczLa3BtBJ4ScvWG0EQEb0vADQTno2wawRAP1sQtKjzVSR2fbWADot+EuUBaaNAMiIpVIBtONJA3B6XgCAMvW4mG1vDaDTeqjyIudAAASP5AWAduh5AQCJjM05Z9tbA+i0HkhexF9ZbQRAO2NVZ9oIgJbqPS+ovADQz2a4qshsemsAnRb8JcoCDTsBZETlBYB20rMJKi8A9C2WQ0WPCv4sn4adALKq1CINkbwA0AKVFwD6GokL9LDgz/Jp2AkgqypLpQJoIz2doGEnED4zu9nM9pvZI9M8P2xmL5nZA8nt2m7H2FNIXKDHBX+WXyiQvAB6RYaTjPeb2UNm9rCZ3WNmb+3k8cdqkQYH+EACYHrjlRc5BwIgi1skXdBmn7vd/W3J7bouxNSbSFxgHgg+eVE0VhsBesgtan2S8ZSk33H31ZI2SdrWyYNX6HkBoI3xyot84wDQnrvfJekXecfR80hcYJ4I/iy/wGojQM9od5Lh7ve4+wvJ5r2Slnfy+GP0vADQBj0vgHmnZGYPmtl3zOxN0+1kZuvNbI+Z7Tlw4EA348vXpLGufM892rx5s8rlck4BAbMX/lKprDYCzFdXSvpOsyfMbL2k9ZK0cuXKzC84Rs8LAG3Q8wKYV34k6SR3/5WZXSTpm5JWNdvR3bcpqfhcs2ZNf3ywaJK4OO+881SpVDQ0NKRdu3ZNWf60XC5r+/btkqR169axPCqCEvxZPquNAPOPmZ2rOHnxyWbPu/s2d1/j7muWLVuW6TVrkasWkbwA0Fq98iLnOAAcOXd/2d1/ldzfKWnQzJbmHFYYmkwVGRkZUaVSUa1WU6VS0cjIyIRdyuWyzj33XG3dulVbt27V8PAwFRoISvBn+TTsBOYXM3uLpJskXeLuBzv1umNJYxwadgJopV55EfwZEIB2zOz1lswBM7MzFX+26di5Rc9qTFycfnp94BseHtbQ0JCKxaKGhoY0PDw84dvS5EZqbGxsSoIDyFPPTBuh8gLofWa2UtLfS/oDd3+ik69dTcaIAbrwAWghvRjCtBEgfGZ2m6RhSUvNbJ+kz0galCR33yrpfZI+YmZVSYckrXXvvSue5XJZIyMjGh4ePrJpGu4TM7Mf+5h0/fX1zVKppF27dk17rDS5MTo6KkkaHByckuAA8hR+8qJeeZFzIADaynCSca2kJZL+c3KhpOruazpx7FotHiSKXE4F0EJ6PkHDTiB87n5Zm+dvkHRDl8KZE+VyuW0fimbfMyUB8cor0qteNb7TzTdLH/rQlO8tlUrTvn6pVNLu3bvpeYFgBZ+8SM8tmDYChC/DScaHJX14Lo6drkhE5QWAVpyeFwAC0qwPRauEQdNkx4knSiedNL7T3XdL55wzq3haJTeAvAV/iZJpIwCyqEZxz4sCyQsALaRnE0wbARCCdn0oJpuc7Nh7yy0TExdPPz3rxAUQuuArL1htBEAWSe6CygsALY33vMg5EABQ+z4Uk6XJjkqloisLBf3Btm3jT/7yl9KrXz3HEQP5CT55YWYyY9oIgNbSyosiV1MBtJAmOul5ASAUM5mqkSY7Bq+5RmvuvXf8iVqNZZQw7wWfvJDiDyNUXgBoJf1AUuRyKoAW0osh5C4A9IJmzTlLH/mI9OCD4ztxkRd9oieSF4WC1ZvxAUAz9coLkhcAMqDnBYDQNW3O+Y53TNyp4TNSqyVXO7YcK5CjnkheFM0UUXkBoIX0airJCwCt0PMCQK+Y3JyzXeJiuiVXZ7McKxCiTBOjzOwCM3vczPaa2aeaPP9BMztgZg8kt44uhThQMFVJXgBoIR0jSF4AaCU9naDyAkDoGlciqdZqE5+cVJXebMnVLM8BvaRt8sLMipJulHShpNMkXWZmpzXZ9Wvu/rbkdlMngxwomqo1khcAplcjeQEgg3oDcIYKAAEpl8vavHmzyuVy/bG0OWe7xIUkLVmyRGamQqEwZcnVmS7HCoQqy7SRMyXtdfcnJcnMbpd0iaTH5jKwRgPFQn0+OwA0U09ecDUVQAtO5QWAwKTTOkZHR1UsFnXDDTdo/fr1ktRyqkjj92/YsEFRFKlYLGrLli0TpoXMdDlWIFRZkhcnSnq2YXufpLOa7Hepmb1T0hOS/oO7Pzt5BzNbL2m9JK1cuTJzkIMF0xiVFwBaqCcvinwgATA9p+cFgMCMjIxodHRUURQpiiJdddVVWr16dabERfr9lUpFURTJzHTw4MEp+8xkOVYgVJ1aDPh/STrZ3d8i6XuSvtJsJ3ff5u5r3H3NsmXLMr/4QLGgao3KCwDTo/ICQBb0vAAQmuHhYRWLxfp2FEWZExfp9zMtBP0gS/LiOUkrGraXJ4/VuftBdx9NNm+S9FudCS82UDSN0bATQAtp8mKAy6kAWkh7XjBSAAhFqVTSDTfcoIGBARUKBdUmT5dvkbhIv3/Xrl3atGkTK4lgXssybeR+SavM7BTFSYu1ki5v3MHMjnf355PNiyX9pJNBDhaovADQWpq8KJC8ANBCvV8nlRcAuqhcLrfsObF+/foZTRWZjGkh6AdtkxfuXjWzqyXdKako6WZ3f9TMrpO0x913SPq4mV0sqSrpF5I+2MkgBwdYbQRAazWn8gJAe/S8ANBtaUPOSqWioaGhaasjZpu4APpFlsoLuftOSTsnPXZtw/2NkjZ2NrRxA4UC00YAtFSl8gJABvWeF4wVALokbahZq9VUqVQ0MjIyNXkxuRqMxAUwRacads6pwaIxbQRASxE9LwBkQM8LAN3WtqEmiQsgk55IXgwUCkwbAdBSvfKCeewAWkjPJuh5AaBbJjfUlKTNmzerXC6TuABmINO0kbwNFE2Hxmp5hwEgYPXKiyIfSIBeYGYXSPqi4n5aN7n756bZ71JJ35B0hrvvOdLjRvS8ADAH2jXkTBtqNva/qNYmfb4hcQG01BPJi8FiQdXJSwYBQIO08qLI1VQgeGZWlHSjpPMl7ZN0v5ntcPfHJu13tKQ/knRfp4493rCTsQJAZ7RqyDk5qZH2v5gucdEuCQL0s55IXgwUWG0EQGvp1dQil1OBXnCmpL3u/qQkmdntki6R9Nik/TZJ+rykP+nUgdNrIeQuAHTKdA05myU1hoeHWyYusqxKAvSrnuh5MThQ0BgNOwG0kCY4SV4APeFESc82bO9LHqszs9MlrXD3b7d6ITNbb2Z7zGzPgQMH2h44vRRC5QWATkkbchYKBZmZlixZImmapEaL5VCb7Q9gXG8kLwpWLwkHgGZqVF4A84aZFSR9QdIft9vX3be5+xp3X7Ns2bK2r11fbYShAkCHlEolbdmyRcViUVEUacOGDSqXy1NWGdn4p3868Rsn9bhouyoJ0Od6InkxUGS1EQCt1SKSF0APeU7Siobt5cljqaMlvVnSiJk9LelsSTvMbM2RHpieFwDmwsGDBxVFkaIomjB1JF1l5JVDhyZ+Q5PmnJNXJWHKCDBRT/S8GCwa00YAtETyAugp90taZWanKE5arJV0efqku78kaWm6bWYjkj7RmdVG0tc80lcCgHFp1UTaryKtmiiVSlOmihy1aJF2lcuSNKU5Z7oqCYCpeiJ5sWCgqNEqyQsA06ux2gjQM9y9amZXS7pT8VKpN7v7o2Z2naQ97r5j7o4df6XyAkAnpVUTU1YKaRhrvivpX0sqViravn27vvKVr9CcE5iBnpg2smCwoMNjtfY7AuhbaV+cgUJPDGtA33P3ne5+qrv/hrv/ZfLYtc0SF+4+3ImqC2m85wVFWgA6rVQqaePGjZKkzZs3T0hcPPf+9+u9ixbV+1lIojknMEM9UXmxMKm8cHcZV0oANBElyQtyFwBaGW/YyfkEgM5Llzud0OPiz/9cJ157rXaVy/XKDEkTKi+ma85ZbvgeKjPQ73ojeTFYlCSNVqP6fQBoROUFgCzSaSOkLgDMhZGRkQmJi2//7u/qPddeK2lqP4um00wapIkQppYAsR5JXsQfRg6P1UheAGiqXgpO7gJACy5WGwEwdxqXQ33v0JA++YlPSGpeQdGuOefIyMiUqSUkL9DPeiR5EScsDo/RtBNAc+lyylReAGglSk4lSF4A6Cj3CVdQbvr4x/XJtWtVKpVmXUEx3QomQL/qkeTFeOUFADRTowkfgAzGe17kHAiA+aNalQYHx7efeUYfXrGivjldBUW7fhbTrmAC9KneSF4MJJUXVZIXAJqrRZGKBaMJH4CW6j0vGCoAdMKhQ9JRR41vv/iitHjxhF2aVVCUy2Wde+659cd27949bQKDpAUQ64n6aqaNAGinFklFyi4AtEHPCwAd8+KLExMXo6NTEhfSeAXFpk2b6lNGtm/frtHRUbm7RkdHtX379i4GDvSmnqi8WJBMGzlUofICQHO1KFKRDyMA2kgWJiJ5AeDIPP+8dMIJ49u1Wsuu4VRQAEeuJyovjl4QzyH79Wg150gAtGJmN5vZfjN7ZJrnzcyuN7O9ZvaQmZ3eqWPXImmAygsAbUT0xwFwpJ59dmLiIopmvNzZunXrNDQ0JDPT0NCQ1q1b1+EggfmnJyovjl4Yh/ny4bGcIwHQxi2SbpA0Xe3jhZJWJbezJH0p+XrEalGkAp9GALSRVl6I4QLAbDz+uPTGN45vp410ZqhUKmlkZIRmnMAM9ETy4phFceXFy4dIXgAhc/e7zOzkFrtcImm7u7uke83sWDM73t2fP9Jj19ypvADQntPzAsAs7dkjnXHG+PYsExcpppIAM9Mb00bqlRdMGwF63ImSnm3Y3pc8NoGZrTezPWa258CBA5leuBY5lRcA2qLnBYBZ2b27o4kLADPXE8mLwWJBRw0VqbwA+oS7b3P3Ne6+ZtmyZZm+pxZReQGgPXpeAJixl1+W3vWu8W0SF0AueiJ5IUmLFw3qhVdIXgA97jlJKxq2lyePHbFq5FxJBdBWWnlhNL0AkMXLL0tnnlnfLN9zT47BAP2tZ5IXxx2zUPt/eTjvMAAcmR2S1iWrjpwt6aVO9LuQpChyDRT5MAKgNU+umFrPnAEByM0LL0jnn69o715dPjSkgWJR5513nsrl8oxfqlwua/PmzbP6XgCxnmjYKUnHH7NQew/8Ku8wALRgZrdJGpa01Mz2SfqMpEFJcvetknZKukjSXkmvSPpQp45djVxFKi8AtOH0vACQxc9/Lr373dKjj+qOyy/X12+9VbVaTZVKRSMjIzNqtFkul3XeeeepUqloaGhIu3btolEnMAs9c93h9YsX6v+9eKh+xQRAeNz9Mnc/3t0H3X25u3/Z3bcmiQt57Cp3/w13X+3uezp17MhdRSaxA2iDnhdA7zCzm81sv5k9Ms3zZmbXm9leM3vIzE7v2MGfeEJjP/2pvnb55XrhnHNULBZlZioWixoeHp5RJcXIyIgqlcqE5AeAmeuZyotTX3e0XqnUtO+FQ1rx2qPyDgdAYKo1khcA2qPnBdBTbpF0g6Tt0zx/oaRVye0sSV9Kvh6xspneW6no4Fe/qoHbbhufcmamhx9+WBs2bMhcSTE8PKyhoaH6/sPDw50IEeg7PVN5sfrExZKkHz3zQs6RAAgRlRcAsnClH0ByDgRAW+5+l6RftNjlEknbk8rOeyUda2bHd+LYIyMjOjg2Vq+WqFarcndVq1XdcccdM6qkKJVK2rVrlzZt2sSUEeAI9EzlxZtOOEbHHb1At973jN6z+ngNFHsm7wKgC6oRyQsA7dHzAphXTpT0bMP2vuSxKc3AzWy9pPWStHLlyrYv3FgtMTAwIHdXrVbT0NCQLr30Ut19990zqqQolUokLYAjlCl5YWYXSPqipKKkm9z9c5OeX6C4nOu3JB2U9Pvu/nQnAy0UTB8/b5X+0zcf0bu33KWzTlmiExYv1OsWL9Sxiwa1eNGgjkm+Ll40qKOG4nlpAPpDjeQFgAyiiJ4XQD9y922StknSmjVr2jbRS6slRkZGNDw8rIcfflh33HGHLr30Uq1fv16rV6+uP0dSAuiOtskLMytKulHS+Yqzmfeb2Q53f6xhtyslveDuv2lmayV9XtLvdzrYD5x9ko5ZNKjbf/iMvvvoz3Tw15Vp9x0omI4aKmrRUFGLBotamNwWDY4/tmCgoIGiqVgoaKBgGiha8jXZLqTPJ48XTGYms3i+mykuOy003E8fL9T3G7+6Y2YqWDzPtmBpyaplKl3Neo6VJWGT/bU6s098zA7+kJi1Vce9WstfMz97xtRYbQRABvWeF4wXwHzwnKQVDdvLk8c6Iq2WKJfL9R4Xd999t1avXk0lBZCDLJUXZ0ra6+5PSpKZ3a54fllj8uISSZ9N7n9D0g1mZj4HS4Nc/NYTdPFbT5AkHR6raf/Lo3rp0Fj99vLh8fuHKrX4NlbT4bH46+hYpAO/HNXhsZoOV2uq1VxjkasWucZqkWqRq1pzVaOofoIDzBfXXfImrSudnHcYc4LKCwBZpD0vGC6AeWGHpKuTzydnSXrJ3adMGTlSzVYLIXEBdF+W5EWzuWSTu/jW93H3qpm9JGmJpJ837jTTuWbtLBwsauWSubuKHEWuahQnMqpJUsM9Pu1x14T7UXI/LUdtfMzdk0SIJ4/HJ09RNH4S1UonU0BZX6uTcWXZjSVwu2O+Vl1I0l/+2zd39N8KgPlp7Rkr9TunLqPyAugBZnabpGFJS81sn6TPSBqUpGQZ9p2SLpK0V9Irkj40F3GwWggQhq427JzpXLO8FQqmoYJpqHcWZQH61m8ed3TeIQDoAa9fvFCvX7ww7zAAZODul7V53iVdNddxTO5/QdUFkI8syYssc8nSffaZ2YCkxYobdwIAAABAT6PHBZC/LCUF90taZWanmNmQpLWK55c12iHpiuT++yR9fy76XQAAAAAAgP7TtvIi6WFxtaQ7FS+VerO7P2pm10na4+47JH1Z0lfNbK+kXyhOcAAAAAAAAByxTD0v3H2n4oY4jY9d23D/sKR/19nQAAAAAAAAsk0bAQAAAAAAyA3JCwAAAAAAEDSSFwAAAAAAIGgkLwAAAAAAQNBIXgAAAAAAgKCZu+dzYLMDkv5vxt2XSvr5HIYzE8TSXEixSGHF04uxnOTuy+Y6mCwYKzoipFiksOIhluZ6bqyQZjRe9OLvultCiodYmgspFqkHx4seHSta6ZU4JWKdK/Mp1mnHitySFzNhZnvcfU3ecUjEMp2QYpHCiodYuiekn49YphdSPMTSXEixzIWQfr6QYpHCiodYmgspFim8eDqpV362XolTIta50i+xMm0EAAAAAAAEjeQFAAAAAAAIWq8kL7blHUADYmkupFiksOIhlu4J6ecjlumFFA+xNBdSLHMhpJ8vpFiksOIhluZCikUKL57EqMPoAAAHrElEQVRO6pWfrVfilIh1rvRFrD3R8wIAAAAAAPSvXqm8AAAAAAAAfYrkBQAAAAAACFrQyQszu8DMHjezvWb2qS4d82Yz229mjzQ89loz+56Z/VPy9TXJ42Zm1yfxPWRmp3cwjhVmttvMHjOzR83sj/KKJXn9hWb2QzN7MInnz5PHTzGz+5Ljfs3MhpLHFyTbe5PnT+5kPMkximb2YzP7Vp6xmNnTZvawmT1gZnuSx/J6n441s2+Y2f8xs5+YWSmvWLqt2+NFKGNF8vrBjBeMFS3jCGasSI7Rl+MFYwVjRYuYghgrkmMEM17M97Gi3ZjQjfc7qwyxXpP8+37IzHaZ2Ul5xJnEkmmsNbNLzczNLLdlPrPEama/1zB23trtGBviaPc3sDIZ53+c/B1clFOcU/7vm/T87MYKdw/yJqko6aeS3iBpSNKDkk7rwnHfKel0SY80PPZXkj6V3P+UpM8n9y+S9B1JJulsSfd1MI7jJZ2e3D9a0hOSTssjluT1TdKrk/uDku5LjvN1SWuTx7dK+khy/6OStib310r62hy8V9dIulXSt5LtXGKR9LSkpZMey+t9+oqkDyf3hyQdm1cs3bzlMV6EMlYkrx/MeMFY0TKOYMaK5Bh9N14wVjBWtIkpiLEied1gxov5PFZkGRO68X53MNZzJR2V3P9IyLEm+x0t6S5J90paE2qsklZJ+rGk1yTbxwUc67aGseo0SU/nFOuU//smPT+rsaLrP8gMfuCSpDsbtjdK2tilY5+siScZj0s6Prl/vKTHk/t/K+myZvvNQUz/U9L5gcRylKQfSTpL0s8lDUx+zyTdKamU3B9I9rMOxrBc0i5J75L0reQPP69YntbUE4yuv0+SFkt6avLPFsLfzFzf8hovQhwrktcPYrxgrJgSSxBjRfJ6fTleMFZMiYuxYjyGYMaK5HWDGC/m+1iRZUzoxvvdqVgn7f92ST8I9feaPL5F0nskjSi/5EWWv4G/UpLAy/OWMda/lfTJhv3vyTHekzV98mJWY0XI00ZOlPRsw/a+5LE8vM7dn0/u/0zS65L7XYkxKU97u+KrErnFkpRTPiBpv6TvKc78veju1SbHrMeTPP+SpCUdDGeLpP8oKUq2l+QYi0v6rpn9o5mtTx7L4306RdIBSf81KRW7ycxelVMs3RbKz5L77zqE8YKxYlqhjBVS/44Xofwcuf+eGSumCGmskMIZL+b7WJEl3m6831nM9Hd7peIr23loG2syTWCFu3+7m4E1keX3eqqkU83sB2Z2r5ld0LXoJsoS62clfcDM9knaKelj3QltxmY1VoScvAiSx6kh79bxzOzVku6QtMHdX84zFnevufvbFF+dOFPSG7t17EZm9m8k7Xf3f8zj+E2c4+6nS7pQ0lVm9s7GJ7v4Pg0oLs/6kru/XdKvFZdy5hFL38vjdx3KeMFYMa1QxgqJ8SIYjBWMFdMIZbxgrOhBZvYBSWsk/XXesTRjZgVJX5D0x3nHktGA4qkjw5Iuk/RfzOzYXCOa3mWSbnH35YqnZnw1+X3PCyH/IM9JWtGwvTx5LA//bGbHS1LydX/y+JzGaGaDik8u/pu7/32esTRy9xcl7VZcinSsmQ00OWY9nuT5xZIOdiiE35Z0sZk9Lel2xSWeX8wpFrn7c8nX/ZL+h+ITsDzep32S9rn7fcn2NxSfcOT+N9MFofwsuf2uQxwvGCsmCmiskPp3vAjl52CsaMBYMVVA48V8HyuyxDvn73dGmX63ZvavJH1a0sXuPtql2CZrF+vRkt4saST5d3e2pB05Ne3M8nvdJ2mHu4+5+1OK+wWt6lJ8jbLEeqXifj1y97KkhZKWdiW6mZnVWBFy8uJ+Sass7vQ8pLhBzo6cYtkh6Yrk/hWK54imj69LuqWeLemlhhK6I2JmJunLkn7i7l/IM5YknmVphtHMFimeI/sTxScb75smnjTO90n6fpKZP2LuvtHdl7v7yYr/Lr7v7u/PIxYze5WZHZ3el/RuSY8oh/fJ3X8m6Vkz+xfJQ+dJeiyPWHIQyniR17/PYMYLxormQhorpL4eLxgrGCumCGmskMIaL/pgrMgyJszp+z0DbWM1s7cr7iVwcZL4ykvLWN39JXdf6u4nJ//u7lUc857QYk18U3HVhcxsqeJpJE92M8hEllifUfzvVGb2LxUnLw50NcpsZjdWeM6NR1rdFJe6PKF4DuSnu3TM2yQ9L2lMcZbtSsXz2nZJ+idJ/yDptcm+JunGJL6H1cFGM5LOUVyC95CkB5LbRXnEkrz+WxR32X1I8X+g1yaPv0HSDyXtlfTfJS1IHl+YbO9Nnn/DHL1fwxrvCt71WJJjPpjcHk3/TnN8n94maU/yPn1T0mvyiqXbt26PF6GMFcnrBzNeMFZMe/ygxorkGH05XjBWMFa0iSvXsaLhuMGMF/N9rGg2Jki6TvGH6a797XUo1n+Q9M8N/753hBrrpH1H8vxbyfB7NcXTXB5L/q7XBhzraZJ+kIwfD0h6d05xNvu/7w8l/WHD73TGY4Ul3wwAAAAAABCkkKeNAAAAAAAAkLwAAAAAAABhI3kBAAAAAACCRvICAAAAAAAEjeQFAAAAAAAIGskLAAAAAAAQNJIXAAAAAAAgaP8f/Q3u33LtLH0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Linear Regression with DataLoader"
      ],
      "metadata": {
        "id": "Drmr_d2ayDeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np; np.random.seed(0)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_data():\n",
        "    x = np.random.uniform(size=(100,1))\n",
        "    y = 1 + 2*x + np.random.normal(scale=0.1,size=(100,1))\n",
        "    \n",
        "    inputs = torch.tensor(x, dtype=torch.float32) # (100, 1)\n",
        "    targets = torch.tensor(y, dtype=torch.float32) # (100, 1)\n",
        "    \n",
        "    dataset = TensorDataset(inputs, targets)\n",
        "    \n",
        "    batch_size = 10\n",
        "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
        "        \n",
        "    return inputs, targets, dataloader\n",
        "\n",
        "class Model(nn.Module):        \n",
        "        \n",
        "    def __init__(self, dataloader):\n",
        "        super().__init__()\n",
        "        self.dataloader = dataloader\n",
        "        \n",
        "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
        "        self.loss = F.mse_loss\n",
        "            \n",
        "    def fit(self, epochs, lr, opt_func=torch.optim.SGD):\n",
        "        opt = opt_func(self.parameters(), lr)\n",
        "\n",
        "        w_trace = []\n",
        "        b_trace = []\n",
        "        loss_trace = []\n",
        "        for i in range(epochs):\n",
        "            for inputs, targets in self.dataloader:\n",
        "                preds = self(inputs) # 1. Generate predictions\n",
        "                loss = self.loss(preds, targets) # 2. Calculate loss\n",
        "                loss.backward() # 3. Compute gradients\n",
        "                opt.step() # 4. Update parameters using gradients\n",
        "                opt.zero_grad() # 5. Reset the gradients to zero\n",
        "\n",
        "                w_trace.append(self.linear.weight.item())\n",
        "                b_trace.append(self.linear.bias.item())\n",
        "                loss_trace.append(loss.item())\n",
        "\n",
        "        return {\"w_trace\": w_trace, \"b_trace\": b_trace, \"loss_trace\": loss_trace}\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        xb = xb.view((-1, 1))\n",
        "        return self.linear(xb)\n",
        "    \n",
        "def main():\n",
        "    inputs, targets, dataloader = load_data() \n",
        "    \n",
        "    model = Model(dataloader)\n",
        "    history = model.fit(epochs=600, lr=1e-1, opt_func=torch.optim.SGD)\n",
        "\n",
        "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1,4,figsize=(15,4))\n",
        "    ax0.plot(history[\"loss_trace\"],label=\"loss\")\n",
        "    ax1.plot(history[\"w_trace\"],label=\"slope\")\n",
        "    ax2.plot(history[\"b_trace\"],label=\"intercept\")\n",
        "    ax3.plot(inputs,targets,'k.',label=\"data\")\n",
        "    ax3.plot(inputs,model(inputs).detach().numpy(),'r-',label=\"linear regression\")\n",
        "    for ax in (ax0, ax1, ax2, ax3):\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "1d8N5SK7yDn0",
        "outputId": "9bc930c3-aac3-4329-8e18-d4fba758101d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Linear Regression with Multiple Outputs"
      ],
      "metadata": {
        "id": "oQAFnfiXMi64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np; np.random.seed(0)\n",
        "import torch\n",
        "\n",
        "def load_data():\n",
        "    inputs = np.array([[73, 67, 43], \n",
        "                       [91, 88, 64], \n",
        "                       [87, 134, 58], \n",
        "                       [102, 43, 37], \n",
        "                       [69, 96, 70]], dtype=np.float32) # (5, 3)\n",
        "    targets = np.array([[56, 70],\n",
        "                        [81, 101],\n",
        "                        [119, 133],\n",
        "                        [22, 37],\n",
        "                        [103, 119]], dtype=np.float32) # (5, 2)\n",
        "    \n",
        "    inputs = torch.from_numpy(inputs)\n",
        "    targets = torch.from_numpy(targets)\n",
        "    \n",
        "    return inputs, targets\n",
        "\n",
        "def model(x, w, b):\n",
        "    return x @ w.t() + b # (5, 3) @ (3, 2) + (2,) = (5, 2)\n",
        "\n",
        "def mse(preds, targets):\n",
        "    return torch.sum((preds-targets)**2) / targets.numel()\n",
        "\n",
        "def main():\n",
        "    # load data\n",
        "    inputs, targets = load_data()\n",
        "    \n",
        "    # weight initialization\n",
        "    w = torch.randn((2, 3), requires_grad=True) # (2, 3)\n",
        "    b = torch.randn((2,), requires_grad=True) # (2,)\n",
        "    \n",
        "    # train\n",
        "    loss_trace = []\n",
        "    for i in range(600):\n",
        "        preds = model(inputs, w, b) # (5, 2)\n",
        "        loss = mse(preds, targets) # ()\n",
        "        loss_trace.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            w -= w.grad * 1e-5\n",
        "            b -= b.grad * 1e-5\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15,4))\n",
        "    ax.plot(loss_trace)\n",
        "    plt.show()\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "35Pk0C0bMjGb",
        "outputId": "71f66d62-3df2-44d9-aade-61fc01b949bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Linear Regression with SPY and WMT"
      ],
      "metadata": {
        "id": "0R_Kqx_BQMD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np; np.random.seed(0)\n",
        "import pandas as pd\n",
        "import torch; torch.manual_seed(0)\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    df = yf.Ticker('SPY').history(period='max')\n",
        "    df['return'] = df['Close'].pct_change()\n",
        "    x = np.array(df['return'][-100:]).reshape((-1,1))\n",
        "\n",
        "    dg = yf.Ticker('WMT').history(period='max')\n",
        "    dg['return'] = dg['Close'].pct_change()\n",
        "    y = np.array(dg['return'][-100:]).reshape((-1,1))\n",
        "    \n",
        "    inputs = torch.tensor(x, dtype=torch.float32) # (100, 1)\n",
        "    targets = torch.tensor(y, dtype=torch.float32) # (100, 1)\n",
        "        \n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "def initialize_weight():\n",
        "    w = torch.randn((1,1), requires_grad=True) # (1,1)\n",
        "    b = torch.randn((1,1), requires_grad=True) # (1,1)\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def train(inputs, targets, w, b, epochs=40_000, lr=6e-1):\n",
        "    w_trace = []\n",
        "    b_trace = []\n",
        "    loss_trace = []\n",
        "    for _ in range(epochs):\n",
        "        preds = model(inputs, w, b) # (100, 1)\n",
        "        loss = mse(preds, targets) # ()\n",
        "        w_trace.append(w.item())\n",
        "        b_trace.append(b.item())\n",
        "        loss_trace.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            w -= w.grad * lr\n",
        "            b -= b.grad * lr\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "            \n",
        "    return w, b, w_trace, b_trace, loss_trace \n",
        "\n",
        "\n",
        "def model(inputs, w, b):\n",
        "    return inputs * w + b # (100, 1) * (1,1) + (1,1) = (100, 1)\n",
        "\n",
        "\n",
        "def mse(preds, targets):\n",
        "    return torch.sum((preds-targets)**2) / targets.numel()\n",
        "\n",
        "\n",
        "def main():\n",
        "    inputs, targets = load_data()\n",
        "    \n",
        "    w, b = initialize_weight()\n",
        "    \n",
        "    w, b, w_trace, b_trace, loss_trace = train(inputs, targets, w, b) \n",
        "\n",
        "    preds = model(inputs, w, b)\n",
        "\n",
        "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1,4,figsize=(15,4))\n",
        "    ax0.plot(loss_trace,label=\"loss\")\n",
        "    ax1.plot(w_trace,label=\"slope\")\n",
        "    ax2.plot(b_trace,label=\"intercept\")\n",
        "    ax3.plot(inputs,targets,'k.',label=\"data\")\n",
        "    ax3.plot(inputs,preds.detach().numpy(),'r-',label=\"linear regression\")\n",
        "    for ax in (ax0, ax1, ax2, ax3):\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "27bGxem9NgpN",
        "outputId": "dd8123e2-7822-408c-d391-a602866424cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 7 CNN***"
      ],
      "metadata": {
        "id": "NiPg-8Wj7RZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Multi-Layer Perceptron [beginner_source/basics/buildmodel_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/buildmodel_tutorial.py)"
      ],
      "metadata": {
        "id": "VjdaHVwsW1Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Build the Neural Network\n",
        "===================\n",
        "\n",
        "Neural networks comprise of layers/modules that perform operations on data.\n",
        "The `torch.nn <https://pytorch.org/docs/stable/nn.html>` namespace \n",
        "provides all the building blocks you need to\n",
        "build your own neural network. \n",
        "Every module in PyTorch subclasses the \n",
        "`nn.Module <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`.\n",
        "A neural network is a module itself that consists of other modules (layers). \n",
        "This nested structure allows for\n",
        "building and managing complex architectures easily.\n",
        "\n",
        "In the following sections, \n",
        "we'll build a neural network to classify images in the FashionMNIST dataset.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Get Device for Training\n",
        "# \n",
        "# We want to be able to train our model on a hardware accelerator like the GPU,\n",
        "# if it is available. Let's check to see if\n",
        "# `torch.cuda <https://pytorch.org/docs/stable/notes/cuda.html>` is available, \n",
        "# else we continue to use the CPU.\n",
        "################################################################################\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "################################################################################\n",
        "# Define the Class\n",
        "# -------------------------\n",
        "# We define our neural network by subclassing ``nn.Module``, and\n",
        "# initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n",
        "# the operations on input data in the ``forward`` method.\n",
        "################################################################################\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "################################################################################\n",
        "# We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print\n",
        "# its structure.\n",
        "################################################################################\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# To use the model, we pass it the input data. This executes the model's ``forward``,\n",
        "# along with some `background operations <https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866>`_.\n",
        "# Do not call ``model.forward()`` directly!\n",
        "#\n",
        "# Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output.\n",
        "# We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module.\n",
        "################################################################################\n",
        "\n",
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "################################################################################\n",
        "# Model Layers\n",
        "# \n",
        "#\n",
        "# Let's break down the layers in the FashionMNIST model. To illustrate it, we\n",
        "# will take a sample minibatch of 3 images of size 28x28 and see what happens to it as\n",
        "# we pass it through the network.\n",
        "################################################################################\n",
        "\n",
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())\n",
        "\n",
        "################################################################################\n",
        "# nn.Flatten\n",
        "# \n",
        "# We initialize the `nn.Flatten  <https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html>`_\n",
        "# layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\n",
        "# the minibatch dimension (at dim=0) is maintained).\n",
        "################################################################################\n",
        "\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())\n",
        "\n",
        "################################################################################\n",
        "# nn.Linear\n",
        "# \n",
        "# The `linear layer <https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>`_\n",
        "# is a module that applies a linear transformation on the input using its stored weights and biases.\n",
        "################################################################################\n",
        "\n",
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())\n",
        "\n",
        "################################################################################\n",
        "# nn.ReLU\n",
        "# \n",
        "# Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
        "# They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n",
        "# learn a wide variety of phenomena.\n",
        "#\n",
        "# In this model, we use `nn.ReLU <https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html>`_ between our\n",
        "# linear layers, but there's other activations to introduce non-linearity in your model.\n",
        "################################################################################\n",
        "\n",
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")\n",
        "\n",
        "################################################################################\n",
        "# nn.Sequential\n",
        "# \n",
        "# `nn.Sequential <https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>`_ is an ordered\n",
        "# container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
        "# sequential containers to put together a quick network like ``seq_modules``.\n",
        "################################################################################\n",
        "\n",
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)\n",
        "\n",
        "################################################################################\n",
        "# nn.Softmax\n",
        "# \n",
        "# The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the\n",
        "# `nn.Softmax <https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html>`_ module. The logits are scaled to values\n",
        "# [0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
        "# which the values must sum to 1.\n",
        "################################################################################\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Model Parameters\n",
        "# -------------------------\n",
        "# Many layers inside a neural network are *parameterized*, i.e. have associated weights\n",
        "# and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n",
        "# tracks all fields defined inside your model object, and makes all parameters\n",
        "# accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n",
        "#\n",
        "# In this example, we iterate over each parameter, and print its size and a preview of its values.\n",
        "################################################################################\n",
        "\n",
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
        "\n",
        "################################################################################\n",
        "# Further Reading\n",
        "# \n",
        "# - `torch.nn API <https://pytorch.org/docs/stable/nn.html>`_\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "fU4pUW_PW1Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CNN Basics [beginner_source/blitz/neural_networks_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/blitz/neural_networks_tutorial.py)"
      ],
      "metadata": {
        "id": "0MiiNfkAdEmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Neural Networks\n",
        "===============\n",
        "\n",
        "Neural networks can be constructed using the ``torch.nn`` package.\n",
        "\n",
        "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
        "``autograd`` to define models and differentiate them.\n",
        "An ``nn.Module`` contains layers, and a method ``forward(input)`` that\n",
        "returns the ``output``.\n",
        "\n",
        "For example, look at this network that classifies digit images:\n",
        "\n",
        ".. figure:: /_static/img/mnist.png\n",
        "   :alt: convnet\n",
        "\n",
        "   convnet\n",
        "\n",
        "It is a simple feed-forward network. It takes the input, feeds it\n",
        "through several layers one after the other, and then finally gives the\n",
        "output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or\n",
        "  weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule:\n",
        "  ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "Define the network\n",
        "------------------\n",
        "\n",
        "Let’s define this network:\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "################################################################################\n",
        "# You just have to define the ``forward`` function, and the ``backward``\n",
        "# function (where gradients are computed) is automatically defined for you\n",
        "# using ``autograd``.\n",
        "# You can use any of the Tensor operations in the ``forward`` function.\n",
        "#\n",
        "# The learnable parameters of a model are returned by ``net.parameters()``\n",
        "################################################################################\n",
        "\n",
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight\n",
        "\n",
        "################################################################################\n",
        "# Let's try a random 32x32 input.\n",
        "# Note: expected input size of this net (LeNet) is 32x32. To use this net on\n",
        "# the MNIST dataset, please resize the images from the dataset to 32x32.\n",
        "################################################################################\n",
        "\n",
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)\n",
        "\n",
        "################################################################################\n",
        "# Zero the gradient buffers of all parameters and backprops with random\n",
        "# gradients:\n",
        "################################################################################\n",
        "\n",
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))\n",
        "\n",
        "################################################################################\n",
        "# .. note::\n",
        "#\n",
        "#     ``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
        "#     package only supports inputs that are a mini-batch of samples, and not\n",
        "#     a single sample.\n",
        "#\n",
        "#     For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
        "#     ``nSamples x nChannels x Height x Width``.\n",
        "#\n",
        "#     If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
        "#     a fake batch dimension.\n",
        "#\n",
        "# Before proceeding further, let's recap all the classes you’ve seen so far.\n",
        "#\n",
        "# **Recap:**\n",
        "#   -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
        "#      operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
        "#      tensor.\n",
        "#   -  ``nn.Module`` - Neural network module. *Convenient way of\n",
        "#      encapsulating parameters*, with helpers for moving them to GPU,\n",
        "#      exporting, loading, etc.\n",
        "#   -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
        "#      registered as a parameter when assigned as an attribute to a*\n",
        "#      ``Module``.\n",
        "#   -  ``autograd.Function`` - Implements *forward and backward definitions\n",
        "#      of an autograd operation*. Every ``Tensor`` operation creates at\n",
        "#      least a single ``Function`` node that connects to functions that\n",
        "#      created a ``Tensor`` and *encodes its history*.\n",
        "#\n",
        "# **At this point, we covered:**\n",
        "#   -  Defining a neural network\n",
        "#   -  Processing inputs and calling backward\n",
        "#\n",
        "# **Still Left:**\n",
        "#   -  Computing the loss\n",
        "#   -  Updating the weights of the network\n",
        "#\n",
        "# Loss Function\n",
        "# -------------\n",
        "# A loss function takes the (output, target) pair of inputs, and computes a\n",
        "# value that estimates how far away the output is from the target.\n",
        "#\n",
        "# There are several different\n",
        "# `loss functions <https://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
        "# nn package .\n",
        "# A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
        "# between the output and the target.\n",
        "#\n",
        "# For example:\n",
        "################################################################################\n",
        "\n",
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)\n",
        "\n",
        "################################################################################\n",
        "# Now, if you follow ``loss`` in the backward direction, using its\n",
        "# ``.grad_fn`` attribute, you will see a graph of computations that looks\n",
        "# like this:\n",
        "#\n",
        "# ::\n",
        "#\n",
        "#     input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "#           -> flatten -> linear -> relu -> linear -> relu -> linear\n",
        "#           -> MSELoss\n",
        "#           -> loss\n",
        "#\n",
        "# So, when we call ``loss.backward()``, the whole graph is differentiated\n",
        "# w.r.t. the neural net parameters, and all Tensors in the graph that have\n",
        "# ``requires_grad=True`` will have their ``.grad`` Tensor accumulated with the\n",
        "# gradient.\n",
        "#\n",
        "# For illustration, let us follow a few steps backward:\n",
        "################################################################################\n",
        "\n",
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n",
        "\n",
        "################################################################################\n",
        "# Backprop\n",
        "# \n",
        "# To backpropagate the error all we have to do is to ``loss.backward()``.\n",
        "# You need to clear the existing gradients though, else gradients will be\n",
        "# accumulated to existing gradients.\n",
        "#\n",
        "# Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
        "# gradients before and after the backward.\n",
        "################################################################################\n",
        "\n",
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "################################################################################\n",
        "# Now, we have seen how to use loss functions.\n",
        "#\n",
        "# **Read Later:**\n",
        "#\n",
        "#   The neural network package contains various modules and loss functions\n",
        "#   that form the building blocks of deep neural networks. A full list with\n",
        "#   documentation is `here <https://pytorch.org/docs/nn>`_.\n",
        "#\n",
        "# **The only thing left to learn is:**\n",
        "#\n",
        "#   - Updating the weights of the network\n",
        "#\n",
        "# Update the weights\n",
        "# ------------------\n",
        "# The simplest update rule used in practice is the Stochastic Gradient\n",
        "# Descent (SGD):\n",
        "#\n",
        "#      ``weight = weight - learning_rate * gradient``\n",
        "#\n",
        "# We can implement this using simple Python code:\n",
        "#\n",
        "# .. code:: python\n",
        "#\n",
        "#     learning_rate = 0.01\n",
        "#     for f in net.parameters():\n",
        "#         f.data.sub_(f.grad.data * learning_rate)\n",
        "#\n",
        "# However, as you use neural networks, you want to use various different\n",
        "# update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
        "# To enable this, we built a small package: ``torch.optim`` that\n",
        "# implements all these methods. Using it is very simple:\n",
        "################################################################################\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update\n",
        "\n",
        "################################################################################\n",
        "# .. Note::\n",
        "#\n",
        "#       Observe how gradient buffers had to be manually set to zero using\n",
        "#       ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
        "#       as explained in the `Backprop`_ section.\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "7DsTkGVmdEwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CNN - CIFAR10 [beginner_source/introyt/introyt1_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/introyt1_tutorial.py) [english](https://www.youtube.com/watch?v=IC0_FRiX-sw)"
      ],
      "metadata": {
        "id": "Sv0GTvfMn-Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg\">\n",
        "\n",
        "Sorce [LeNet-5 - A Classic CNN Architecture](https://www.datasciencecentral.com/lenet-5-a-classic-cnn-architecture/)"
      ],
      "metadata": {
        "id": "vq9-qL19vaf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Introduction to PyTorch\n",
        "\n",
        "Follow along with the video below or on \n",
        "`youtube <https://www.youtube.com/watch?v=IC0_FRiX-sw>`.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "################################################################################\n",
        "# Let’s see a few basic tensor manipulations. \n",
        "# First, just a few of the ways to create tensors:\n",
        "################################################################################ \n",
        "\n",
        "z = torch.zeros(5, 3)\n",
        "print(z)\n",
        "print(z.dtype)\n",
        "\n",
        "################################################################################\n",
        "# Above, we create a 5x3 matrix filled with zeros, \n",
        "# and query its datatype to find out that \n",
        "# the zeros are 32-bit floating point numbers, which is the default PyTorch.\n",
        "# \n",
        "# What if you wanted integers instead? \n",
        "# You can always override the default:\n",
        "################################################################################\n",
        "\n",
        "i = torch.ones((5, 3), dtype=torch.int16)\n",
        "print(i)\n",
        "\n",
        "################################################################################\n",
        "# You can see that when we do change the default, \n",
        "# the tensor helpfully reports this when printed.\n",
        "# \n",
        "# It’s common to initialize learning weights randomly, \n",
        "# often with a specific seed for the PRNG for reproducibility of results:\n",
        "################################################################################\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "r1 = torch.rand(2, 2)\n",
        "print('A random tensor:')\n",
        "print(r1)\n",
        "\n",
        "r2 = torch.rand(2, 2)\n",
        "print('\\nA different random tensor:')\n",
        "print(r2) # new values\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "r3 = torch.rand(2, 2)\n",
        "print('\\nShould match r1:')\n",
        "print(r3) # repeats values of r1 because of re-seed\n",
        "\n",
        "################################################################################\n",
        "# PyTorch tensors perform arithmetic operations intuitively. \n",
        "# Tensors of similar shapes may be added, multiplied, etc. \n",
        "# Operations with scalars are distributed over the tensor:\n",
        "################################################################################ \n",
        "\n",
        "ones = torch.ones(2, 3)\n",
        "print(ones)\n",
        "\n",
        "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
        "print(twos)\n",
        "\n",
        "threes = ones + twos       # addition allowed because shapes are similar\n",
        "print(threes)              # tensors are added element-wise\n",
        "print(threes.shape)        # this has the same dimensions as input tensors\n",
        "\n",
        "r1 = torch.rand(2, 3)\n",
        "r2 = torch.rand(3, 2)\n",
        "# uncomment this line to get a runtime error\n",
        "# r3 = r1 + r2\n",
        "\n",
        "################################################################################\n",
        "# Here’s a small sample of the mathematical operations available:\n",
        "################################################################################\n",
        "\n",
        "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\n",
        "print('A random matrix, r:')\n",
        "print(r)\n",
        "\n",
        "# Common mathematical operations are supported:\n",
        "print('\\nAbsolute value of r:')\n",
        "print(torch.abs(r))\n",
        "\n",
        "# ...as are trigonometric functions:\n",
        "print('\\nInverse sine of r:')\n",
        "print(torch.asin(r))\n",
        "\n",
        "# ...and linear algebra operations like determinant and singular value decomposition\n",
        "print('\\nDeterminant of r:')\n",
        "print(torch.det(r))\n",
        "print('\\nSingular value decomposition of r:')\n",
        "print(torch.svd(r))\n",
        "\n",
        "# ...and statistical and aggregate operations:\n",
        "print('\\nAverage and standard deviation of r:')\n",
        "print(torch.std_mean(r))\n",
        "print('\\nMaximum value of r:')\n",
        "print(torch.max(r))\n",
        "\n",
        "################################################################################\n",
        "# There’s a good deal more to know about the power of PyTorch tensors,\n",
        "# including how to set them up for parallel computations on GPU - \n",
        "# we’ll be going into more depth in another video.\n",
        "# \n",
        "# PyTorch Models\n",
        "#\n",
        "# Follow along with the video beginning at `10:00 \n",
        "# <https://www.youtube.com/watch?v=IC0_FRiX-sw&t=600s>`.\n",
        "#\n",
        "# Let’s talk about how we can express models in PyTorch\n",
        "################################################################################\n",
        "\n",
        "import torch                     # for all things PyTorch\n",
        "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
        "import torch.nn.functional as F  # for the activation function\n",
        "\n",
        "################################################################################\n",
        "# .. figure:: /_static/img/mnist.png\n",
        "#    :alt: le-net-5 diagram\n",
        "#\n",
        "# *Figure: LeNet-5*\n",
        "# \n",
        "# Above is a diagram of LeNet-5, \n",
        "# one of the earliest convolutional neural nets, and \n",
        "# one of the drivers of the explosion in Deep Learning. \n",
        "# It was built to read small images of handwritten numbers (the MNIST dataset),\n",
        "# and correctly classify which digit was represented in the image.\n",
        "# \n",
        "# Here’s the abridged version of how it works:\n",
        "# \n",
        "# -  Layer C1 is a convolutional layer, meaning that  \n",
        "#    it scans the input image for features it learned during training.\n",
        "#    It outputs a map of where it saw each of its learned features in the image.\n",
        "#    This “activation map” is downsampled in layer S2.\n",
        "# -  Layer C3 is another convolutional layer, \n",
        "#    this time scanning C1’s activation map for *combinations* of features. \n",
        "#    It also puts out an activation map describing \n",
        "#    the spatial locations of these feature combinations, \n",
        "#    which is downsampled in layer S4.\n",
        "# -  Finally, the fully-connected layers at the end, F5, F6, and OUTPUT,\n",
        "#    are a *classifier* that takes the final activation map, and\n",
        "#    classifies it into one of ten bins representing the 10 digits.\n",
        "# \n",
        "# How do we express this simple neural network in code?\n",
        "################################################################################\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel (black & white)\n",
        "        # 6 output channels\n",
        "        # 3x3 square convolution kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Looking over this code, you should be able to spot \n",
        "# some structural similarities with the diagram above.\n",
        "# \n",
        "# This demonstrates the structure of a typical PyTorch model: \n",
        "#\n",
        "# -  It inherits from ``torch.nn.Module`` \n",
        "#    - modules may be nested -  \n",
        "#    in fact, even the ``Conv2d`` and ``Linear`` layer classes inherit \n",
        "#    from ``torch.nn.Module``.\n",
        "# -  A model will have an ``__init__()`` function, \n",
        "#    where it instantiates its layers, and \n",
        "#    loads any data artifacts it might need \n",
        "#    (e.g., an NLP model might load a vocabulary).\n",
        "# -  A model will have a ``forward()`` function. \n",
        "#    This is where the actual computation happens: \n",
        "#    An input is passed through the network layers and various functions \n",
        "#    to generate an output.\n",
        "# -  Other than that, \n",
        "#    you can build out your model class like any other Python class, \n",
        "#    adding whatever properties and methods you need to \n",
        "#    support your model’s computation.\n",
        "# \n",
        "# Let’s instantiate this object and run a sample input through it.\n",
        "################################################################################\n",
        "\n",
        "net = LeNet()\n",
        "print(net)                         # what does the object tell us about itself?\n",
        "\n",
        "input = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\n",
        "print('\\nImage batch shape:')\n",
        "print(input.shape)\n",
        "\n",
        "output = net(input)                # we don't call forward() directly\n",
        "print('\\nRaw output:')\n",
        "print(output)\n",
        "print(output.shape)\n",
        "\n",
        "################################################################################\n",
        "# There are a few important things happening above:\n",
        "# \n",
        "# First, we instantiate the ``LeNet`` class, and we print the ``net`` object.  \n",
        "# A subclass of ``torch.nn.Module`` will report \n",
        "# the layers it has created and their shapes and parameters. \n",
        "# This can provide a handy overview of a model \n",
        "# if you want to get the gist of its processing.\n",
        "# \n",
        "# Below that, \n",
        "# we create a dummy input representing a 32x32 image with 1 color channel. \n",
        "# Normally, you would load an image tile and \n",
        "# convert it to a tensor of this shape.\n",
        "# \n",
        "# You may have noticed \n",
        "# an extra dimension to our tensor - the *batch dimension.* \n",
        "# PyTorch models assume they are working on *batches* of data\n",
        "# - for example, \n",
        "# a batch of 16 of our image tiles would have the shape ``(16, 1, 32, 32)``. \n",
        "# Since we’re only using one image, \n",
        "# we create a batch of 1 with shape ``(1, 1, 32, 32)``.\n",
        "# \n",
        "# We ask the model for an inference by calling it like a function:\n",
        "# ``net(input)``. \n",
        "# The output of this call represents \n",
        "# the model’s confidence that the input represents a particular digit. \n",
        "# (Since this instance of the model hasn’t learned anything yet, \n",
        "# we shouldn’t expect to see any signal in the output.) \n",
        "# Looking at the shape of ``output``, \n",
        "# we can see that it also has a batch dimension, \n",
        "# the size of which should always match the input batch dimension. \n",
        "# If we had passed in an input batch of 16 instances, \n",
        "# ``output`` would have a shape of ``(16, 10)``.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Datasets and Dataloaders\n",
        "#\n",
        "# Follow along with the video beginning at \n",
        "# `14:00 <https://www.youtube.com/watch?v=IC0_FRiX-sw&t=840s>`.\n",
        "#\n",
        "# Below, we’re going to demonstrate using one of the ready-to-download,\n",
        "# open-access datasets from TorchVision, \n",
        "# how to transform the images for consumption by your model, and \n",
        "# how to use the DataLoader to feed batches of data to your model.\n",
        "#\n",
        "# The first thing we need to do is \n",
        "# transform our incoming images into a PyTorch tensor.\n",
        "################################################################################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "################################################################################\n",
        "# Here, we specify two transformations for our input:\n",
        "#\n",
        "# -  ``transforms.ToTensor()`` \n",
        "#    converts images loaded by Pillow into PyTorch tensors.\n",
        "# -  ``transforms.Normalize()`` \n",
        "#    adjusts the values of the tensor so that \n",
        "#    their average is zero and their standard deviation is 0.5. \n",
        "#    Most activation functions have their strongest gradients around x = 0, so\n",
        "#    centering our data there can speed learning.\n",
        "# \n",
        "# There are many more transforms available, including \n",
        "# cropping, centering,, rotation, and reflection.\n",
        "# \n",
        "# Next, we’ll create an instance of the CIFAR10 dataset. \n",
        "# This is a set of 32x32 color image tiles representing 10 classes of objects: \n",
        "# 6 of animals (bird, cat, deer, dog, frog, horse) and \n",
        "# 4 of vehicles (airplane, automobile, ship, truck):\n",
        "################################################################################\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "################################################################################\n",
        "# .. note::\n",
        "#      When you run the cell above, it may take a little time  \n",
        "#      for the dataset to download.\n",
        "# \n",
        "# This is an example of creating a dataset object in PyTorch. \n",
        "# Downloadable datasets (like CIFAR-10 above) are \n",
        "# subclasses of ``torch.utils.data.Dataset``. \n",
        "# ``Dataset`` classes in PyTorch include the downloadable datasets \n",
        "# in TorchVision, Torchtext, and TorchAudio, as well as  \n",
        "# utility dataset classes such as ``torchvision.datasets.ImageFolder``,\n",
        "# which will read a folder of labeled images. \n",
        "# You can also create your own subclasses of ``Dataset``.\n",
        "# \n",
        "# When we instantiate our dataset, we need to tell it a few things:\n",
        "#\n",
        "# -  The filesystem path to where we want the data to go. \n",
        "# -  Whether or not we are using this set for training; \n",
        "#    most datasets will be split into training and test subsets.\n",
        "# -  Whether we would like to download the dataset if we haven’t already.\n",
        "# -  The transformations we want to apply to the data.\n",
        "# \n",
        "# Once your dataset is ready, you can give it to the ``DataLoader``:\n",
        "################################################################################\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "################################################################################\n",
        "# A ``Dataset`` subclass wraps access to the data, and \n",
        "# is specialized to the type of data it’s serving. \n",
        "# The ``DataLoader`` knows *nothing* about the data, but \n",
        "# organizes the input tensors served by the ``Dataset`` \n",
        "# into batches with the parameters you specify.\n",
        "# \n",
        "# In the example above, \n",
        "# we’ve asked a ``DataLoader`` to give us batches of 4 images from ``trainset``, \n",
        "# randomizing their order (``shuffle=True``),\n",
        "# and we told it to spin up two workers to load data from disk.\n",
        "# \n",
        "# It’s good practice to visualize the batches your ``DataLoader`` serves:\n",
        "################################################################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "################################################################################\n",
        "# Running the above cell should show you a strip of four images, \n",
        "# and the correct label for each.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Training Your PyTorch Model\n",
        "#\n",
        "# Follow along with the video beginning at \n",
        "# `17:10 <https://www.youtube.com/watch?v=IC0_FRiX-sw&t=1030s>`.\n",
        "#\n",
        "# Let’s put all the pieces together, and train a model:\n",
        "################################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "################################################################################\n",
        "# First, we’ll need training and test datasets. \n",
        "# If you haven’t already, run the cell below \n",
        "# to make sure the dataset is downloaded. (It may take a minute.)\n",
        "################################################################################\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "################################################################################\n",
        "# We’ll run our check on the output from ``DataLoader``:\n",
        "################################################################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "################################################################################\n",
        "# This is the model we’ll train. \n",
        "# If it looks familiar, that’s because it’s a variant of LeNet \n",
        "# - discussed earlier in this video - adapted for 3-color images.\n",
        "################################################################################ \n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "################################################################################\n",
        "# The last ingredients we need are a loss function and an optimizer:\n",
        "################################################################################\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "################################################################################\n",
        "# The loss function, as discussed earlier in this video, is a measure of\n",
        "# how far from our ideal output the model’s prediction was. \n",
        "# Cross-entropy loss is \n",
        "# a typical loss function for classification models like ours.\n",
        "# \n",
        "# The **optimizer** is what drives the learning. \n",
        "# Here we have created an optimizer that implements \n",
        "# *stochastic gradient descent,* \n",
        "# one of the more straightforward optimization algorithms. \n",
        "# Besides parameters of the algorithm, \n",
        "# like the learning rate (``lr``) and momentum, \n",
        "# we also pass in ``net.parameters()``, \n",
        "# which is a collection of all the learning weights in the model \n",
        "# - which is what the optimizer adjusts.\n",
        "# \n",
        "# Finally, all of this is assembled into the training loop. \n",
        "# Go ahead and run this cell, \n",
        "# as it will likely take a few minutes to execute:\n",
        "################################################################################ \n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "################################################################################\n",
        "# Here, we are doing only **2 training epochs** (line 1) \n",
        "# - that is, two passes over the training dataset. \n",
        "# Each pass has an inner loop that **iterates over the training data** (line 4), \n",
        "# serving batches of transformed input images and their correct labels.\n",
        "# \n",
        "# **Zeroing the gradients** (line 9) is an important step. \n",
        "# Gradients are accumulated over a batch; \n",
        "# if we do not reset them for every batch, they will keep accumulating, \n",
        "# which will provide incorrect gradient values, making learning impossible.\n",
        "# \n",
        "# In line 12, we **ask the model for its predictions** on this batch. \n",
        "# In the following line (13), we compute the loss \n",
        "# - the difference between\n",
        "# ``outputs`` (the model prediction) and ``labels`` (the correct output).\n",
        "# \n",
        "# In line 14, we do the ``backward()`` pass, \n",
        "# and calculate the gradients that will direct the learning.\n",
        "# \n",
        "# In line 15, the optimizer performs one learning step \n",
        "# - it uses the gradients from the ``backward()`` call \n",
        "# to nudge the learning weights \n",
        "# in the direction it thinks will reduce the loss.\n",
        "# \n",
        "# The remainder of the loop does some light reporting on the epoch number,\n",
        "# how many training instances have been completed, and \n",
        "# what the collected loss is over the training loop.\n",
        "# \n",
        "# **When you run the cell above,** you should see something like this:\n",
        "# \n",
        "# ::\n",
        "# \n",
        "#    [1,  2000] loss: 2.235\n",
        "#    [1,  4000] loss: 1.940\n",
        "#    [1,  6000] loss: 1.713\n",
        "#    [1,  8000] loss: 1.573\n",
        "#    [1, 10000] loss: 1.507\n",
        "#    [1, 12000] loss: 1.442\n",
        "#    [2,  2000] loss: 1.378\n",
        "#    [2,  4000] loss: 1.364\n",
        "#    [2,  6000] loss: 1.349\n",
        "#    [2,  8000] loss: 1.319\n",
        "#    [2, 10000] loss: 1.284\n",
        "#    [2, 12000] loss: 1.267\n",
        "#    Finished Training\n",
        "# \n",
        "# Note that the loss is monotonically descending, \n",
        "# indicating that our model is continuing to \n",
        "# improve its performance on the training dataset.\n",
        "# \n",
        "# As a final step, \n",
        "# we should check that the model is actually doing *general* learning, \n",
        "# and not simply “memorizing” the dataset. \n",
        "# This is called **overfitting,** and \n",
        "# usually indicates that the dataset is \n",
        "# too small (not enough examples for general learning), or that \n",
        "# the model has more learning parameters than it needs \n",
        "# to correctly model the dataset.\n",
        "# \n",
        "# This is the reason datasets are split into training and test subsets -\n",
        "# to test the generality of the model, \n",
        "# we ask it to make predictions on data it hasn’t trained on:\n",
        "################################################################################ \n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "################################################################################\n",
        "# If you followed along, \n",
        "# you should see that the model is roughly 50% accurate at this point. \n",
        "# That’s not exactly state-of-the-art, but it’s\n",
        "# far better than the 10% accuracy we’d expect from a random output. \n",
        "# This demonstrates that some general learning did happen in the model.\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "XdJFh_Exn-gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### CNN - CIFAR10 [beginner_source/blitz/cifar10_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/blitz/cifar10_tutorial.py)"
      ],
      "metadata": {
        "id": "Z-T2lwEodZNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training a Classifier\n",
        "=====================\n",
        "\n",
        "This is it. You have seen how to define neural networks, compute loss and make\n",
        "updates to the weights of the network.\n",
        "\n",
        "Now you might be thinking,\n",
        "\n",
        "What about data?\n",
        "----------------\n",
        "\n",
        "Generally, when you have to deal with image, text, audio or video data,\n",
        "you can use standard python packages that load data into a numpy array.\n",
        "Then you can convert this array into a ``torch.*Tensor``.\n",
        "\n",
        "-  For images, packages such as Pillow, OpenCV are useful\n",
        "-  For audio, packages such as scipy and librosa\n",
        "-  For text, either raw Python or Cython based loading, or NLTK and\n",
        "   SpaCy are useful\n",
        "\n",
        "Specifically for vision, we have created a package called\n",
        "``torchvision``, that has data loaders for common datasets such as\n",
        "ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz.,\n",
        "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
        "\n",
        "This provides a huge convenience and avoids writing boilerplate code.\n",
        "\n",
        "For this tutorial, we will use the CIFAR10 dataset.\n",
        "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
        "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
        "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
        "\n",
        ".. figure:: /_static/img/cifar10.png\n",
        "   :alt: cifar10\n",
        "\n",
        "   cifar10\n",
        "\n",
        "\n",
        "Training an image classifier\n",
        "----------------------------\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1. Load and normalize the CIFAR10 training and test datasets using\n",
        "   ``torchvision``\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data\n",
        "\n",
        "1. Load and normalize CIFAR10\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "Using ``torchvision``, it’s extremely easy to load CIFAR10.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "################################################################################\n",
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1].\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# .. note::\n",
        "#     If running on Windows and you get a BrokenPipeError, try setting\n",
        "#     the num_worker of torch.utils.data.DataLoader() to 0.\n",
        "################################################################################\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "################################################################################\n",
        "# Let us show some of the training images, for fun.\n",
        "################################################################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
        "\n",
        "################################################################################\n",
        "# 2. Define a Convolutional Neural Network\n",
        "# \n",
        "# Copy the neural network from the Neural Networks section before and modify it to\n",
        "# take 3-channel images (instead of 1-channel images as it was defined).\n",
        "################################################################################\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "################################################################################\n",
        "# 3. Define a Loss function and optimizer\n",
        "# \n",
        "# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "################################################################################\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "################################################################################\n",
        "# 4. Train the network\n",
        "#\n",
        "# This is when things start to get interesting.\n",
        "# We simply have to loop over our data iterator, and feed the inputs to the\n",
        "# network and optimize.\n",
        "################################################################################\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "################################################################################\n",
        "# Let's quickly save our trained model:\n",
        "################################################################################\n",
        "\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "################################################################################\n",
        "# See `here <https://pytorch.org/docs/stable/notes/serialization.html>`_\n",
        "# for more details on saving PyTorch models.\n",
        "#\n",
        "# 5. Test the network on the test data\n",
        "#\n",
        "# We have trained the network for 2 passes over the training dataset.\n",
        "# But we need to check if the network has learnt anything at all.\n",
        "#\n",
        "# We will check this by predicting the class label that the neural network\n",
        "# outputs, and checking it against the ground-truth. If the prediction is\n",
        "# correct, we add the sample to the list of correct predictions.\n",
        "#\n",
        "# Okay, first step. Let us display an image from the test set to get familiar.\n",
        "################################################################################\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "\n",
        "################################################################################\n",
        "# Next, let's load back in our saved model (note: saving and re-loading the model\n",
        "# wasn't necessary here, we only did it to illustrate how to do so):\n",
        "################################################################################\n",
        "\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "################################################################################\n",
        "# Okay, now let us see what the neural network thinks these examples above are:\n",
        "################################################################################\n",
        "\n",
        "outputs = net(images)\n",
        "\n",
        "################################################################################\n",
        "# The outputs are energies for the 10 classes.\n",
        "# The higher the energy for a class, the more the network\n",
        "# thinks that the image is of the particular class.\n",
        "# So, let's get the index of the highest energy:\n",
        "################################################################################\n",
        "\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))\n",
        "\n",
        "################################################################################\n",
        "# The results seem pretty good.\n",
        "#\n",
        "# Let us look at how the network performs on the whole dataset.\n",
        "################################################################################\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "################################################################################\n",
        "# That looks way better than chance, which is 10% accuracy (randomly picking\n",
        "# a class out of 10 classes).\n",
        "# Seems like the network learnt something.\n",
        "#\n",
        "# Hmmm, what are the classes that performed well, and the classes that did\n",
        "# not perform well:\n",
        "################################################################################\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "\n",
        "################################################################################\n",
        "# Okay, so what next?\n",
        "#\n",
        "# How do we run these neural networks on the GPU?\n",
        "#\n",
        "# Training on GPU\n",
        "# ----------------\n",
        "# Just like how you transfer a Tensor onto the GPU, you transfer the neural\n",
        "# net onto the GPU.\n",
        "#\n",
        "# Let's first define our device as the first visible cuda device if we have\n",
        "# CUDA available:\n",
        "################################################################################\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)\n",
        "\n",
        "################################################################################\n",
        "# The rest of this section assumes that ``device`` is a CUDA device.\n",
        "#\n",
        "# Then these methods will recursively go over all modules and convert their\n",
        "# parameters and buffers to CUDA tensors:\n",
        "#\n",
        "# .. code:: python\n",
        "#\n",
        "#     net.to(device)\n",
        "#\n",
        "#\n",
        "# Remember that you will have to send the inputs and targets at every step\n",
        "# to the GPU too:\n",
        "#\n",
        "# .. code:: python\n",
        "#\n",
        "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
        "#\n",
        "# Why don't I notice MASSIVE speedup compared to CPU? Because your network\n",
        "# is really small.\n",
        "#\n",
        "# **Exercise:** Try increasing the width of your network (argument 2 of\n",
        "# the first ``nn.Conv2d``, and argument 1 of the second ``nn.Conv2d`` –\n",
        "# they need to be the same number), see what kind of speedup you get.\n",
        "#\n",
        "# **Goals achieved**:\n",
        "#\n",
        "# - Understanding PyTorch's Tensor library and neural networks at a high level.\n",
        "# - Train a small neural network to classify images\n",
        "#\n",
        "# Training on multiple GPUs\n",
        "# -------------------------\n",
        "# If you want to see even more MASSIVE speedup using all of your GPUs,\n",
        "# please check out :doc:`data_parallel_tutorial`.\n",
        "#\n",
        "# Where do I go next?\n",
        "# -------------------\n",
        "#\n",
        "# -  :doc:`Train neural nets to play video games </intermediate/reinforcement_q_learning>`\n",
        "# -  `Train a state-of-the-art ResNet network on imagenet`_\n",
        "# -  `Train a face generator using Generative Adversarial Networks`_\n",
        "# -  `Train a word-level language model using Recurrent LSTM networks`_\n",
        "# -  `More examples`_\n",
        "# -  `More tutorials`_\n",
        "# -  `Discuss PyTorch on the Forums`_\n",
        "# -  `Chat with other users on Slack`_\n",
        "#\n",
        "# .. _Train a state-of-the-art ResNet network on imagenet: https://github.com/pytorch/examples/tree/master/imagenet\n",
        "# .. _Train a face generator using Generative Adversarial Networks: https://github.com/pytorch/examples/tree/master/dcgan\n",
        "# .. _Train a word-level language model using Recurrent LSTM networks: https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "# .. _More examples: https://github.com/pytorch/examples\n",
        "# .. _More tutorials: https://github.com/pytorch/tutorials\n",
        "# .. _Discuss PyTorch on the Forums: https://discuss.pytorch.org/\n",
        "# .. _Chat with other users on Slack: https://pytorch.slack.com/messages/beginner/\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "hUR4iaSEdZWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 8 PCA to VAE***"
      ],
      "metadata": {
        "id": "vzpW2lItDqX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### PCA"
      ],
      "metadata": {
        "id": "6syGGl9hDqpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch; torch.manual_seed(0) \n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    train_dataset = datasets.MNIST(root='../data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "    test_dataset = datasets.MNIST(root='../data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "    batch_size = 64\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
        "    return train_loader, test_loader \n",
        "\n",
        "\n",
        "class PCA_Class(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Linear(784, 20)\n",
        "        self.decoder = nn.Linear(20, 784)\n",
        "  \n",
        "    def forward(self, x): # (64,1,28,28) or (64,28,28)\n",
        "        out = x.view(x.size(0), -1) # (64,784)\n",
        "        out = self.encoder(out) # (64,20)\n",
        "        out = self.decoder(out) # (64,784)\n",
        "        out = out.view(x.size()) # (64,1,28,28) or (64,28,28)\n",
        "        return out\n",
        "  \n",
        "    def get_codes(self, x): # (64,1,28,28) or (64,28,28)\n",
        "        out = x.view(x.size(0), -1) # (64,784)\n",
        "        out = self.encoder(out) # (64,20)\n",
        "        return out # (64,20)\n",
        "    \n",
        "    \n",
        "def train(model, Loss, optimizer, num_epochs, train_loader, test_loader, device):\n",
        "    train_loss_arr = []\n",
        "    test_loss_arr = []\n",
        "\n",
        "    best_test_loss = 99999999\n",
        "    early_stop, early_stop_max = 0., 100.\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        epoch_loss = 0.\n",
        "        for batch_X, _ in train_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward Pass\n",
        "            model.train()\n",
        "            outputs = model(batch_X)\n",
        "            train_loss = Loss(outputs, batch_X)\n",
        "            epoch_loss += train_loss.data\n",
        "            # Backward and optimize\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss_arr.append(epoch_loss / len(train_loader.dataset))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            model.eval()\n",
        "            test_loss = 0.\n",
        "            for batch_X, _ in test_loader:\n",
        "                batch_X = batch_X.to(device)\n",
        "                # Forward Pass\n",
        "                outputs = model(batch_X)\n",
        "                batch_loss = Loss(outputs, batch_X)\n",
        "                test_loss += batch_loss.data\n",
        "            test_loss_arr.append(test_loss)\n",
        "\n",
        "            if best_test_loss > test_loss:\n",
        "                best_test_loss = test_loss\n",
        "                early_stop = 0\n",
        "                print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f} *'.format(epoch, num_epochs, epoch_loss, test_loss))\n",
        "            else:\n",
        "                early_stop += 1\n",
        "                print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}'.format(epoch, num_epochs, epoch_loss, test_loss))   \n",
        "\n",
        "        if early_stop >= early_stop_max:\n",
        "            break\n",
        "\n",
        "\n",
        "def main():\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.01\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_loader, test_loader = get_data()\n",
        "    \n",
        "    AE = PCA_Class().to(device)\n",
        "    AE_loss = nn.MSELoss()\n",
        "    AE_optimizer = optim.Adam(AE.parameters(), lr=learning_rate)\n",
        "\n",
        "    train(AE, AE_loss, AE_optimizer, num_epochs, train_loader, test_loader, device)\n",
        "\n",
        "    for imgs, labels in test_loader:\n",
        "        for img in imgs:\n",
        "            \n",
        "            fig, (ax0, ax1) = plt.subplots(1,2,figsize=(15,5))\n",
        "    \n",
        "            ax0.set_title(\"Original\")\n",
        "            ax0.imshow(img.squeeze(),cmap='binary')\n",
        "    \n",
        "            ax1.set_title(\"Compressed\")\n",
        "            img = img.to(device) \n",
        "            AE_img = AE(img).detach().cpu().numpy().reshape((28,28))\n",
        "            ax1.imshow(AE_img,cmap='binary')\n",
        "\n",
        "            for ax in (ax0, ax1):\n",
        "                ax.axis('off')\n",
        "    \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            break\n",
        "        break\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792,
          "referenced_widgets": [
            "a74cac3c56b24850a137701c854dc24a",
            "593fd4d8bcb14d978f7018c77e051600",
            "d555bd740e3a4d0eb050d3692e6088af",
            "264c0a2608d746abb50d8199bb07f5e4",
            "52e550ff7ad2455e8ecab9d1afccf94c",
            "0a59fda4e21a440c8cca073aa1d96b38",
            "1d9060ac61d0412cb034bd07a234feaa",
            "884ce49ee215460a9f253af22edef78c",
            "e22c660bbde945adbf216b2939001b88",
            "fd0592879da2438e9e780d2c53f319e2",
            "57cb89c3b128400785964b96e46b3820",
            "82980009c9ce436d84e5340d0624e355",
            "c18e7d32a3414710800376e4ad3f82ec",
            "5aee56b3ea534a53aba9f61bd43804d7",
            "78eb8bd2830d46a9a731073f482b80ac",
            "a30cbc82bff74ae3bf76149812b68cb2",
            "717426b5f7f34f90b6a9d605860c048b",
            "899031868bc3427a9d6d6cf8dfe1a93a",
            "f25ed6cb7274403eb8e8f779e7e9937f",
            "1cd7d9fb519d47ce9a9decd81c0e060c",
            "7dbe1a39536941388d1119c5f0f13cb4",
            "83ca57aceab24ca9aa7511b5594aba41",
            "968352747d664161b026a74193da6381",
            "f474464a36f94ae5a3d8e90db627b787",
            "b43a91c50aad421abde5694593500649",
            "4567f6ab0c3b4d698dd7a0b9b734ab91",
            "a1f90575be6b4851a3cf5fe475be1eb6",
            "c4f35927e8ee49c49f4af002ba6808e3",
            "f219d2ca0fd54655b7622e48c5290249",
            "06ea5dee1337461bb87d3d7ba8ad154f",
            "bcc9a8bc772240ff9d75677163fc8585",
            "7d68b0852b344ab5879ae5712fbcb2d0",
            "33a58bd8564d491bb085fe8278772de4",
            "f8f1a63398f941b1a6703ad24cbb91dd",
            "f687fd8f1a1b43b2adaf2540f54ff051",
            "16eabf5e40cb4e50849fa3d0e21151ec",
            "12f8818fac9e46a38c27b70a6abe5e22",
            "6e7f10fe1d994d1c9b122f7ecb7988ad",
            "2560cb529df04494ac2270875df46b05",
            "32e6265cd48f42bbb17bc012b199dfa6",
            "b19493f59a024b71a188dbdba423f1fe",
            "661abc2e1ce442af9ad81aad257067a9",
            "c714aa539f8740738103f5a37d4a2d01",
            "d82b546705914748b1403edc4c9e3890"
          ]
        },
        "id": "E3UNB_C8D3Ur",
        "outputId": "fc05b8f7-90ce-4a5d-d162-e56b70bfce5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a74cac3c56b24850a137701c854dc24a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82980009c9ce436d84e5340d0624e355"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "968352747d664161b026a74193da6381"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8f1a63398f941b1a6703ad24cbb91dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Epoch [0/10], Train Loss: 29.7318, Test Loss: 3.8384 *\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### AE - Linear and Conv2d [Auto Encoder (Pytorch)](https://chioni.github.io/posts/ae/)"
      ],
      "metadata": {
        "id": "70vSgRibDqwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source\n",
        "# https://chioni.github.io/posts/ae/\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch; torch.manual_seed(0) \n",
        "import torch.nn.functional as F\n",
        "from random import sample\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    train_dataset = datasets.MNIST(root='../data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "    test_dataset = datasets.MNIST(root='../data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "    total_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n",
        "\n",
        "\n",
        "    # 데이터 중에서 5인 것과 아닌 것을 구별하자\n",
        "    normal_dataset = sample([total_dataset.__getitem__(idx) for idx in range(len(total_dataset)) \n",
        "                      if total_dataset.__getitem__(idx)[1] == 5], 6000)\n",
        "\n",
        "    anomal_dataset = sample([total_dataset.__getitem__(idx) for idx in range(len(total_dataset))\n",
        "                       if total_dataset.__getitem__(idx)[1] != 5], 1500)\n",
        "\n",
        "\n",
        "    # 모델을 학습할 때 완전히 5인 것만 학습하지 않고 다른 라벨도 조금 섞어줘서 semi-supervised하게 모델을 학습해주자\n",
        "    train_size, test_size = 5000, 500\n",
        "\n",
        "    train_dataset = normal_dataset[:train_size]\n",
        "    normal_train = train_dataset \n",
        "    anomal_train = anomal_dataset[:test_size]\n",
        "    train_dataset.extend(anomal_train)\n",
        "\n",
        "    test_dataset = normal_dataset[train_size:]\n",
        "    normal_test = test_dataset \n",
        "    anomal_test = anomal_dataset[test_size:]\n",
        "    test_dataset.extend(anomal_test)\n",
        "\n",
        "    batch_size = 512\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
        "    return train_loader, test_loader, normal_train, anomal_train, normal_test, anomal_test  \n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(input_dim, hidden_dim1),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim1, hidden_dim2),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.Linear(hidden_dim2, hidden_dim1),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim1, input_dim),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "  \n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size(0), -1)\n",
        "        out = self.encoder(out)\n",
        "        out = self.decoder(out)\n",
        "        out = out.view(x.size())\n",
        "        return out\n",
        "  \n",
        "    def get_codes(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    \n",
        "class ConvAutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Conv2d(1, 3, kernel_size = 5),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(3, 5, kernel_size = 5),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.ConvTranspose2d(5, 3, kernel_size = 5),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(3, 1, kernel_size = 5),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = self.decoder(out)\n",
        "        return out\n",
        "\n",
        "    def get_codes(self, x):\n",
        "        return self.encoder(x)\n",
        "    \n",
        "    \n",
        "def train(model, Loss, optimizer, num_epochs, train_loader, test_loader, device):\n",
        "    train_loss_arr = []\n",
        "    test_loss_arr = []\n",
        "\n",
        "    best_test_loss = 99999999\n",
        "    early_stop, early_stop_max = 0., 100.\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        epoch_loss = 0.\n",
        "        for batch_X, _ in train_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward Pass\n",
        "            model.train()\n",
        "            outputs = model(batch_X)\n",
        "            train_loss = Loss(outputs, batch_X)\n",
        "            epoch_loss += train_loss.data\n",
        "            # Backward and optimize\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss_arr.append(epoch_loss / len(train_loader.dataset))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            model.eval()\n",
        "            test_loss = 0.\n",
        "            for batch_X, _ in test_loader:\n",
        "                batch_X = batch_X.to(device)\n",
        "                # Forward Pass\n",
        "                outputs = model(batch_X)\n",
        "                batch_loss = Loss(outputs, batch_X)\n",
        "                test_loss += batch_loss.data\n",
        "            test_loss_arr.append(test_loss)\n",
        "\n",
        "            if best_test_loss > test_loss:\n",
        "                best_test_loss = test_loss\n",
        "                early_stop = 0\n",
        "                print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f} *'.format(epoch, num_epochs, epoch_loss, test_loss))\n",
        "            else:\n",
        "                early_stop += 1\n",
        "                print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}'.format(epoch, num_epochs, epoch_loss, test_loss))   \n",
        "\n",
        "        if early_stop >= early_stop_max:\n",
        "            break\n",
        "\n",
        "\n",
        "def main():\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.01\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_loader, test_loader, normal_train, anomal_train, normal_test, anomal_test = get_data()\n",
        "    \n",
        "    AE = AutoEncoder(28 * 28, 64, 32).to(device)\n",
        "    AE_loss = nn.MSELoss()\n",
        "    AE_optimizer = optim.Adam(AE.parameters(), lr=learning_rate)\n",
        "\n",
        "    CAE = ConvAutoEncoder().to(device)\n",
        "    CAE_loss = nn.MSELoss()\n",
        "    CAE_optimizer = optim.Adam(CAE.parameters(), lr=learning_rate)\n",
        "\n",
        "    train(AE, AE_loss, AE_optimizer, num_epochs, train_loader, test_loader, device)\n",
        "    train(CAE, CAE_loss, CAE_optimizer, num_epochs, train_loader, test_loader, device)\n",
        "    \n",
        "    AE_normal, AE_anomal = [], []\n",
        "    CAE_normal,CAE_anomal = [], []\n",
        "    for X, _ in normal_train:\n",
        "        X = X.to(device)\n",
        "        # Forward Pass\n",
        "        AE_output = AE(X)\n",
        "        CAE_output = CAE(X.view((1,1,28,28)))\n",
        "        if _ == 5: \n",
        "            AE_normal.append(AE_loss(AE_output, X).item())\n",
        "            CAE_normal.append(CAE_loss(CAE_output.view(1,28,28), X).item())\n",
        "        else: \n",
        "            AE_anomal.append(AE_loss(AE_output, X).item())\n",
        "            CAE_anomal.append(CAE_loss(CAE_output.view(1,28,28), X).item())\n",
        "            \n",
        "    fig, (ax0, ax1) = plt.subplots(1,2,figsize=(15,5))\n",
        "    \n",
        "    ax0.set_title(\"Simple AutoEncoder\")\n",
        "    _, bins, _ = ax0.hist(AE_anomal, bins=100, density=True, label='abnormal', alpha=0.3)\n",
        "    ax0.hist(AE_normal, bins=bins, density=True, label='normal', alpha=0.3)\n",
        "    \n",
        "    ax1.set_title(\"Conv AutoEncoder\")\n",
        "    _, bins, _ = ax1.hist(CAE_anomal, bins=100, density=True, label='abnormal', alpha=0.3)\n",
        "    ax1.hist(CAE_normal, bins=bins, density=True, label='normal', alpha=0.3)\n",
        "    \n",
        "    for ax in (ax0, ax1):\n",
        "        ax.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    y_true = []\n",
        "    AE_y_pred, CAE_y_pred = [],[]\n",
        "    for X, y in normal_test + anomal_test:\n",
        "        X = X.to(device)\n",
        "        # Forward Pass\n",
        "        AE_output, CAE_output = AE(X), CAE(X.view((1,1,28,28)))\n",
        "        if y == 5: \n",
        "            y_true.append(1)\n",
        "        else: \n",
        "            y_true.append(0)\n",
        "            \n",
        "        if AE_loss(AE_output, X).item() < 0.04: \n",
        "            AE_y_pred.append(1)\n",
        "        else: \n",
        "            AE_y_pred.append(0)\n",
        "            \n",
        "        if CAE_loss(CAE_output.view(1,28,28), X).item() < 0.003: \n",
        "            CAE_y_pred.append(1)\n",
        "        else: \n",
        "            CAE_y_pred.append(0)  \n",
        "    \n",
        "    fig, (ax0, ax1) = plt.subplots(1,2,figsize=(15,5))\n",
        "    \n",
        "    ax0.set_title(\"Simple AutoEncoder\")\n",
        "    cm_AE = confusion_matrix(y_true, AE_y_pred, labels=[0, 1])\n",
        "    sns.heatmap(cm_AE, annot=True, ax=ax0, fmt='d', annot_kws={\"fontsize\":20})\n",
        "\n",
        "    ax1.set_title(\"Conv AutoEncoder\")\n",
        "    cm_CAE = confusion_matrix(y_true, CAE_y_pred, labels=[0, 1])\n",
        "    sns.heatmap(cm_CAE, annot=True, ax=ax1, fmt='d', annot_kws={\"fontsize\":20})\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "yvIe3i34E3bF",
        "outputId": "16b2dd9a-ea14-4cb9-8c22-28a04cc233a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/10], Train Loss: 0.8858, Test Loss: 0.2948 *\n",
            "Epoch [0/10], Train Loss: 0.6430, Test Loss: 0.1168 *\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### VAE [examples/blob/main/vae/main.py](https://github.com/pytorch/examples/blob/main/vae/main.py) [한국어 from AE to VAE 2022 11 03](https://www.youtube.com/watch?v=kUWj4rIoEkA&list=PLXziV1DL41oh1ruw4rplU04-rcz55zsrc&index=18) "
      ],
      "metadata": {
        "id": "X6LQD38_9nGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "################################################################################\n",
        "# [How to call module written with argparse in iPython notebook](https://stackoverflow.com/questions/30656777/how-to-call-module-written-with-argparse-in-ipython-notebook)\n",
        "# If all arguments have a default value, \n",
        "# then adding this to the top of the notebook should be enough:\n",
        "#     import sys\n",
        "#     sys.argv = ['']\n",
        "################################################################################\n",
        "\n",
        "import sys\n",
        "sys.argv = ['']\n",
        "\n",
        "################################################################################\n",
        "# I need a directory called \"results\" to save image\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "os.makedirs(\"results\",exist_ok=True)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "                    help='input batch size for training (default: 128)')\n",
        "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "                    help='number of epochs to train (default: 10)')\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                        help='disables macOS GPU training')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                    help='random seed (default: 1)')\n",
        "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                    help='how many batches to wait before logging training status')\n",
        "args = parser.parse_args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=args.batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        with torch.no_grad():\n",
        "            sample = torch.randn(64, 20).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "            save_image(sample.view(64, 1, 28, 28),\n",
        "                       'results/sample_' + str(epoch) + '.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI0XmIWt_7XW",
        "outputId": "06b88176-b826-4326-bc27-77d22a4332cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.513916\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 310.610535\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 240.650162\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 219.223282\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 215.092834\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 208.450150\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 203.374252\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 193.761414\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 194.701691\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 192.943787\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 179.935974\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 174.059189\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 182.615845\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 168.166260\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 166.474884\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 161.468369\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 162.560242\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 151.492172\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 157.339233\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 154.514923\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 155.953918\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 150.720627\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 153.107208\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 146.414719\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 141.649368\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 144.722427\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 144.666168\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 145.757019\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 137.389725\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 142.351166\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 139.231476\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 135.016708\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 141.466721\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 141.817535\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 140.084778\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 135.187820\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 140.311981\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 132.970703\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 131.564804\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 130.650513\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 131.395416\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 131.117462\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 132.647141\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 126.399261\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 137.452454\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 129.695908\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 132.370544\n",
            "====> Epoch: 1 Average loss: 164.3999\n",
            "====> Test set loss: 127.1312\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 123.400551\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 121.694229\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 126.027855\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 130.092682\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 129.918304\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 131.958038\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 127.191177\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 126.707947\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 129.505249\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 123.088737\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 124.846184\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 122.070251\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 126.800415\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 124.355583\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 124.957962\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 123.351974\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 124.220383\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 121.673820\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 122.104126\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 121.135086\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 120.269890\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 117.271950\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 121.590324\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 122.184265\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 113.772049\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 119.839844\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 116.891342\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 120.535057\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 119.203583\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 121.571419\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 117.791855\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 117.478806\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 120.247604\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 121.837173\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 117.096184\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 120.548645\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 121.445099\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 116.878960\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 120.193253\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 114.595978\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 119.027542\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 117.176300\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 116.074333\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 115.181000\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 116.626251\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 116.407516\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 117.282837\n",
            "====> Epoch: 2 Average loss: 121.4730\n",
            "====> Test set loss: 115.5182\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 119.812393\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 116.181084\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 117.979385\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 118.826073\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 118.300140\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 115.060806\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 114.772011\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 117.651718\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 115.713219\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 114.756004\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 115.088036\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 121.638176\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 116.541245\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 114.060516\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 116.793037\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 112.347046\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 114.994858\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 108.262062\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 115.619370\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 116.166336\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 114.771515\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 113.296280\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 111.888588\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 116.978378\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 113.229958\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 111.170731\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 115.284073\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 113.249512\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 115.299637\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 116.101105\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 113.700859\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 109.192627\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 110.903336\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 110.506554\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 116.920372\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 110.808762\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 112.163315\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 109.719086\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 112.233147\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 115.030220\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 116.133865\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 111.194016\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 110.952850\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 111.239288\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 111.056671\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 112.337738\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 113.904976\n",
            "====> Epoch: 3 Average loss: 114.3986\n",
            "====> Test set loss: 111.6140\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 119.927101\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 113.099297\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 114.474380\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 111.979301\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 113.996315\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 115.193130\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 115.614243\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 111.216278\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 110.935532\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 111.370071\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 114.633698\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 109.535095\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 113.227135\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 108.740433\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 109.787849\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 109.039825\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 110.221123\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 113.708534\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 109.808281\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 113.607368\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 107.947556\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 110.039047\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 107.909737\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 111.238861\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 111.636841\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 114.592972\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 114.395973\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 105.026215\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 110.897820\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 110.787949\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 108.271187\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 109.125755\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 110.882652\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 113.125580\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 110.288322\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 114.608009\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 111.146141\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 110.643524\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 109.027512\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 109.993134\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 111.724144\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 110.801956\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 109.860802\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 107.881287\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 106.389946\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 109.070030\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 112.113243\n",
            "====> Epoch: 4 Average loss: 111.5173\n",
            "====> Test set loss: 110.0446\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 107.220207\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 111.419189\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 110.443207\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 110.733215\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 107.499329\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 107.949326\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 109.386627\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 111.873756\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 110.902214\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 109.035873\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 114.665878\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 106.677757\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 112.690979\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 111.956528\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 107.250427\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 109.406822\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 107.425453\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 107.737129\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 112.936760\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 107.703629\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 112.490601\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 108.374435\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 107.784683\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 106.629303\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 105.404846\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 109.309616\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 114.137131\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 112.099640\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 111.573021\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 111.767700\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 110.517471\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 110.758888\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 107.208870\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 111.733521\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 108.347153\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 106.105995\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 108.584000\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 108.911949\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 114.895737\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 108.747696\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 112.489975\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 107.473259\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 107.304901\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 109.965561\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 109.803207\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 110.093475\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 113.812096\n",
            "====> Epoch: 5 Average loss: 109.7671\n",
            "====> Test set loss: 108.5274\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 106.595451\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 107.493309\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 112.212296\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 109.128990\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 105.629524\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 109.422958\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 113.110245\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 107.599716\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 113.056396\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 108.110641\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 107.254776\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 112.971840\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 103.641022\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 109.411407\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 106.513748\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 107.153824\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 107.102531\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 109.758301\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 104.173454\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 107.732803\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 104.276436\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 108.211731\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 111.330933\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 111.427231\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 112.934952\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 110.925354\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 109.430573\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 110.481567\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 105.335785\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 104.843369\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 111.716408\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 109.104828\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 111.151443\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 113.615601\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 110.466125\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 104.465347\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 106.281311\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 106.316940\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 105.622574\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 106.626862\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 107.629913\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 109.059311\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 110.086037\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 107.760559\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 106.883514\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 110.443115\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 105.637817\n",
            "====> Epoch: 6 Average loss: 108.6316\n",
            "====> Test set loss: 107.4445\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 106.583267\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 106.604034\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 107.066254\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 104.571602\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 106.792557\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 107.760315\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 108.598465\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 105.894798\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 108.224396\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 108.242493\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 105.871780\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 110.562965\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 106.484993\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 109.544769\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 108.122040\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 110.097595\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 109.173035\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 107.396324\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 108.176605\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 110.139526\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 103.833069\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 105.030624\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 102.045441\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 108.917725\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 106.576660\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 107.623634\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 110.931122\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 108.538857\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 107.054527\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 107.438957\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 109.466873\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 106.663834\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 110.683289\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 110.024933\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 107.380859\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 108.151833\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 110.887993\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 107.621986\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 106.357559\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 107.735786\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 105.710190\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 106.358932\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 105.751923\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 110.394012\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 106.712120\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 108.088638\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 106.939667\n",
            "====> Epoch: 7 Average loss: 107.7868\n",
            "====> Test set loss: 106.9430\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 107.996902\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 107.116913\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 103.831490\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 105.284073\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 106.703552\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 108.085220\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 109.366447\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 104.183258\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 106.124016\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 108.048706\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 107.668571\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 103.181107\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 107.624100\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 108.893463\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 105.308945\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 110.833496\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 101.992386\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 105.933136\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 106.874672\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 108.844406\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 107.601112\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 106.145737\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 108.087799\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 104.488327\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 108.159210\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 108.139221\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 105.836510\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 109.589256\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 107.030411\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 105.376511\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 108.862808\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 109.072113\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 105.294945\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 110.166283\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 107.337372\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 103.432220\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 105.940910\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 106.284218\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 107.932175\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 105.562088\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 106.523102\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 104.677956\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 105.782143\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 104.012817\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 108.898293\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 105.881935\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 105.740814\n",
            "====> Epoch: 8 Average loss: 107.1427\n",
            "====> Test set loss: 106.2717\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 107.913177\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 101.965012\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 108.402122\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 103.045654\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 103.946350\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 106.630104\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 107.161606\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 106.337791\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 107.146057\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 109.696449\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 105.854507\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 103.352432\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 109.654739\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 106.870949\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 107.828072\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 106.734283\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 114.162399\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 109.235992\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 111.620224\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 103.358948\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 108.734161\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 106.839233\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 106.739517\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 103.318466\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 108.387482\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 107.976921\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 107.490616\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 102.839897\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 107.930962\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 108.814598\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 104.950989\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 106.195168\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 107.267784\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 106.727844\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 105.430695\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 103.171455\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 107.744804\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 107.228569\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 107.133865\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 106.052505\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 103.376694\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 103.849770\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 104.881042\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 101.318665\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 103.777893\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 108.499504\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 102.416367\n",
            "====> Epoch: 9 Average loss: 106.6203\n",
            "====> Test set loss: 105.9227\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 102.777596\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 106.306168\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 104.921463\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 106.796227\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 106.219666\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 106.717453\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 105.422104\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 106.361282\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 109.275826\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 104.041466\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 103.900940\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 106.440041\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 105.603378\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 109.148239\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 105.880104\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 106.075867\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 105.669037\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 106.070358\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 106.682495\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 105.656845\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 106.049698\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 103.153679\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 108.605453\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 106.884735\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 107.513779\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 107.372627\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 103.051727\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 108.382904\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 108.626808\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 105.976768\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 108.687622\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 105.369141\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 104.615295\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 108.095512\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 104.024460\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 108.638412\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 106.239494\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 106.970825\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 103.362305\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 108.794334\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 105.945724\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 106.115036\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 106.540268\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 106.757278\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 109.478302\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 112.755409\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 107.189720\n",
            "====> Epoch: 10 Average loss: 106.1725\n",
            "====> Test set loss: 105.5328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 9 GAN***"
      ],
      "metadata": {
        "id": "GfJZxho8jQAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### DCGAN - CelebA [beginner_source/dcgan_faces_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/dcgan_faces_tutorial.py)"
      ],
      "metadata": {
        "id": "1dZCr3ZgzKM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "DCGAN Tutorial\n",
        "==============\n",
        "\n",
        "**Author**: `Nathan Inkawhich <https://github.com/inkawhich>`__\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Introduction\n",
        "# ------------\n",
        "# \n",
        "# This tutorial will give an introduction to DCGANs through an example. We\n",
        "# will train a generative adversarial network (GAN) to generate new\n",
        "# celebrities after showing it pictures of many real celebrities. Most of\n",
        "# the code here is from the dcgan implementation in\n",
        "# `pytorch/examples <https://github.com/pytorch/examples>`__, and this\n",
        "# document will give a thorough explanation of the implementation and shed\n",
        "# light on how and why this model works. But don’t worry, no prior\n",
        "# knowledge of GANs is required, but it may require a first-timer to spend\n",
        "# some time reasoning about what is actually happening under the hood.\n",
        "# Also, for the sake of time it will help to have a GPU, or two. Lets\n",
        "# start from the beginning.\n",
        "# \n",
        "# Generative Adversarial Networks\n",
        "# -------------------------------\n",
        "# \n",
        "# What is a GAN?\n",
        "# ~~~~~~~~~~~~~~\n",
        "# \n",
        "# GANs are a framework for teaching a DL model to capture the training\n",
        "# data’s distribution so we can generate new data from that same\n",
        "# distribution. GANs were invented by Ian Goodfellow in 2014 and first\n",
        "# described in the paper `Generative Adversarial\n",
        "# Nets <https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>`__.\n",
        "# They are made of two distinct models, a *generator* and a\n",
        "# *discriminator*. The job of the generator is to spawn ‘fake’ images that\n",
        "# look like the training images. The job of the discriminator is to look\n",
        "# at an image and output whether or not it is a real training image or a\n",
        "# fake image from the generator. During training, the generator is\n",
        "# constantly trying to outsmart the discriminator by generating better and\n",
        "# better fakes, while the discriminator is working to become a better\n",
        "# detective and correctly classify the real and fake images. The\n",
        "# equilibrium of this game is when the generator is generating perfect\n",
        "# fakes that look as if they came directly from the training data, and the\n",
        "# discriminator is left to always guess at 50% confidence that the\n",
        "# generator output is real or fake.\n",
        "# \n",
        "# Now, lets define some notation to be used throughout tutorial starting\n",
        "# with the discriminator. Let :math:`x` be data representing an image.\n",
        "# :math:`D(x)` is the discriminator network which outputs the (scalar)\n",
        "# probability that :math:`x` came from training data rather than the\n",
        "# generator. Here, since we are dealing with images, the input to\n",
        "# :math:`D(x)` is an image of CHW size 3x64x64. Intuitively, :math:`D(x)`\n",
        "# should be HIGH when :math:`x` comes from training data and LOW when\n",
        "# :math:`x` comes from the generator. :math:`D(x)` can also be thought of\n",
        "# as a traditional binary classifier.\n",
        "# \n",
        "# For the generator’s notation, let :math:`z` be a latent space vector\n",
        "# sampled from a standard normal distribution. :math:`G(z)` represents the\n",
        "# generator function which maps the latent vector :math:`z` to data-space.\n",
        "# The goal of :math:`G` is to estimate the distribution that the training\n",
        "# data comes from (:math:`p_{data}`) so it can generate fake samples from\n",
        "# that estimated distribution (:math:`p_g`).\n",
        "# \n",
        "# So, :math:`D(G(z))` is the probability (scalar) that the output of the\n",
        "# generator :math:`G` is a real image. As described in `Goodfellow’s\n",
        "# paper <https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>`__,\n",
        "# :math:`D` and :math:`G` play a minimax game in which :math:`D` tries to\n",
        "# maximize the probability it correctly classifies reals and fakes\n",
        "# (:math:`logD(x)`), and :math:`G` tries to minimize the probability that\n",
        "# :math:`D` will predict its outputs are fake (:math:`log(1-D(G(z)))`).\n",
        "# From the paper, the GAN loss function is\n",
        "# \n",
        "# .. math:: \\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big]\n",
        "# \n",
        "# In theory, the solution to this minimax game is where\n",
        "# :math:`p_g = p_{data}`, and the discriminator guesses randomly if the\n",
        "# inputs are real or fake. However, the convergence theory of GANs is\n",
        "# still being actively researched and in reality models do not always\n",
        "# train to this point.\n",
        "# \n",
        "# What is a DCGAN?\n",
        "# ~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# A DCGAN is a direct extension of the GAN described above, except that it\n",
        "# explicitly uses convolutional and convolutional-transpose layers in the\n",
        "# discriminator and generator, respectively. It was first described by\n",
        "# Radford et. al. in the paper `Unsupervised Representation Learning With\n",
        "# Deep Convolutional Generative Adversarial\n",
        "# Networks <https://arxiv.org/pdf/1511.06434.pdf>`__. The discriminator\n",
        "# is made up of strided\n",
        "# `convolution <https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`__\n",
        "# layers, `batch\n",
        "# norm <https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d>`__\n",
        "# layers, and\n",
        "# `LeakyReLU <https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU>`__\n",
        "# activations. The input is a 3x64x64 input image and the output is a\n",
        "# scalar probability that the input is from the real data distribution.\n",
        "# The generator is comprised of\n",
        "# `convolutional-transpose <https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d>`__\n",
        "# layers, batch norm layers, and\n",
        "# `ReLU <https://pytorch.org/docs/stable/nn.html#relu>`__ activations. The\n",
        "# input is a latent vector, :math:`z`, that is drawn from a standard\n",
        "# normal distribution and the output is a 3x64x64 RGB image. The strided\n",
        "# conv-transpose layers allow the latent vector to be transformed into a\n",
        "# volume with the same shape as an image. In the paper, the authors also\n",
        "# give some tips about how to setup the optimizers, how to calculate the\n",
        "# loss functions, and how to initialize the model weights, all of which\n",
        "# will be explained in the coming sections.\n",
        "# \n",
        "\n",
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Inputs\n",
        "# ------\n",
        "# \n",
        "# Let’s define some inputs for the run:\n",
        "# \n",
        "# -  **dataroot** - the path to the root of the dataset folder. We will\n",
        "#    talk more about the dataset in the next section\n",
        "# -  **workers** - the number of worker threads for loading the data with\n",
        "#    the DataLoader\n",
        "# -  **batch_size** - the batch size used in training. The DCGAN paper\n",
        "#    uses a batch size of 128\n",
        "# -  **image_size** - the spatial size of the images used for training.\n",
        "#    This implementation defaults to 64x64. If another size is desired,\n",
        "#    the structures of D and G must be changed. See\n",
        "#    `here <https://github.com/pytorch/examples/issues/70>`__ for more\n",
        "#    details\n",
        "# -  **nc** - number of color channels in the input images. For color\n",
        "#    images this is 3\n",
        "# -  **nz** - length of latent vector\n",
        "# -  **ngf** - relates to the depth of feature maps carried through the\n",
        "#    generator\n",
        "# -  **ndf** - sets the depth of feature maps propagated through the\n",
        "#    discriminator\n",
        "# -  **num_epochs** - number of training epochs to run. Training for\n",
        "#    longer will probably lead to better results but will also take much\n",
        "#    longer\n",
        "# -  **lr** - learning rate for training. As described in the DCGAN paper,\n",
        "#    this number should be 0.0002\n",
        "# -  **beta1** - beta1 hyperparameter for Adam optimizers. As described in\n",
        "#    paper, this number should be 0.5\n",
        "# -  **ngpu** - number of GPUs available. If this is 0, code will run in\n",
        "#    CPU mode. If this number is greater than 0 it will run on that number\n",
        "#    of GPUs\n",
        "# \n",
        "\n",
        "# Root directory for dataset\n",
        "dataroot = \"data/celeba\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Data\n",
        "# ----\n",
        "# \n",
        "# In this tutorial we will use the `Celeb-A Faces\n",
        "# dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__ which can\n",
        "# be downloaded at the linked site, or in `Google\n",
        "# Drive <https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg>`__.\n",
        "# The dataset will download as a file named *img_align_celeba.zip*. Once\n",
        "# downloaded, create a directory named *celeba* and extract the zip file\n",
        "# into that directory. Then, set the *dataroot* input for this notebook to\n",
        "# the *celeba* directory you just created. The resulting directory\n",
        "# structure should be:\n",
        "# \n",
        "# ::\n",
        "# \n",
        "#    /path/to/celeba\n",
        "#        -> img_align_celeba  \n",
        "#            -> 188242.jpg\n",
        "#            -> 173822.jpg\n",
        "#            -> 284702.jpg\n",
        "#            -> 537394.jpg\n",
        "#               ...\n",
        "# \n",
        "# This is an important step because we will be using the ImageFolder\n",
        "# dataset class, which requires there to be subdirectories in the\n",
        "# dataset’s root folder. Now, we can create the dataset, create the\n",
        "# dataloader, set the device to run on, and finally visualize some of the\n",
        "# training data.\n",
        "# \n",
        "\n",
        "# We can use an image folder dataset the way we have it setup.\n",
        "# Create the dataset\n",
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Implementation\n",
        "# --------------\n",
        "# \n",
        "# With our input parameters set and the dataset prepared, we can now get\n",
        "# into the implementation. We will start with the weight initialization\n",
        "# strategy, then talk about the generator, discriminator, loss functions,\n",
        "# and training loop in detail.\n",
        "# \n",
        "# Weight Initialization\n",
        "# ~~~~~~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# From the DCGAN paper, the authors specify that all model weights shall\n",
        "# be randomly initialized from a Normal distribution with mean=0,\n",
        "# stdev=0.02. The ``weights_init`` function takes an initialized model as\n",
        "# input and reinitializes all convolutional, convolutional-transpose, and\n",
        "# batch normalization layers to meet this criteria. This function is\n",
        "# applied to the models immediately after initialization.\n",
        "# \n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Generator\n",
        "# ~~~~~~~~~\n",
        "# \n",
        "# The generator, :math:`G`, is designed to map the latent space vector\n",
        "# (:math:`z`) to data-space. Since our data are images, converting\n",
        "# :math:`z` to data-space means ultimately creating a RGB image with the\n",
        "# same size as the training images (i.e. 3x64x64). In practice, this is\n",
        "# accomplished through a series of strided two dimensional convolutional\n",
        "# transpose layers, each paired with a 2d batch norm layer and a relu\n",
        "# activation. The output of the generator is fed through a tanh function\n",
        "# to return it to the input data range of :math:`[-1,1]`. It is worth\n",
        "# noting the existence of the batch norm functions after the\n",
        "# conv-transpose layers, as this is a critical contribution of the DCGAN\n",
        "# paper. These layers help with the flow of gradients during training. An\n",
        "# image of the generator from the DCGAN paper is shown below.\n",
        "#\n",
        "# .. figure:: /_static/img/dcgan_generator.png\n",
        "#    :alt: dcgan_generator\n",
        "#\n",
        "# Notice, how the inputs we set in the input section (*nz*, *ngf*, and\n",
        "# *nc*) influence the generator architecture in code. *nz* is the length\n",
        "# of the z input vector, *ngf* relates to the size of the feature maps\n",
        "# that are propagated through the generator, and *nc* is the number of\n",
        "# channels in the output image (set to 3 for RGB images). Below is the\n",
        "# code for the generator.\n",
        "# \n",
        "\n",
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Now, we can instantiate the generator and apply the ``weights_init``\n",
        "# function. Check out the printed model to see how the generator object is\n",
        "# structured.\n",
        "# \n",
        "\n",
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.02.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Discriminator\n",
        "# ~~~~~~~~~~~~~\n",
        "# \n",
        "# As mentioned, the discriminator, :math:`D`, is a binary classification\n",
        "# network that takes an image as input and outputs a scalar probability\n",
        "# that the input image is real (as opposed to fake). Here, :math:`D` takes\n",
        "# a 3x64x64 input image, processes it through a series of Conv2d,\n",
        "# BatchNorm2d, and LeakyReLU layers, and outputs the final probability\n",
        "# through a Sigmoid activation function. This architecture can be extended\n",
        "# with more layers if necessary for the problem, but there is significance\n",
        "# to the use of the strided convolution, BatchNorm, and LeakyReLUs. The\n",
        "# DCGAN paper mentions it is a good practice to use strided convolution\n",
        "# rather than pooling to downsample because it lets the network learn its\n",
        "# own pooling function. Also batch norm and leaky relu functions promote\n",
        "# healthy gradient flow which is critical for the learning process of both\n",
        "# :math:`G` and :math:`D`.\n",
        "# \n",
        "\n",
        "#########################################################################\n",
        "# Discriminator Code\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Now, as with the generator, we can create the discriminator, apply the\n",
        "# ``weights_init`` function, and print the model’s structure.\n",
        "# \n",
        "\n",
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    \n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Loss Functions and Optimizers\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# With :math:`D` and :math:`G` setup, we can specify how they learn\n",
        "# through the loss functions and optimizers. We will use the Binary Cross\n",
        "# Entropy loss\n",
        "# (`BCELoss <https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss>`__)\n",
        "# function which is defined in PyTorch as:\n",
        "# \n",
        "# .. math:: \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\n",
        "# \n",
        "# Notice how this function provides the calculation of both log components\n",
        "# in the objective function (i.e. :math:`log(D(x))` and\n",
        "# :math:`log(1-D(G(z)))`). We can specify what part of the BCE equation to\n",
        "# use with the :math:`y` input. This is accomplished in the training loop\n",
        "# which is coming up soon, but it is important to understand how we can\n",
        "# choose which component we wish to calculate just by changing :math:`y`\n",
        "# (i.e. GT labels).\n",
        "# \n",
        "# Next, we define our real label as 1 and the fake label as 0. These\n",
        "# labels will be used when calculating the losses of :math:`D` and\n",
        "# :math:`G`, and this is also the convention used in the original GAN\n",
        "# paper. Finally, we set up two separate optimizers, one for :math:`D` and\n",
        "# one for :math:`G`. As specified in the DCGAN paper, both are Adam\n",
        "# optimizers with learning rate 0.0002 and Beta1 = 0.5. For keeping track\n",
        "# of the generator’s learning progression, we will generate a fixed batch\n",
        "# of latent vectors that are drawn from a Gaussian distribution\n",
        "# (i.e. fixed_noise) . In the training loop, we will periodically input\n",
        "# this fixed_noise into :math:`G`, and over the iterations we will see\n",
        "# images form out of the noise.\n",
        "# \n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training\n",
        "# ~~~~~~~~\n",
        "# \n",
        "# Finally, now that we have all of the parts of the GAN framework defined,\n",
        "# we can train it. Be mindful that training GANs is somewhat of an art\n",
        "# form, as incorrect hyperparameter settings lead to mode collapse with\n",
        "# little explanation of what went wrong. Here, we will closely follow\n",
        "# Algorithm 1 from Goodfellow’s paper, while abiding by some of the best\n",
        "# practices shown in `ganhacks <https://github.com/soumith/ganhacks>`__.\n",
        "# Namely, we will “construct different mini-batches for real and fake”\n",
        "# images, and also adjust G’s objective function to maximize\n",
        "# :math:`logD(G(z))`. Training is split up into two main parts. Part 1\n",
        "# updates the Discriminator and Part 2 updates the Generator.\n",
        "# \n",
        "# **Part 1 - Train the Discriminator**\n",
        "# \n",
        "# Recall, the goal of training the discriminator is to maximize the\n",
        "# probability of correctly classifying a given input as real or fake. In\n",
        "# terms of Goodfellow, we wish to “update the discriminator by ascending\n",
        "# its stochastic gradient”. Practically, we want to maximize\n",
        "# :math:`log(D(x)) + log(1-D(G(z)))`. Due to the separate mini-batch\n",
        "# suggestion from ganhacks, we will calculate this in two steps. First, we\n",
        "# will construct a batch of real samples from the training set, forward\n",
        "# pass through :math:`D`, calculate the loss (:math:`log(D(x))`), then\n",
        "# calculate the gradients in a backward pass. Secondly, we will construct\n",
        "# a batch of fake samples with the current generator, forward pass this\n",
        "# batch through :math:`D`, calculate the loss (:math:`log(1-D(G(z)))`),\n",
        "# and *accumulate* the gradients with a backward pass. Now, with the\n",
        "# gradients accumulated from both the all-real and all-fake batches, we\n",
        "# call a step of the Discriminator’s optimizer.\n",
        "# \n",
        "# **Part 2 - Train the Generator**\n",
        "# \n",
        "# As stated in the original paper, we want to train the Generator by\n",
        "# minimizing :math:`log(1-D(G(z)))` in an effort to generate better fakes.\n",
        "# As mentioned, this was shown by Goodfellow to not provide sufficient\n",
        "# gradients, especially early in the learning process. As a fix, we\n",
        "# instead wish to maximize :math:`log(D(G(z)))`. In the code we accomplish\n",
        "# this by: classifying the Generator output from Part 1 with the\n",
        "# Discriminator, computing G’s loss *using real labels as GT*, computing\n",
        "# G’s gradients in a backward pass, and finally updating G’s parameters\n",
        "# with an optimizer step. It may seem counter-intuitive to use the real\n",
        "# labels as GT labels for the loss function, but this allows us to use the\n",
        "# :math:`log(x)` part of the BCELoss (rather than the :math:`log(1-x)`\n",
        "# part) which is exactly what we want.\n",
        "# \n",
        "# Finally, we will do some statistic reporting and at the end of each\n",
        "# epoch we will push our fixed_noise batch through the generator to\n",
        "# visually track the progress of G’s training. The training statistics\n",
        "# reported are:\n",
        "# \n",
        "# -  **Loss_D** - discriminator loss calculated as the sum of losses for\n",
        "#    the all real and all fake batches (:math:`log(D(x)) + log(1 - D(G(z)))`).\n",
        "# -  **Loss_G** - generator loss calculated as :math:`log(D(G(z)))`\n",
        "# -  **D(x)** - the average output (across the batch) of the discriminator\n",
        "#    for the all real batch. This should start close to 1 then\n",
        "#    theoretically converge to 0.5 when G gets better. Think about why\n",
        "#    this is.\n",
        "# -  **D(G(z))** - average discriminator outputs for the all fake batch.\n",
        "#    The first number is before D is updated and the second number is\n",
        "#    after D is updated. These numbers should start near 0 and converge to\n",
        "#    0.5 as G gets better. Think about why this is.\n",
        "# \n",
        "# **Note:** This step might take a while, depending on how many epochs you\n",
        "# run and if you removed some data from the dataset.\n",
        "# \n",
        "\n",
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        \n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            \n",
        "        iters += 1\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Results\n",
        "# -------\n",
        "# \n",
        "# Finally, lets check out how we did. Here, we will look at three\n",
        "# different results. First, we will see how D and G’s losses changed\n",
        "# during training. Second, we will visualize G’s output on the fixed_noise\n",
        "# batch for every epoch. And third, we will look at a batch of real data\n",
        "# next to a batch of fake data from G.\n",
        "# \n",
        "# **Loss versus training iteration**\n",
        "# \n",
        "# Below is a plot of D & G’s losses versus training iterations.\n",
        "# \n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# **Visualization of G’s progression**\n",
        "# \n",
        "# Remember how we saved the generator’s output on the fixed_noise batch\n",
        "# after every epoch of training. Now, we can visualize the training\n",
        "# progression of G with an animation. Press the play button to start the\n",
        "# animation.\n",
        "# \n",
        "\n",
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# **Real Images vs. Fake Images**\n",
        "# \n",
        "# Finally, lets take a look at some real images and fake images side by\n",
        "# side.\n",
        "# \n",
        "\n",
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Where to Go Next\n",
        "# ----------------\n",
        "# \n",
        "# We have reached the end of our journey, but there are several places you\n",
        "# could go from here. You could:\n",
        "# \n",
        "# -  Train for longer to see how good the results get\n",
        "# -  Modify this model to take a different dataset and possibly change the\n",
        "#    size of the images and the model architecture\n",
        "# -  Check out some other cool GAN projects\n",
        "#    `here <https://github.com/nashory/gans-awesome-applications>`__\n",
        "# -  Create GANs that generate\n",
        "#    `music <https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/>`__\n",
        "# \n"
      ],
      "metadata": {
        "id": "ElOB-KMbzKZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### DCGAN [examples/blob/main/dcgan/main.py](https://github.com/pytorch/examples/blob/main/dcgan/main.py)"
      ],
      "metadata": {
        "id": "1Dse-TnEjQGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')\n",
        "parser.add_argument('--dataroot', required=False, help='path to dataset')\n",
        "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
        "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
        "parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')\n",
        "parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
        "parser.add_argument('--ngf', type=int, default=64)\n",
        "parser.add_argument('--ndf', type=int, default=64)\n",
        "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
        "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
        "parser.add_argument('--cuda', action='store_true', default=False, help='enables cuda')\n",
        "parser.add_argument('--dry-run', action='store_true', help='check a single training cycle works')\n",
        "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
        "parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
        "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
        "parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
        "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
        "parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')\n",
        "parser.add_argument('--mps', action='store_true', default=False, help='enables macOS GPU training')\n",
        "\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "try:\n",
        "    os.makedirs(opt.outf)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "if opt.manualSeed is None:\n",
        "    opt.manualSeed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", opt.manualSeed)\n",
        "random.seed(opt.manualSeed)\n",
        "torch.manual_seed(opt.manualSeed)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not opt.cuda:\n",
        "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "if torch.backends.mps.is_available() and not opt.mps:\n",
        "    print(\"WARNING: You have mps device, to enable macOS GPU run with --mps\")\n",
        "  \n",
        "if opt.dataroot is None and str(opt.dataset).lower() != 'fake':\n",
        "    raise ValueError(\"`dataroot` parameter is required for dataset \\\"%s\\\"\" % opt.dataset)\n",
        "\n",
        "if opt.dataset in ['imagenet', 'folder', 'lfw']:\n",
        "    # folder dataset\n",
        "    dataset = dset.ImageFolder(root=opt.dataroot,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(opt.imageSize),\n",
        "                                   transforms.CenterCrop(opt.imageSize),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ]))\n",
        "    nc=3\n",
        "elif opt.dataset == 'lsun':\n",
        "    classes = [ c + '_train' for c in opt.classes.split(',')]\n",
        "    dataset = dset.LSUN(root=opt.dataroot, classes=classes,\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.Resize(opt.imageSize),\n",
        "                            transforms.CenterCrop(opt.imageSize),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                        ]))\n",
        "    nc=3\n",
        "elif opt.dataset == 'cifar10':\n",
        "    dataset = dset.CIFAR10(root=opt.dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(opt.imageSize),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "    nc=3\n",
        "\n",
        "elif opt.dataset == 'mnist':\n",
        "        dataset = dset.MNIST(root=opt.dataroot, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(opt.imageSize),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,), (0.5,)),\n",
        "                           ]))\n",
        "        nc=1\n",
        "\n",
        "elif opt.dataset == 'fake':\n",
        "    dataset = dset.FakeData(image_size=(3, opt.imageSize, opt.imageSize),\n",
        "                            transform=transforms.ToTensor())\n",
        "    nc=3\n",
        "\n",
        "assert dataset\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
        "                                         shuffle=True, num_workers=int(opt.workers))\n",
        "use_mps = opt.mps and torch.backends.mps.is_available()\n",
        "if opt.cuda:\n",
        "    device = torch.device(\"cuda:0\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "ngpu = int(opt.ngpu)\n",
        "nz = int(opt.nz)\n",
        "ngf = int(opt.ngf)\n",
        "ndf = int(opt.ndf)\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "if opt.netG != '':\n",
        "    netG.load_state_dict(torch.load(opt.netG))\n",
        "print(netG)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)\n",
        "if opt.netD != '':\n",
        "    netD.load_state_dict(torch.load(opt.netD))\n",
        "print(netD)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "\n",
        "if opt.dry_run:\n",
        "    opt.niter = 1\n",
        "\n",
        "for epoch in range(opt.niter):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,), real_label,\n",
        "                           dtype=real_cpu.dtype, device=device)\n",
        "\n",
        "        output = netD(real_cpu)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "              % (epoch, opt.niter, i, len(dataloader),\n",
        "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        if i % 100 == 0:\n",
        "            vutils.save_image(real_cpu,\n",
        "                    '%s/real_samples.png' % opt.outf,\n",
        "                    normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),\n",
        "                    '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
        "                    normalize=True)\n",
        "\n",
        "        if opt.dry_run:\n",
        "            break\n",
        "    # do checkpointing\n",
        "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
        "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))"
      ],
      "metadata": {
        "id": "ZcUxQD0rjXXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 10 RNN***"
      ],
      "metadata": {
        "id": "F8EFJZhDiCN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Three Important Layers: Linear, Conv2D, LSTM [beginner_source/introyt/modelsyt_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt/modelsyt_tutorial.py) [english](https://www.youtube.com/watch?v=OSqIP-mOWOI)"
      ],
      "metadata": {
        "id": "CtZL4iV5oRAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Building Models with PyTorch\n",
        "============================\n",
        "\n",
        "Follow along with the video below or on \n",
        "`youtube <https://www.youtube.com/watch?v=OSqIP-mOWOI>`.\n",
        "\n",
        "``torch.nn.Module`` and ``torch.nn.Parameter``\n",
        "----------------------------------------------\n",
        "\n",
        "In this video, we’ll be discussing some of the tools PyTorch makes\n",
        "available for building deep learning networks.\n",
        "\n",
        "Except for ``Parameter``, the classes we discuss in this video are all\n",
        "subclasses of ``torch.nn.Module``. This is the PyTorch base class meant\n",
        "to encapsulate behaviors specific to PyTorch Models and their\n",
        "components.\n",
        "\n",
        "One important behavior of ``torch.nn.Module`` is registering parameters.\n",
        "If a particular ``Module`` subclass has learning weights, these weights\n",
        "are expressed as instances of ``torch.nn.Parameter``. The ``Parameter``\n",
        "class is a subclass of ``torch.Tensor``, with the special behavior that\n",
        "when they are assigned as attributes of a ``Module``, they are added to\n",
        "the list of that modules parameters. These parameters may be accessed\n",
        "through the ``parameters()`` method on the ``Module`` class.\n",
        "\n",
        "As a simple example, here’s a very simple model with two linear layers\n",
        "and an activation function. We’ll create an instance of it and ask it to\n",
        "report on its parameters:\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "        \n",
        "        self.linear1 = torch.nn.Linear(100, 200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 10)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "    print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# This shows the fundamental structure of a PyTorch model: there is an\n",
        "# ``__init__()`` method that defines the layers and other components of a\n",
        "# model, and a ``forward()`` method where the computation gets done. Note\n",
        "# that we can print the model, or any of its submodules, to learn about\n",
        "# its structure.\n",
        "# \n",
        "# Common Layer Types\n",
        "# ------------------\n",
        "# \n",
        "# Linear Layers\n",
        "# ~~~~~~~~~~~~~\n",
        "# \n",
        "# The most basic type of neural network layer is a *linear* or *fully\n",
        "# connected* layer. This is a layer where every input influences every\n",
        "# output of the layer to a degree specified by the layer’s weights. If a\n",
        "# model has *m* inputs and *n* outputs, the weights will be an *m* x *n*\n",
        "# matrix. For example:\n",
        "################################################################################ \n",
        "\n",
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "    print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)\n",
        "\n",
        "################################################################################\n",
        "# If you do the matrix multiplication of ``x`` by the linear layer’s\n",
        "# weights, and add the biases, you’ll find that you get the output vector\n",
        "# ``y``.\n",
        "# \n",
        "# One other important feature to note: When we checked the weights of our\n",
        "# layer with ``lin.weight``, it reported itself as a ``Parameter`` (which\n",
        "# is a subclass of ``Tensor``), and let us know that it’s tracking\n",
        "# gradients with autograd. This is a default behavior for ``Parameter``\n",
        "# that differs from ``Tensor``.\n",
        "# \n",
        "# Linear layers are used widely in deep learning models. One of the most\n",
        "# common places you’ll see them is in classifier models, which will\n",
        "# usually have one or more linear layers at the end, where the last layer\n",
        "# will have *n* outputs, where *n* is the number of classes the classifier\n",
        "# addresses.\n",
        "# \n",
        "# Convolutional Layers\n",
        "# ~~~~~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# *Convolutional* layers are built to handle data with a high degree of\n",
        "# spatial correlation. They are very commonly used in computer vision,\n",
        "# where they detect close groupings of features which the compose into\n",
        "# higher-level features. They pop up in other contexts too - for example,\n",
        "# in NLP applications, where a word’s immediate context (that is, the\n",
        "# other words nearby in the sequence) can affect the meaning of a\n",
        "# sentence.\n",
        "# \n",
        "# We saw convolutional layers in action in LeNet5 in an earlier video:\n",
        "################################################################################\n",
        "\n",
        "import torch.functional as F\n",
        "\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "################################################################################\n",
        "# Let’s break down what’s happening in the convolutional layers of this\n",
        "# model. Starting with ``conv1``:\n",
        "# \n",
        "# -  LeNet5 is meant to take in a 1x32x32 black & white image. **The first\n",
        "#    argument to a convolutional layer’s constructor is the number of\n",
        "#    input channels.** Here, it is 1. If we were building this model to\n",
        "#    look at 3-color channels, it would be 3.\n",
        "# -  A convolutional layer is like a window that scans over the image,\n",
        "#    looking for a pattern it recognizes. These patterns are called\n",
        "#    *features,* and one of the parameters of a convolutional layer is the\n",
        "#    number of features we would like it to learn. **This is the second\n",
        "#    argument to the constructor is the number of output features.** Here,\n",
        "#    we’re asking our layer to learn 6 features.\n",
        "# -  Just above, I likened the convolutional layer to a window - but how\n",
        "#    big is the window? **The third argument is the window or kernel\n",
        "#    size.** Here, the “5” means we’ve chosen a 5x5 kernel. (If you want a\n",
        "#    kernel with height different from width, you can specify a tuple for\n",
        "#    this argument - e.g., ``(3, 5)`` to get a 3x5 convolution kernel.)\n",
        "# \n",
        "# The output of a convolutional layer is an *activation map* - a spatial\n",
        "# representation of the presence of features in the input tensor.\n",
        "# ``conv1`` will give us an output tensor of 6x28x28; 6 is the number of\n",
        "# features, and 28 is the height and width of our map. (The 28 comes from\n",
        "# the fact that when scanning a 5-pixel window over a 32-pixel row, there\n",
        "# are only 28 valid positions.)\n",
        "# \n",
        "# We then pass the output of the convolution through a ReLU activation\n",
        "# function (more on activation functions later), then through a max\n",
        "# pooling layer. The max pooling layer takes features near each other in\n",
        "# the activation map and groups them together. It does this by reducing\n",
        "# the tensor, merging every 2x2 group of cells in the output into a single\n",
        "# cell, and assigning that cell the maximum value of the 4 cells that went\n",
        "# into it. This gives us a lower-resolution version of the activation map,\n",
        "# with dimensions 6x14x14.\n",
        "# \n",
        "# Our next convolutional layer, ``conv2``, expects 6 input channels\n",
        "# (corresponding to the 6 features sought by the first layer), has 16\n",
        "# output channels, and a 3x3 kernel. It puts out a 16x12x12 activation\n",
        "# map, which is again reduced by a max pooling layer to 16x6x6. Prior to\n",
        "# passing this output to the linear layers, it is reshaped to a 16 \\* 6 \\*\n",
        "# 6 = 576-element vector for consumption by the next layer.\n",
        "# \n",
        "# There are convolutional layers for addressing 1D, 2D, and 3D tensors.\n",
        "# There are also many more optional arguments for a conv layer\n",
        "# constructor, including stride length(e.g., only scanning every second or\n",
        "# every third position) in the input, padding (so you can scan out to the\n",
        "# edges of the input), and more. See the\n",
        "# `documentation <https://pytorch.org/docs/stable/nn.html#convolution-layers>`__\n",
        "# for more information.\n",
        "# \n",
        "# Recurrent Layers\n",
        "# ~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# *Recurrent neural networks* (or *RNNs)* are used for sequential data -\n",
        "# anything from time-series measurements from a scientific instrument to\n",
        "# natural language sentences to DNA nucleotides. An RNN does this by\n",
        "# maintaining a *hidden state* that acts as a sort of memory for what it\n",
        "# has seen in the sequence so far.\n",
        "# \n",
        "# The internal structure of an RNN layer - or its variants, the LSTM (long\n",
        "# short-term memory) and GRU (gated recurrent unit) - is moderately\n",
        "# complex and beyond the scope of this video, but we’ll show you what one\n",
        "# looks like in action with an LSTM-based part-of-speech tagger (a type of\n",
        "# classifier that tells you if a word is a noun, verb, etc.):\n",
        "################################################################################ \n",
        "\n",
        "class LSTMTagger(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "################################################################################\n",
        "# The constructor has four arguments:\n",
        "# \n",
        "# -  ``vocab_size`` is the number of words in the input vocabulary. Each\n",
        "#    word is a one-hot vector (or unit vector) in a\n",
        "#    ``vocab_size``-dimensional space.\n",
        "# -  ``tagset_size`` is the number of tags in the output set.\n",
        "# -  ``embedding_dim`` is the size of the *embedding* space for the\n",
        "#    vocabulary. An embedding maps a vocabulary onto a low-dimensional\n",
        "#    space, where words with similar meanings are close together in the\n",
        "#    space.\n",
        "# -  ``hidden_dim`` is the size of the LSTM’s memory.\n",
        "# \n",
        "# The input will be a sentence with the words represented as indices of\n",
        "# one-hot vectors. The embedding layer will then map these down to an\n",
        "# ``embedding_dim``-dimensional space. The LSTM takes this sequence of\n",
        "# embeddings and iterates over it, fielding an output vector of length\n",
        "# ``hidden_dim``. The final linear layer acts as a classifier; applying\n",
        "# ``log_softmax()`` to the output of the final layer converts the output\n",
        "# into a normalized set of estimated probabilities that a given word maps\n",
        "# to a given tag.\n",
        "# \n",
        "# If you’d like to see this network in action, check out the `Sequence\n",
        "# Models and LSTM\n",
        "# Networks <https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html>`__\n",
        "# tutorial on pytorch.org.\n",
        "# \n",
        "# Transformers\n",
        "# ~~~~~~~~~~~~\n",
        "# \n",
        "# *Transformers* are multi-purpose networks that have taken over the state\n",
        "# of the art in NLP with models like BERT. A discussion of transformer\n",
        "# architecture is beyond the scope of this video, but PyTorch has a\n",
        "# ``Transformer`` class that allows you to define the overall parameters\n",
        "# of a transformer model - the number of attention heads, the number of\n",
        "# encoder & decoder layers, dropout and activation functions, etc. (You\n",
        "# can even build the BERT model from this single class, with the right\n",
        "# parameters!) The ``torch.nn.Transformer`` class also has classes to\n",
        "# encapsulate the individual components (``TransformerEncoder``,\n",
        "# ``TransformerDecoder``) and subcomponents (``TransformerEncoderLayer``,\n",
        "# ``TransformerDecoderLayer``). For details, check out the\n",
        "# `documentation <https://pytorch.org/docs/stable/nn.html#transformer-layers>`__\n",
        "# on transformer classes, and the relevant\n",
        "# `tutorial <https://pytorch.org/tutorials/beginner/transformer_tutorial.html>`__\n",
        "# on pytorch.org.\n",
        "# \n",
        "# Other Layers and Functions\n",
        "# --------------------------\n",
        "# \n",
        "# Data Manipulation Layers\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# There are other layer types that perform important functions in models,\n",
        "# but don’t participate in the learning process themselves.\n",
        "# \n",
        "# **Max pooling** (and its twin, min pooling) reduce a tensor by combining\n",
        "# cells, and assigning the maximum value of the input cells to the output\n",
        "# cell (we saw this). For example:\n",
        "################################################################################\n",
        "\n",
        "my_tensor = torch.rand(1, 6, 6)\n",
        "print(my_tensor)\n",
        "\n",
        "maxpool_layer = torch.nn.MaxPool2d(3)\n",
        "print(maxpool_layer(my_tensor))\n",
        "\n",
        "################################################################################\n",
        "# If you look closely at the values above, you’ll see that each of the\n",
        "# values in the maxpooled output is the maximum value of each quadrant of\n",
        "# the 6x6 input.\n",
        "# \n",
        "# **Normalization layers** re-center and normalize the output of one layer\n",
        "# before feeding it to another. Centering the and scaling the intermediate\n",
        "# tensors has a number of beneficial effects, such as letting you use\n",
        "# higher learning rates without exploding/vanishing gradients.\n",
        "################################################################################ \n",
        "\n",
        "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
        "print(my_tensor)\n",
        "\n",
        "print(my_tensor.mean())\n",
        "\n",
        "norm_layer = torch.nn.BatchNorm1d(4)\n",
        "normed_tensor = norm_layer(my_tensor)\n",
        "print(normed_tensor)\n",
        "\n",
        "print(normed_tensor.mean())\n",
        "\n",
        "################################################################################\n",
        "# Running the cell above, we’ve added a large scaling factor and offset to\n",
        "# an input tensor; you should see the input tensor’s ``mean()`` somewhere\n",
        "# in the neighborhood of 15. After running it through the normalization\n",
        "# layer, you can see that the values are smaller, and grouped around zero\n",
        "# - in fact, the mean should be very small (> 1e-8).\n",
        "# \n",
        "# This is beneficial because many activation functions (discussed below)\n",
        "# have their strongest gradients near 0, but sometimes suffer from\n",
        "# vanishing or exploding gradients for inputs that drive them far away\n",
        "# from zero. Keeping the data centered around the area of steepest\n",
        "# gradient will tend to mean faster, better learning and higher feasible\n",
        "# learning rates.\n",
        "# \n",
        "# **Dropout layers** are a tool for encouraging *sparse representations*\n",
        "# in your model - that is, pushing it to do inference with less data.\n",
        "# \n",
        "# Dropout layers work by randomly setting parts of the input tensor\n",
        "# *during training* - dropout layers are always turned off for inference.\n",
        "# This forces the model to learn against this masked or reduced dataset.\n",
        "# For example:\n",
        "################################################################################\n",
        "\n",
        "my_tensor = torch.rand(1, 4, 4)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.4)\n",
        "print(dropout(my_tensor))\n",
        "print(dropout(my_tensor))\n",
        "\n",
        "################################################################################\n",
        "# Above, you can see the effect of dropout on a sample tensor. You can use\n",
        "# the optional ``p`` argument to set the probability of an individual\n",
        "# weight dropping out; if you don’t it defaults to 0.5.\n",
        "# \n",
        "# Activation Functions\n",
        "# ~~~~~~~~~~~~~~~~~~~~\n",
        "# \n",
        "# Activation functions make deep learning possible. A neural network is\n",
        "# really a program - with many parameters - that *simulates a mathematical\n",
        "# function*. If all we did was multiple tensors by layer weights\n",
        "# repeatedly, we could only simulate *linear functions;* further, there\n",
        "# would be no point to having many layers, as the whole network would\n",
        "# reduce could be reduced to a single matrix multiplication. Inserting\n",
        "# *non-linear* activation functions between layers is what allows a deep\n",
        "# learning model to simulate any function, rather than just linear ones.\n",
        "# \n",
        "# ``torch.nn.Module`` has objects encapsulating all of the major\n",
        "# activation functions including ReLU and its many variants, Tanh,\n",
        "# Hardtanh, sigmoid, and more. It also includes other functions, such as\n",
        "# Softmax, that are most useful at the output stage of a model.\n",
        "# \n",
        "# Loss Functions\n",
        "# ~~~~~~~~~~~~~~\n",
        "# \n",
        "# Loss functions tell us how far a model’s prediction is from the correct\n",
        "# answer. PyTorch contains a variety of loss functions, including common\n",
        "# MSE (mean squared error = L2 norm), Cross Entropy Loss and Negative\n",
        "# Likelihood Loss (useful for classifiers), and others.\n",
        "################################################################################ "
      ],
      "metadata": {
        "id": "oGC00w4doRK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Classifying Names with a Character-Level RNN [intermediate_source/char_rnn_classification_tutorial.py](https://github.com/pytorch/tutorials/blob/main/intermediate_source/char_rnn_classification_tutorial.py)"
      ],
      "metadata": {
        "id": "_Fy7clxOiCpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NLP From Scratch: Classifying Names with a Character-Level RNN\n",
        "**************************************************************\n",
        "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
        "\n",
        "We will be building and training a basic character-level RNN to classify\n",
        "words. This tutorial, along with the following two, show how to do\n",
        "preprocess data for NLP modeling \"from scratch\", in particular not using\n",
        "many of the convenience functions of `torchtext`, so you can see how\n",
        "preprocessing for NLP modeling works at a low level.\n",
        "\n",
        "A character-level RNN reads words as a series of characters -\n",
        "outputting a prediction and \"hidden state\" at each step, feeding its\n",
        "previous hidden state into each next step. We take the final prediction\n",
        "to be the output, i.e. which class the word belongs to.\n",
        "\n",
        "Specifically, we'll train on a few thousand surnames from 18 languages\n",
        "of origin, and predict which language a name is from based on the\n",
        "spelling:\n",
        "\n",
        "::\n",
        "\n",
        "    $ python predict.py Hinton\n",
        "    (-0.47) Scottish\n",
        "    (-1.52) English\n",
        "    (-3.57) Irish\n",
        "\n",
        "    $ python predict.py Schmidhuber\n",
        "    (-0.19) German\n",
        "    (-2.48) Czech\n",
        "    (-2.68) Dutch\n",
        "\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  https://pytorch.org/ For installation instructions\n",
        "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
        "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
        "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
        "\n",
        "It would also be useful to know about RNNs and how they work:\n",
        "\n",
        "-  `The Unreasonable Effectiveness of Recurrent Neural\n",
        "   Networks <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n",
        "   shows a bunch of real life examples\n",
        "-  `Understanding LSTM\n",
        "   Networks <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n",
        "   is about LSTMs specifically but also informative about RNNs in\n",
        "   general\n",
        "\n",
        "Preparing the Data\n",
        "==================\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n",
        "Included in the ``data/names`` directory are 18 text files named as\n",
        "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
        "line, mostly romanized (but we still need to convert from Unicode to\n",
        "ASCII).\n",
        "\n",
        "We'll end up with a dictionary of lists of names per language,\n",
        "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
        "(for language and name in our case) are used for later extensibility.\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "print(findFiles('data/names/*.txt'))\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Now we have ``category_lines``, a dictionary mapping each category\n",
        "# (language) to a list of lines (names). We also kept track of\n",
        "# ``all_categories`` (just a list of languages) and ``n_categories`` for\n",
        "# later reference.\n",
        "#\n",
        "\n",
        "print(category_lines['Italian'][:5])\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Turning Names into Tensors\n",
        "# --------------------------\n",
        "#\n",
        "# Now that we have all the names organized, we need to turn them into\n",
        "# Tensors to make any use of them.\n",
        "#\n",
        "# To represent a single letter, we use a \"one-hot vector\" of size\n",
        "# ``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
        "# at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
        "#\n",
        "# To make a word we join a bunch of those into a 2D matrix\n",
        "# ``<line_length x 1 x n_letters>``.\n",
        "#\n",
        "# That extra 1 dimension is because PyTorch assumes everything is in\n",
        "# batches - we're just using a batch size of 1 here.\n",
        "#\n",
        "\n",
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(letterToTensor('J'))\n",
        "\n",
        "print(lineToTensor('Jones').size())\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Creating the Network\n",
        "# ====================\n",
        "#\n",
        "# Before autograd, creating a recurrent neural network in Torch involved\n",
        "# cloning the parameters of a layer over several timesteps. The layers\n",
        "# held hidden state and gradients which are now entirely handled by the\n",
        "# graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
        "# as regular feed-forward layers.\n",
        "#\n",
        "# This RNN module (mostly copied from `the PyTorch for Torch users\n",
        "# tutorial <https://pytorch.org/tutorials/beginner/former_torchies/\n",
        "# nn_tutorial.html#example-2-recurrent-net>`__)\n",
        "# is just 2 linear layers which operate on an input and hidden state, with\n",
        "# a LogSoftmax layer after the output.\n",
        "#\n",
        "# .. figure:: https://i.imgur.com/Z2xbySO.png\n",
        "#    :alt:\n",
        "#\n",
        "#\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "n_hidden = 128\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# To run a step of this network we need to pass an input (in our case, the\n",
        "# Tensor for the current letter) and a previous hidden state (which we\n",
        "# initialize as zeros at first). We'll get back the output (probability of\n",
        "# each language) and a next hidden state (which we keep for the next\n",
        "# step).\n",
        "#\n",
        "\n",
        "input = letterToTensor('A')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output, next_hidden = rnn(input, hidden)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# For the sake of efficiency we don't want to be creating a new Tensor for\n",
        "# every step, so we will use ``lineToTensor`` instead of\n",
        "# ``letterToTensor`` and use slices. This could be further optimized by\n",
        "# pre-computing batches of Tensors.\n",
        "#\n",
        "\n",
        "input = lineToTensor('Albert')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output, next_hidden = rnn(input[0], hidden)\n",
        "print(output)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
        "# every item is the likelihood of that category (higher is more likely).\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#\n",
        "# Training\n",
        "# ========\n",
        "# Preparing for Training\n",
        "# ----------------------\n",
        "#\n",
        "# Before going into training we should make a few helper functions. The\n",
        "# first is to interpret the output of the network, which we know to be a\n",
        "# likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
        "# of the greatest value:\n",
        "#\n",
        "\n",
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output))\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# We will also want a quick way to get a training example (a name and its\n",
        "# language):\n",
        "#\n",
        "\n",
        "import random\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "for i in range(10):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training the Network\n",
        "# --------------------\n",
        "#\n",
        "# Now all it takes to train this network is show it a bunch of examples,\n",
        "# have it make guesses, and tell it if it's wrong.\n",
        "#\n",
        "# For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
        "# layer of the RNN is ``nn.LogSoftmax``.\n",
        "#\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Each loop of training will:\n",
        "#\n",
        "# -  Create input and target tensors\n",
        "# -  Create a zeroed initial hidden state\n",
        "# -  Read each letter in and\n",
        "#\n",
        "#    -  Keep hidden state for next letter\n",
        "#\n",
        "# -  Compare final output to target\n",
        "# -  Back-propagate\n",
        "# -  Return the output and loss\n",
        "#\n",
        "\n",
        "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
        "\n",
        "def train(category_tensor, line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return output, loss.item()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Now we just have to run that with a bunch of examples. Since the\n",
        "# ``train`` function returns both the output and loss we can print its\n",
        "# guesses and also keep track of loss for plotting. Since there are 1000s\n",
        "# of examples we print only every ``print_every`` examples, and take an\n",
        "# average of the loss.\n",
        "#\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train(category_tensor, line_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        current_loss = 0\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Plotting the Results\n",
        "# --------------------\n",
        "#\n",
        "# Plotting the historical loss from ``all_losses`` shows the network\n",
        "# learning:\n",
        "#\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Evaluating the Results\n",
        "# ======================\n",
        "#\n",
        "# To see how well the network performs on different categories, we will\n",
        "# create a confusion matrix, indicating for every actual language (rows)\n",
        "# which language the network guesses (columns). To calculate the confusion\n",
        "# matrix a bunch of samples are run through the network with\n",
        "# ``evaluate()``, which is the same as ``train()`` minus the backprop.\n",
        "#\n",
        "\n",
        "# Keep track of correct guesses in a confusion matrix\n",
        "confusion = torch.zeros(n_categories, n_categories)\n",
        "n_confusion = 10000\n",
        "\n",
        "# Just return an output given a line\n",
        "def evaluate(line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Go through a bunch of examples and record which are correctly guessed\n",
        "for i in range(n_confusion):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output = evaluate(line_tensor)\n",
        "    guess, guess_i = categoryFromOutput(output)\n",
        "    category_i = all_categories.index(category)\n",
        "    confusion[category_i][guess_i] += 1\n",
        "\n",
        "# Normalize by dividing every row by its sum\n",
        "for i in range(n_categories):\n",
        "    confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "# Set up plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion.numpy())\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Set up axes\n",
        "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax.set_yticklabels([''] + all_categories)\n",
        "\n",
        "# Force label at every tick\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# You can pick out bright spots off the main axis that show which\n",
        "# languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish\n",
        "# for Italian. It seems to do very well with Greek, and very poorly with\n",
        "# English (perhaps because of overlap with other languages).\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Running on User Input\n",
        "# ---------------------\n",
        "#\n",
        "\n",
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line))\n",
        "\n",
        "        # Get top N categories\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Dovesky')\n",
        "predict('Jackson')\n",
        "predict('Satoshi')\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# The final versions of the scripts `in the Practical PyTorch\n",
        "# repo <https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification>`__\n",
        "# split the above code into a few files:\n",
        "#\n",
        "# -  ``data.py`` (loads files)\n",
        "# -  ``model.py`` (defines the RNN)\n",
        "# -  ``train.py`` (runs training)\n",
        "# -  ``predict.py`` (runs ``predict()`` with command line arguments)\n",
        "# -  ``server.py`` (serve prediction as a JSON API with bottle.py)\n",
        "#\n",
        "# Run ``train.py`` to train and save the network.\n",
        "#\n",
        "# Run ``predict.py`` with a name to view predictions:\n",
        "#\n",
        "# ::\n",
        "#\n",
        "#     $ python predict.py Hazaki\n",
        "#     (-0.42) Japanese\n",
        "#     (-1.39) Polish\n",
        "#     (-3.51) Czech\n",
        "#\n",
        "# Run ``server.py`` and visit http://localhost:5533/Yourname to get JSON\n",
        "# output of predictions.\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Exercises\n",
        "# =========\n",
        "#\n",
        "# -  Try with a different dataset of line -> category, for example:\n",
        "#\n",
        "#    -  Any word -> language\n",
        "#    -  First name -> gender\n",
        "#    -  Character name -> writer\n",
        "#    -  Page title -> blog or subreddit\n",
        "#\n",
        "# -  Get better results with a bigger and/or better shaped network\n",
        "#\n",
        "#    -  Add more linear layers\n",
        "#    -  Try the ``nn.LSTM`` and ``nn.GRU`` layers\n",
        "#    -  Combine multiple of these RNNs as a higher level network\n",
        "#"
      ],
      "metadata": {
        "id": "rqQt-Zp4iC_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Generating Names with a Character-Level RNN [intermediate_source/char_rnn_generation_tutorial.py](https://github.com/pytorch/tutorials/blob/main/intermediate_source/char_rnn_generation_tutorial.py)"
      ],
      "metadata": {
        "id": "djlKgzhiiYEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NLP From Scratch: Generating Names with a Character-Level RNN\n",
        "*************************************************************\n",
        "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
        "\n",
        "This is our second of three tutorials on \"NLP From Scratch\".\n",
        "In the `first tutorial </intermediate/char_rnn_classification_tutorial>`\n",
        "we used a RNN to classify names into their language of origin. This time\n",
        "we'll turn around and generate names from languages.\n",
        "\n",
        "::\n",
        "\n",
        "    > python sample.py Russian RUS\n",
        "    Rovakov\n",
        "    Uantov\n",
        "    Shavakov\n",
        "\n",
        "    > python sample.py German GER\n",
        "    Gerren\n",
        "    Ereng\n",
        "    Rosher\n",
        "\n",
        "    > python sample.py Spanish SPA\n",
        "    Salla\n",
        "    Parer\n",
        "    Allan\n",
        "\n",
        "    > python sample.py Chinese CHI\n",
        "    Chan\n",
        "    Hang\n",
        "    Iun\n",
        "\n",
        "We are still hand-crafting a small RNN with a few linear layers. The big\n",
        "difference is instead of predicting a category after reading in all the\n",
        "letters of a name, we input a category and output one letter at a time.\n",
        "Recurrently predicting characters to form language (this could also be\n",
        "done with words or other higher order constructs) is often referred to\n",
        "as a \"language model\".\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  https://pytorch.org/ For installation instructions\n",
        "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
        "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
        "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
        "\n",
        "It would also be useful to know about RNNs and how they work:\n",
        "\n",
        "-  `The Unreasonable Effectiveness of Recurrent Neural\n",
        "   Networks <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n",
        "   shows a bunch of real life examples\n",
        "-  `Understanding LSTM\n",
        "   Networks <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n",
        "   is about LSTMs specifically but also informative about RNNs in\n",
        "   general\n",
        "\n",
        "I also suggest the previous tutorial, :doc:`/intermediate/char_rnn_classification_tutorial`\n",
        "\n",
        "\n",
        "Preparing the Data\n",
        "==================\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n",
        "See the last tutorial for more detail of this process. In short, there\n",
        "are a bunch of plain text files ``data/names/[Language].txt`` with a\n",
        "name per line. We split lines into an array, convert Unicode to ASCII,\n",
        "and end up with a dictionary ``{language: [names ...]}``.\n",
        "\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    with open(filename, encoding='utf-8') as some_file:\n",
        "        return [unicodeToAscii(line.strip()) for line in some_file]\n",
        "\n",
        "# Build the category_lines dictionary, a list of lines per category\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "if n_categories == 0:\n",
        "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
        "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
        "        'the current directory.')\n",
        "\n",
        "print('# categories:', n_categories, all_categories)\n",
        "print(unicodeToAscii(\"O'Néàl\"))\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Creating the Network\n",
        "# ====================\n",
        "#\n",
        "# This network extends `the last tutorial's RNN <#Creating-the-Network>`__\n",
        "# with an extra argument for the category tensor, which is concatenated\n",
        "# along with the others. The category tensor is a one-hot vector just like\n",
        "# the letter input.\n",
        "#\n",
        "# We will interpret the output as the probability of the next letter. When\n",
        "# sampling, the most likely output letter is used as the next input\n",
        "# letter.\n",
        "#\n",
        "# I added a second linear layer ``o2o`` (after combining hidden and\n",
        "# output) to give it more muscle to work with. There's also a dropout\n",
        "# layer, which `randomly zeros parts of its\n",
        "# input <https://arxiv.org/abs/1207.0580>`__ with a given probability\n",
        "# (here 0.1) and is usually used to fuzz inputs to prevent overfitting.\n",
        "# Here we're using it towards the end of the network to purposely add some\n",
        "# chaos and increase sampling variety.\n",
        "#\n",
        "# .. figure:: https://i.imgur.com/jzVrf7f.png\n",
        "#    :alt:\n",
        "#\n",
        "#\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden):\n",
        "        input_combined = torch.cat((category, input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1)\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training\n",
        "# =========\n",
        "# Preparing for Training\n",
        "# ----------------------\n",
        "#\n",
        "# First of all, helper functions to get random pairs of (category, line):\n",
        "#\n",
        "\n",
        "import random\n",
        "\n",
        "# Random item from a list\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Get a random category and random line from that category\n",
        "def randomTrainingPair():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    return category, line\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# For each timestep (that is, for each letter in a training word) the\n",
        "# inputs of the network will be\n",
        "# ``(category, current letter, hidden state)`` and the outputs will be\n",
        "# ``(next letter, next hidden state)``. So for each training set, we'll\n",
        "# need the category, a set of input letters, and a set of output/target\n",
        "# letters.\n",
        "#\n",
        "# Since we are predicting the next letter from the current letter for each\n",
        "# timestep, the letter pairs are groups of consecutive letters from the\n",
        "# line - e.g. for ``\"ABCD<EOS>\"`` we would create (\"A\", \"B\"), (\"B\", \"C\"),\n",
        "# (\"C\", \"D\"), (\"D\", \"EOS\").\n",
        "#\n",
        "# .. figure:: https://i.imgur.com/JH58tXY.png\n",
        "#    :alt:\n",
        "#\n",
        "# The category tensor is a `one-hot\n",
        "# tensor <https://en.wikipedia.org/wiki/One-hot>`__ of size\n",
        "# ``<1 x n_categories>``. When training we feed it to the network at every\n",
        "# timestep - this is a design choice, it could have been included as part\n",
        "# of initial hidden state or some other strategy.\n",
        "#\n",
        "\n",
        "# One-hot vector for category\n",
        "def categoryTensor(category):\n",
        "    li = all_categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "# One-hot matrix of first to last letters (not including EOS) for input\n",
        "def inputTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# LongTensor of second letter to end (EOS) for target\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    letter_indexes.append(n_letters - 1) # EOS\n",
        "    return torch.LongTensor(letter_indexes)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# For convenience during training we'll make a ``randomTrainingExample``\n",
        "# function that fetches a random (category, line) pair and turns them into\n",
        "# the required (category, input, target) tensors.\n",
        "#\n",
        "\n",
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def randomTrainingExample():\n",
        "    category, line = randomTrainingPair()\n",
        "    category_tensor = categoryTensor(category)\n",
        "    input_line_tensor = inputTensor(line)\n",
        "    target_line_tensor = targetTensor(line)\n",
        "    return category_tensor, input_line_tensor, target_line_tensor\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training the Network\n",
        "# --------------------\n",
        "#\n",
        "# In contrast to classification, where only the last output is used, we\n",
        "# are making a prediction at every step, so we are calculating loss at\n",
        "# every step.\n",
        "#\n",
        "# The magic of autograd allows you to simply sum these losses at each step\n",
        "# and call backward at the end.\n",
        "#\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return output, loss.item() / input_line_tensor.size(0)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# To keep track of how long training takes I am adding a\n",
        "# ``timeSince(timestamp)`` function which returns a human readable string:\n",
        "#\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training is business as usual - call train a bunch of times and wait a\n",
        "# few minutes, printing the current time and loss every ``print_every``\n",
        "# examples, and keeping store of an average loss per ``plot_every`` examples\n",
        "# in ``all_losses`` for plotting later.\n",
        "#\n",
        "\n",
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Plotting the Losses\n",
        "# -------------------\n",
        "#\n",
        "# Plotting the historical loss from all\\_losses shows the network\n",
        "# learning:\n",
        "#\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Sampling the Network\n",
        "# ====================\n",
        "#\n",
        "# To sample we give the network a letter and ask what the next one is,\n",
        "# feed that in as the next letter, and repeat until the EOS token.\n",
        "#\n",
        "# -  Create tensors for input category, starting letter, and empty hidden\n",
        "#    state\n",
        "# -  Create a string ``output_name`` with the starting letter\n",
        "# -  Up to a maximum output length,\n",
        "#\n",
        "#    -  Feed the current letter to the network\n",
        "#    -  Get the next letter from highest output, and next hidden state\n",
        "#    -  If the letter is EOS, stop here\n",
        "#    -  If a regular letter, add to ``output_name`` and continue\n",
        "#\n",
        "# -  Return the final name\n",
        "#\n",
        "# .. Note::\n",
        "#    Rather than having to give it a starting letter, another\n",
        "#    strategy would have been to include a \"start of string\" token in\n",
        "#    training and have the network choose its own starting letter.\n",
        "#\n",
        "\n",
        "max_length = 20\n",
        "\n",
        "# Sample from a category and starting letter\n",
        "def sample(category, start_letter='A'):\n",
        "    with torch.no_grad():  # no need to track history in sampling\n",
        "        category_tensor = categoryTensor(category)\n",
        "        input = inputTensor(start_letter)\n",
        "        hidden = rnn.initHidden()\n",
        "\n",
        "        output_name = start_letter\n",
        "\n",
        "        for i in range(max_length):\n",
        "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi[0][0]\n",
        "            if topi == n_letters - 1:\n",
        "                break\n",
        "            else:\n",
        "                letter = all_letters[topi]\n",
        "                output_name += letter\n",
        "            input = inputTensor(letter)\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples(category, start_letters='ABC'):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(category, start_letter))\n",
        "\n",
        "samples('Russian', 'RUS')\n",
        "\n",
        "samples('German', 'GER')\n",
        "\n",
        "samples('Spanish', 'SPA')\n",
        "\n",
        "samples('Chinese', 'CHI')\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Exercises\n",
        "# =========\n",
        "#\n",
        "# -  Try with a different dataset of category -> line, for example:\n",
        "#\n",
        "#    -  Fictional series -> Character name\n",
        "#    -  Part of speech -> Word\n",
        "#    -  Country -> City\n",
        "#\n",
        "# -  Use a \"start of sentence\" token so that sampling can be done without\n",
        "#    choosing a start letter\n",
        "# -  Get better results with a bigger and/or better shaped network\n",
        "#\n",
        "#    -  Try the nn.LSTM and nn.GRU layers\n",
        "#    -  Combine multiple of these RNNs as a higher level network\n",
        "#"
      ],
      "metadata": {
        "id": "14iS5YBHiYPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Translation with a Sequence to Sequence Network and Attention [intermediate_source/seq2seq_translation_tutorial.py](https://github.com/pytorch/tutorials/blob/main/intermediate_source/seq2seq_translation_tutorial.py)"
      ],
      "metadata": {
        "id": "TD93uSCYiYYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "*******************************************************************************\n",
        "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
        "\n",
        "This is the third and final tutorial on doing \"NLP From Scratch\", where we\n",
        "write our own classes and functions to preprocess the data to do our NLP\n",
        "modeling tasks. We hope after you complete this tutorial that you'll proceed to\n",
        "learn how `torchtext` can handle much of this preprocessing for you in the\n",
        "three tutorials immediately following this one.\n",
        "\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "::\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the `sequence\n",
        "to sequence network <https://arxiv.org/abs/1409.3215>`__, in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "To improve upon this model we'll use an `attention\n",
        "mechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  https://pytorch.org/ For installation instructions\n",
        "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
        "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
        "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
        "\n",
        "\n",
        "It would also be useful to know about Sequence to Sequence networks and\n",
        "how they work:\n",
        "\n",
        "-  `Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "   Statistical Machine Translation <https://arxiv.org/abs/1406.1078>`__\n",
        "-  `Sequence to Sequence Learning with Neural\n",
        "   Networks <https://arxiv.org/abs/1409.3215>`__\n",
        "-  `Neural Machine Translation by Jointly Learning to Align and\n",
        "   Translate <https://arxiv.org/abs/1409.0473>`__\n",
        "-  `A Neural Conversational Model <https://arxiv.org/abs/1506.05869>`__\n",
        "\n",
        "You will also find the previous tutorials on\n",
        ":doc:`/intermediate/char_rnn_classification_tutorial`\n",
        "and :doc:`/intermediate/char_rnn_generation_tutorial`\n",
        "helpful as those concepts are very similar to the Encoder and Decoder\n",
        "models, respectively.\n",
        "\n",
        "**Requirements**\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "######################################################################\n",
        "# Loading data files\n",
        "# ==================\n",
        "#\n",
        "# The data for this project is a set of many thousands of English to\n",
        "# French translation pairs.\n",
        "#\n",
        "# `This question on Open Data Stack\n",
        "# Exchange <https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
        "# pointed me to the open translation site https://tatoeba.org/ which has\n",
        "# downloads available at https://tatoeba.org/eng/downloads - and better\n",
        "# yet, someone did the extra work of splitting language pairs into\n",
        "# individual text files here: https://www.manythings.org/anki/\n",
        "#\n",
        "# The English to French pairs are too big to include in the repo, so\n",
        "# download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "# separated list of translation pairs:\n",
        "#\n",
        "# ::\n",
        "#\n",
        "#     I am cold.    J'ai froid.\n",
        "#\n",
        "# .. Note::\n",
        "#    Download the data from\n",
        "#    `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "#    and extract it to the current directory.\n",
        "\n",
        "######################################################################\n",
        "# Similar to the character encoding used in the character-level RNN\n",
        "# tutorials, we will be representing each word in a language as a one-hot\n",
        "# vector, or giant vector of zeros except for a single one (at the index\n",
        "# of the word). Compared to the dozens of characters that might exist in a\n",
        "# language, there are many many more words, so the encoding vector is much\n",
        "# larger. We will however cheat a bit and trim the data to only use a few\n",
        "# thousand words per language.\n",
        "#\n",
        "# .. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
        "#    :alt:\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# We'll need a unique index per word to use as the inputs and targets of\n",
        "# the networks later. To keep track of all this we will use a helper class\n",
        "# called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "# (``index2word``) dictionaries, as well as a count of each word\n",
        "# ``word2count`` which will be used to replace rare words later.\n",
        "#\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# The files are all in Unicode, to simplify we will turn Unicode\n",
        "# characters to ASCII, make everything lowercase, and trim most\n",
        "# punctuation.\n",
        "#\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# To read the data file we will split the file into lines, and then split\n",
        "# lines into pairs. The files are all English → Other Language, so if we\n",
        "# want to translate from Other Language → English I added the ``reverse``\n",
        "# flag to reverse the pairs.\n",
        "#\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Since there are a *lot* of example sentences and we want to train\n",
        "# something quickly, we'll trim the data set to only relatively short and\n",
        "# simple sentences. Here the maximum length is 10 words (that includes\n",
        "# ending punctuation) and we're filtering to sentences that translate to\n",
        "# the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "# earlier).\n",
        "#\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# The full process for preparing the data is:\n",
        "#\n",
        "# -  Read text file and split into lines, split lines into pairs\n",
        "# -  Normalize text, filter by length and content\n",
        "# -  Make word lists from sentences in pairs\n",
        "#\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# The Seq2Seq Model\n",
        "# =================\n",
        "#\n",
        "# A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "# sequence and uses its own output as input for subsequent steps.\n",
        "#\n",
        "# A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
        "# seq2seq network, or `Encoder Decoder\n",
        "# network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "# consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "# an input sequence and outputs a single vector, and the decoder reads\n",
        "# that vector to produce an output sequence.\n",
        "#\n",
        "# .. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "#    :alt:\n",
        "#\n",
        "# Unlike sequence prediction with a single RNN, where every input\n",
        "# corresponds to an output, the seq2seq model frees us from sequence\n",
        "# length and order, which makes it ideal for translation between two\n",
        "# languages.\n",
        "#\n",
        "# Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "# black cat\". Most of the words in the input sentence have a direct\n",
        "# translation in the output sentence, but are in slightly different\n",
        "# orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "# construction there is also one more word in the input sentence. It would\n",
        "# be difficult to produce a correct translation directly from the sequence\n",
        "# of input words.\n",
        "#\n",
        "# With a seq2seq model the encoder creates a single vector which, in the\n",
        "# ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "# vector — a single point in some N dimensional space of sentences.\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# The Encoder\n",
        "# -----------\n",
        "#\n",
        "# The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "# every word from the input sentence. For every input word the encoder\n",
        "# outputs a vector and a hidden state, and uses the hidden state for the\n",
        "# next input word.\n",
        "#\n",
        "# .. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
        "#    :alt:\n",
        "#\n",
        "#\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "######################################################################\n",
        "# The Decoder\n",
        "# -----------\n",
        "#\n",
        "# The decoder is another RNN that takes the encoder output vector(s) and\n",
        "# outputs a sequence of words to create the translation.\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Simple Decoder\n",
        "# ^^^^^^^^^^^^^^\n",
        "#\n",
        "# In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "# This last output is sometimes called the *context vector* as it encodes\n",
        "# context from the entire sequence. This context vector is used as the\n",
        "# initial hidden state of the decoder.\n",
        "#\n",
        "# At every step of decoding, the decoder is given an input token and\n",
        "# hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "# token, and the first hidden state is the context vector (the encoder's\n",
        "# last hidden state).\n",
        "#\n",
        "# .. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
        "#    :alt:\n",
        "#\n",
        "#\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "######################################################################\n",
        "# I encourage you to train and observe the results of this model, but to\n",
        "# save space we'll be going straight for the gold and introducing the\n",
        "# Attention Mechanism.\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Attention Decoder\n",
        "# ^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# If only the context vector is passed between the encoder and decoder,\n",
        "# that single vector carries the burden of encoding the entire sentence.\n",
        "#\n",
        "# Attention allows the decoder network to \"focus\" on a different part of\n",
        "# the encoder's outputs for every step of the decoder's own outputs. First\n",
        "# we calculate a set of *attention weights*. These will be multiplied by\n",
        "# the encoder output vectors to create a weighted combination. The result\n",
        "# (called ``attn_applied`` in the code) should contain information about\n",
        "# that specific part of the input sequence, and thus help the decoder\n",
        "# choose the right output words.\n",
        "#\n",
        "# .. figure:: https://i.imgur.com/1152PYf.png\n",
        "#    :alt:\n",
        "#\n",
        "# Calculating the attention weights is done with another feed-forward\n",
        "# layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "# Because there are sentences of all sizes in the training data, to\n",
        "# actually create and train this layer we have to choose a maximum\n",
        "# sentence length (input length, for encoder outputs) that it can apply\n",
        "# to. Sentences of the maximum length will use all the attention weights,\n",
        "# while shorter sentences will only use the first few.\n",
        "#\n",
        "# .. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
        "#    :alt:\n",
        "#\n",
        "#\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# .. note:: There are other forms of attention that work around the length\n",
        "#   limitation by using a relative position approach. Read about \"local\n",
        "#   attention\" in `Effective Approaches to Attention-based Neural Machine\n",
        "#   Translation <https://arxiv.org/abs/1508.04025>`__.\n",
        "#\n",
        "# Training\n",
        "# ========\n",
        "#\n",
        "# Preparing Training Data\n",
        "# -----------------------\n",
        "#\n",
        "# To train, for each pair we will need an input tensor (indexes of the\n",
        "# words in the input sentence) and target tensor (indexes of the words in\n",
        "# the target sentence). While creating these vectors we will append the\n",
        "# EOS token to both sequences.\n",
        "#\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training the Model\n",
        "# ------------------\n",
        "#\n",
        "# To train we run the input sentence through the encoder, and keep track\n",
        "# of every output and the latest hidden state. Then the decoder is given\n",
        "# the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "# encoder as its first hidden state.\n",
        "#\n",
        "# \"Teacher forcing\" is the concept of using the real target outputs as\n",
        "# each next input, instead of using the decoder's guess as the next input.\n",
        "# Using teacher forcing causes it to converge faster but `when the trained\n",
        "# network is exploited, it may exhibit\n",
        "# instability <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__.\n",
        "#\n",
        "# You can observe outputs of teacher-forced networks that read with\n",
        "# coherent grammar but wander far from the correct translation -\n",
        "# intuitively it has learned to represent the output grammar and can \"pick\n",
        "# up\" the meaning once the teacher tells it the first few words, but it\n",
        "# has not properly learned how to create the sentence from the translation\n",
        "# in the first place.\n",
        "#\n",
        "# Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "# choose to use teacher forcing or not with a simple if statement. Turn\n",
        "# ``teacher_forcing_ratio`` up to use more of it.\n",
        "#\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# This is a helper function to print time elapsed and estimated time\n",
        "# remaining given the current time and progress %.\n",
        "#\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# The whole training process looks like this:\n",
        "#\n",
        "# -  Start a timer\n",
        "# -  Initialize optimizers and criterion\n",
        "# -  Create set of training pairs\n",
        "# -  Start empty losses array for plotting\n",
        "#\n",
        "# Then we call ``train`` many times and occasionally print the progress (%\n",
        "# of examples, time so far, estimated time) and average loss.\n",
        "#\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Plotting results\n",
        "# ----------------\n",
        "#\n",
        "# Plotting is done with matplotlib, using the array of loss values\n",
        "# ``plot_losses`` saved while training.\n",
        "#\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Evaluation\n",
        "# ==========\n",
        "#\n",
        "# Evaluation is mostly the same as training, but there are no targets so\n",
        "# we simply feed the decoder's predictions back to itself for each step.\n",
        "# Every time it predicts a word we add it to the output string, and if it\n",
        "# predicts the EOS token we stop there. We also store the decoder's\n",
        "# attention outputs for display later.\n",
        "#\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# We can evaluate random sentences from the training set and print out the\n",
        "# input, target, and output to make some subjective quality judgements:\n",
        "#\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training and Evaluating\n",
        "# =======================\n",
        "#\n",
        "# With all these helper functions in place (it looks like extra work, but\n",
        "# it makes it easier to run multiple experiments) we can actually\n",
        "# initialize a network and start training.\n",
        "#\n",
        "# Remember that the input sentences were heavily filtered. For this small\n",
        "# dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "# single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "# reasonable results.\n",
        "#\n",
        "# .. Note::\n",
        "#    If you run this notebook you can train, interrupt the kernel,\n",
        "#    evaluate, and continue training later. Comment out the lines where the\n",
        "#    encoder and decoder are initialized and run ``trainIters`` again.\n",
        "#\n",
        "\n",
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "\n",
        "######################################################################\n",
        "#\n",
        "\n",
        "evaluateRandomly(encoder1, attn_decoder1)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Visualizing Attention\n",
        "# ---------------------\n",
        "#\n",
        "# A useful property of the attention mechanism is its highly interpretable\n",
        "# outputs. Because it is used to weight specific encoder outputs of the\n",
        "# input sequence, we can imagine looking where the network is focused most\n",
        "# at each time step.\n",
        "#\n",
        "# You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "# displayed as a matrix, with the columns being input steps and rows being\n",
        "# output steps:\n",
        "#\n",
        "\n",
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# For a better viewing experience we will do the extra work of adding axes\n",
        "# and labels:\n",
        "#\n",
        "\n",
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Exercises\n",
        "# =========\n",
        "#\n",
        "# -  Try with a different dataset\n",
        "#\n",
        "#    -  Another language pair\n",
        "#    -  Human → Machine (e.g. IOT commands)\n",
        "#    -  Chat → Response\n",
        "#    -  Question → Answer\n",
        "#\n",
        "# -  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
        "#    GloVe\n",
        "# -  Try with more layers, more hidden units, and more sentences. Compare\n",
        "#    the training time and results.\n",
        "# -  If you use a translation file where pairs have two of the same phrase\n",
        "#    (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
        "#    this:\n",
        "#\n",
        "#    -  Train as an autoencoder\n",
        "#    -  Save only the Encoder network\n",
        "#    -  Train a new Decoder for translation from there\n",
        "#"
      ],
      "metadata": {
        "id": "hTWo0_oqiYhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 11 Transfer Learning***"
      ],
      "metadata": {
        "id": "Nq73kqcwhFQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Transfer Learning using ResNet-18 [beginner_source/blitz/autograd_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/blitz/autograd_tutorial.py)"
      ],
      "metadata": {
        "id": "i7oaPmi8cs94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/max/640/1*ovNMgv2yulRn6mnpzOSkUg.webp\">\n",
        "\n",
        "Source [The 10 Deep Learning Methods AI Practitioners Need to Apply](https://data-notes.co/the-10-deep-learning-methods-ai-practitioners-need-to-apply-885259f402c1)\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/XwcnU5x.png\">\n",
        "\n",
        "Sorce [Transfer Learning with ResNet in PyTorch](https://www.pluralsight.com/guides/introduction-to-resnet)"
      ],
      "metadata": {
        "id": "7a5_JqkKvtF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Source\n",
        "# https://github.com/pytorch/tutorials/blob/main/beginner_source/blitz/autograd_tutorial.py\n",
        "\"\"\"\n",
        "A Gentle Introduction to ``torch.autograd``\n",
        "---------------------------------\n",
        "``torch.autograd`` is PyTorch’s automatic differentiation engine \n",
        "that powers neural network training. \n",
        "In this section, you will get a conceptual understanding of \n",
        "how autograd helps a neural network train.\n",
        "\n",
        "Background\n",
        "~~~~~~~~~~\n",
        "Neural networks (NNs) are a collection of nested functions \n",
        "that are executed on some input data.  \n",
        "These functions are defined by *parameters* (consisting of weights and biases), \n",
        "which in PyTorch are stored in tensors.\n",
        "\n",
        "Training a NN happens in two steps:\n",
        "\n",
        "**Forward Propagation**: In forward prop, the NN makes its best guess\n",
        "about the correct output. \n",
        "It runs the input data through each of its functions to make this guess.\n",
        "\n",
        "**Backward Propagation**: In backprop, \n",
        "the NN adjusts its parameters proportionate to the error in its guess. \n",
        "It does this by traversing backwards from the output, \n",
        "collecting the derivatives of the error with\n",
        "respect to the parameters of the functions (*gradients*), \n",
        "and optimizing the parameters using gradient descent. \n",
        "For a more detailed walkthrough of backprop, check out this `video from\n",
        "3Blue1Brown <https://www.youtube.com/watch?v=tIeHLnjs5U8>`.\n",
        "\n",
        "Usage in PyTorch\n",
        "~~~~~~~~~~~~~~~~\n",
        "Let's take a look at a single training step.\n",
        "For this example, we load a pretrained resnet18 model from ``torchvision``.\n",
        "We create a random data tensor to represent a single image with 3 channels, \n",
        "and height & width of 64,\n",
        "and its corresponding ``label`` initialized to some random values. \n",
        "Label in pretrained models has shape (1,1000).\n",
        "\n",
        ".. note::\n",
        "    This tutorial work only on CPU and will not work on GPU \n",
        "    (even if tensor are moved to CUDA).\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "data = torch.rand(1, 3, 64, 64)\n",
        "labels = torch.rand(1, 1000)\n",
        "\n",
        "################################################################################\n",
        "# Next, we run the input data through the model \n",
        "# through each of its layers to make a prediction.\n",
        "# This is the **forward pass**.\n",
        "################################################################################\n",
        "\n",
        "prediction = model(data) # forward pass\n",
        "\n",
        "################################################################################\n",
        "# We use the model's prediction and the corresponding label \n",
        "# to calculate the error (``loss``).\n",
        "# The next step is to backpropagate this error through the network.\n",
        "# Backward propagation is kicked off \n",
        "# when we call ``.backward()`` on the error tensor.\n",
        "# Autograd then calculates and stores the gradients for each model parameter \n",
        "# in the parameter's ``.grad`` attribute.\n",
        "################################################################################\n",
        "\n",
        "loss = (prediction - labels).sum()\n",
        "loss.backward() # backward pass\n",
        "\n",
        "################################################################################\n",
        "# Next, we load an optimizer, \n",
        "# in this case SGD with a learning rate of 0.01 and \n",
        "# `momentum \n",
        "# <https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d>`\n",
        "# of 0.9.\n",
        "# We register all the parameters of the model in the optimizer.\n",
        "################################################################################\n",
        "\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "\n",
        "################################################################################\n",
        "# Finally, we call ``.step()`` to initiate gradient descent. \n",
        "# The optimizer adjusts each parameter by its gradient stored in ``.grad``.\n",
        "################################################################################\n",
        "\n",
        "optim.step() #gradient descent\n",
        "\n",
        "################################################################################\n",
        "# At this point, you have everything you need to train your neural network.\n",
        "# The below sections detail the workings of autograd - feel free to skip them.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Differentiation in Autograd\n",
        "# \n",
        "# Let's take a look at how ``autograd`` collects gradients. \n",
        "# We create two tensors ``a`` and ``b`` with ``requires_grad=True``. \n",
        "# This signals to ``autograd`` that every operation on them should be tracked.\n",
        "################################################################################\n",
        "\n",
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n",
        "\n",
        "################################################################################\n",
        "# We create another tensor ``Q`` from ``a`` and ``b``.\n",
        "#\n",
        "# .. math::\n",
        "#    Q = 3a^3 - b^2\n",
        "################################################################################\n",
        "\n",
        "Q = 3*a**3 - b**2\n",
        "\n",
        "################################################################################\n",
        "# Let's assume ``a`` and ``b`` to be parameters of an NN, \n",
        "# and ``Q`` to be the error. \n",
        "# In NN training, we want gradients of the error w.r.t. parameters, i.e.\n",
        "#\n",
        "# .. math::\n",
        "#    \\frac{\\partial Q}{\\partial a} = 9a^2\n",
        "#\n",
        "# .. math::\n",
        "#    \\frac{\\partial Q}{\\partial b} = -2b\n",
        "#\n",
        "# When we call ``.backward()`` on ``Q``, \n",
        "# autograd calculates these gradients\n",
        "# and stores them in the respective tensors' ``.grad`` attribute.\n",
        "#\n",
        "# We need to explicitly pass a ``gradient`` argument in ``Q.backward()`` \n",
        "# because it is a vector.\n",
        "# ``gradient`` is a tensor of the same shape as ``Q``, \n",
        "# and it represents the gradient of Q w.r.t. itself, i.e.\n",
        "#\n",
        "# .. math::\n",
        "#    \\frac{dQ}{dQ} = 1\n",
        "#\n",
        "# Equivalently, \n",
        "# we can also aggregate Q into a scalar and call backward implicitly, \n",
        "# like ``Q.sum().backward()``.\n",
        "################################################################################\n",
        "\n",
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)\n",
        "\n",
        "################################################################################\n",
        "# Gradients are now deposited in ``a.grad`` and ``b.grad``\n",
        "################################################################################\n",
        "\n",
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)\n",
        "\n",
        "################################################################################\n",
        "# Optional Reading - Vector Calculus using ``autograd``\n",
        "#\n",
        "# Mathematically, \n",
        "# if you have a vector valued function :math:`\\vec{y}=f(\\vec{x})`, \n",
        "# then the gradient of :math:`\\vec{y}` with\n",
        "# respect to :math:`\\vec{x}` is a Jacobian matrix :math:`J`:\n",
        "#\n",
        "# .. math::\n",
        "#\n",
        "#      J\n",
        "#      =\n",
        "#       \\left(\\begin{array}{cc}\n",
        "#       \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n",
        "#       ... &\n",
        "#       \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n",
        "#       \\end{array}\\right)\n",
        "#      =\n",
        "#      \\left(\\begin{array}{ccc}\n",
        "#       \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "#       \\vdots & \\ddots & \\vdots\\\\\n",
        "#       \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "#       \\end{array}\\right)\n",
        "#\n",
        "# Generally speaking, \n",
        "# ``torch.autograd`` is an engine for computing vector-Jacobian product. \n",
        "# That is, given any vector :math:`\\vec{v}`, compute the product\n",
        "# :math:`J^{T}\\cdot \\vec{v}`\n",
        "#\n",
        "# If :math:`\\vec{v}` happens to be the gradient of a scalar function \n",
        "# :math:`l=g\\left(\\vec{y}\\right)`:\n",
        "#\n",
        "# .. math::\n",
        "#\n",
        "#   \\vec{v}\n",
        "#    =\n",
        "#    \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\n",
        "#\n",
        "# then by the chain rule, the vector-Jacobian product would be the\n",
        "# gradient of :math:`l` with respect to :math:`\\vec{x}`:\n",
        "#\n",
        "# .. math::\n",
        "#\n",
        "#\n",
        "#      J^{T}\\cdot \\vec{v}=\\left(\\begin{array}{ccc}\n",
        "#       \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        "#       \\vdots & \\ddots & \\vdots\\\\\n",
        "#       \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "#       \\end{array}\\right)\\left(\\begin{array}{c}\n",
        "#       \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        "#       \\vdots\\\\\n",
        "#       \\frac{\\partial l}{\\partial y_{m}}\n",
        "#       \\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "#       \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        "#       \\vdots\\\\\n",
        "#       \\frac{\\partial l}{\\partial x_{n}}\n",
        "#       \\end{array}\\right)\n",
        "#\n",
        "# This characteristic of vector-Jacobian product is \n",
        "# what we use in the above example;\n",
        "# ``external_grad`` represents :math:`\\vec{v}`.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Computational Graph\n",
        "#\n",
        "# Conceptually, autograd keeps a record of data (tensors) & \n",
        "# all executed operations (along with the resulting new tensors) \n",
        "# in a directed acyclic graph (DAG) consisting of `Function \n",
        "# <https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`\n",
        "# objects. \n",
        "# In this DAG, leaves are the input tensors, roots are the output tensors. \n",
        "# By tracing this graph from roots to leaves, \n",
        "# you can automatically compute the gradients using the chain rule.\n",
        "#\n",
        "# In a forward pass, autograd does two things simultaneously:\n",
        "#\n",
        "# - run the requested operation to compute a resulting tensor, and\n",
        "# - maintain the operation’s *gradient function* in the DAG.\n",
        "#\n",
        "# The backward pass kicks off when ``.backward()`` is called on the DAG root. \n",
        "# ``autograd`` then:\n",
        "#\n",
        "# - computes the gradients from each ``.grad_fn``,\n",
        "# - accumulates them in the respective tensor’s ``.grad`` attribute, and\n",
        "# - using the chain rule, propagates all the way to the leaf tensors.\n",
        "#\n",
        "# Below is a visual representation of the DAG in our example. \n",
        "# In the graph, the arrows are in the direction of the forward pass. \n",
        "# The nodes represent \n",
        "# the backward functions of each operation in the forward pass. \n",
        "# The leaf nodes in blue represent our leaf tensors ``a`` and ``b``.\n",
        "#\n",
        "# .. figure:: /_static/img/dag_autograd.png\n",
        "#\n",
        "# .. note::\n",
        "#   **DAGs are dynamic in PyTorch**\n",
        "#   An important thing to note is that the graph is recreated from scratch; \n",
        "#   after each ``.backward()`` call, autograd starts populating a new graph. \n",
        "#   This is exactly what allows you to use control flow statements in your model;\n",
        "#   you can change the shape, size and operations at every iteration if needed.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Exclusion from the DAG\n",
        "#\n",
        "# ``torch.autograd`` tracks operations on all tensors \n",
        "# which have their ``requires_grad`` flag set to ``True``. \n",
        "# For tensors that don’t require gradients, \n",
        "# setting this attribute to ``False`` \n",
        "# excludes it from the gradient computation DAG.\n",
        "#\n",
        "# The output tensor of an operation will require gradients \n",
        "# even if only a single input tensor has ``requires_grad=True``.\n",
        "################################################################################\n",
        "\n",
        "x = torch.rand(5, 5)\n",
        "y = torch.rand(5, 5)\n",
        "z = torch.rand((5, 5), requires_grad=True)\n",
        "\n",
        "a = x + y\n",
        "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
        "b = x + z\n",
        "print(f\"Does `b` require gradients?: {b.requires_grad}\")\n",
        "\n",
        "################################################################################\n",
        "# In a NN, parameters that don't compute gradients \n",
        "# are usually called **frozen parameters**.\n",
        "# It is useful to \"freeze\" part of your model \n",
        "# if you know in advance that you won't need the gradients of those parameters\n",
        "# (this offers some performance benefits by reducing autograd computations).\n",
        "#\n",
        "# Another common usecase where exclusion from the DAG is important is for\n",
        "# `finetuning a pretrained network \n",
        "# <https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html>`\n",
        "#\n",
        "# In finetuning, \n",
        "# we freeze most of the model and typically only modify the classifier layers \n",
        "# to make predictions on new labels.\n",
        "# Let's walk through a small example to demonstrate this. \n",
        "# As before, we load a pretrained resnet18 model, and freeze all the parameters.\n",
        "################################################################################\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# Freeze all the parameters in the network\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "################################################################################\n",
        "# Let's say we want to finetune the model on a new dataset with 10 labels.\n",
        "# In resnet, the classifier is the last linear layer ``model.fc``.\n",
        "# We can simply replace it with a new linear layer (unfrozen by default)\n",
        "# that acts as our classifier.\n",
        "################################################################################\n",
        "\n",
        "model.fc = nn.Linear(512, 10)\n",
        "\n",
        "################################################################################\n",
        "# Now all parameters in the model, except the parameters of ``model.fc``, \n",
        "# are frozen.\n",
        "# The only parameters that compute gradients are \n",
        "# the weights and bias of ``model.fc``.\n",
        "################################################################################\n",
        "\n",
        "# Optimize only the classifier\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "\n",
        "################################################################################\n",
        "# Notice although we register all the parameters in the optimizer,\n",
        "# the only parameters that are computing gradients \n",
        "# (and hence updated in gradient descent)\n",
        "# are the weights and bias of the classifier.\n",
        "#\n",
        "# The same exclusionary functionality is available as a context manager in\n",
        "# `torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html>`\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Further readings:\n",
        "#\n",
        "# -  `In-place operations & Multithreaded Autograd <https://pytorch.org/docs/stable/notes/autograd.html>`\n",
        "# -  `Example implementation of reverse-mode autodiff <https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC>`\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "J84WoAaMctJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309b5827-f69a-49de-b530-4219f61e7220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n",
            "Does `a` require gradients? : False\n",
            "Does `b` require gradients?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Transfer Learning [beginner_source/transfer_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py)"
      ],
      "metadata": {
        "id": "lDfrlXRcVckL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Transfer Learning for Computer Vision Tutorial\n",
        "==============================================\n",
        "**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_\n",
        "\n",
        "In this tutorial, you will learn how to train a convolutional neural network for\n",
        "image classification using transfer learning. You can read more about the transfer\n",
        "learning at `cs231n notes <https://cs231n.github.io/transfer-learning/>`__\n",
        "\n",
        "Quoting these notes,\n",
        "\n",
        "    In practice, very few people train an entire Convolutional Network\n",
        "    from scratch (with random initialization), because it is relatively\n",
        "    rare to have a dataset of sufficient size. Instead, it is common to\n",
        "    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
        "    contains 1.2 million images with 1000 categories), and then use the\n",
        "    ConvNet either as an initialization or a fixed feature extractor for\n",
        "    the task of interest.\n",
        "\n",
        "These two major transfer learning scenarios look as follows:\n",
        "\n",
        "-  **Finetuning the convnet**: Instead of random initialization, we\n",
        "   initialize the network with a pretrained network, like the one that is\n",
        "   trained on imagenet 1000 dataset. Rest of the training looks as\n",
        "   usual.\n",
        "-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n",
        "   for all of the network except that of the final fully connected\n",
        "   layer. This last fully connected layer is replaced with a new one\n",
        "   with random weights and only this layer is trained.\n",
        "\n",
        "\"\"\"\n",
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "######################################################################\n",
        "# Load Data\n",
        "# ---------\n",
        "#\n",
        "# We will use torchvision and torch.utils.data packages for loading the\n",
        "# data.\n",
        "#\n",
        "# The problem we're going to solve today is to train a model to classify\n",
        "# **ants** and **bees**. We have about 120 training images each for ants and bees.\n",
        "# There are 75 validation images for each class. Usually, this is a very\n",
        "# small dataset to generalize upon, if trained from scratch. Since we\n",
        "# are using transfer learning, we should be able to generalize reasonably\n",
        "# well.\n",
        "#\n",
        "# This dataset is a very small subset of imagenet.\n",
        "#\n",
        "# .. Note ::\n",
        "#    Download the data from\n",
        "#    `here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_\n",
        "#    and extract it to the current directory.\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = 'data/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "######################################################################\n",
        "# Visualize a few images\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^\n",
        "# Let's visualize a few training images so as to understand the data\n",
        "# augmentations.\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training the model\n",
        "# ------------------\n",
        "#\n",
        "# Now, let's write a general function to train a model. Here, we will\n",
        "# illustrate:\n",
        "#\n",
        "# -  Scheduling the learning rate\n",
        "# -  Saving the best model\n",
        "#\n",
        "# In the following, parameter ``scheduler`` is an LR scheduler object from\n",
        "# ``torch.optim.lr_scheduler``.\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Visualizing the model predictions\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# Generic function to display predictions for a few images\n",
        "#\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "######################################################################\n",
        "# Finetuning the convnet\n",
        "# ----------------------\n",
        "#\n",
        "# Load a pretrained model and reset final fully connected layer.\n",
        "#\n",
        "\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "######################################################################\n",
        "# Train and evaluate\n",
        "# ^^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
        "# minute.\n",
        "#\n",
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=25)\n",
        "\n",
        "######################################################################\n",
        "#\n",
        "\n",
        "visualize_model(model_ft)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# ConvNet as fixed feature extractor\n",
        "# ----------------------------------\n",
        "#\n",
        "# Here, we need to freeze all the network except the final layer. We need\n",
        "# to set ``requires_grad = False`` to freeze the parameters so that the\n",
        "# gradients are not computed in ``backward()``.\n",
        "#\n",
        "# You can read more about this in the documentation\n",
        "# `here <https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n",
        "#\n",
        "\n",
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Train and evaluate\n",
        "# ^^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# On CPU this will take about half the time compared to previous scenario.\n",
        "# This is expected as gradients don't need to be computed for most of the\n",
        "# network. However, forward does need to be computed.\n",
        "#\n",
        "\n",
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=25)\n",
        "\n",
        "######################################################################\n",
        "#\n",
        "\n",
        "visualize_model(model_conv)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()\n",
        "\n",
        "######################################################################\n",
        "# Further Learning\n",
        "# -----------------\n",
        "#\n",
        "# If you would like to learn more about the applications of transfer learning,\n",
        "# checkout our `Quantized Transfer Learning for Computer Vision Tutorial <https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html>`_.\n",
        "#\n",
        "#\n"
      ],
      "metadata": {
        "id": "lcf5STWiVcw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 12 Super Resolution***"
      ],
      "metadata": {
        "id": "OMHWOhlyIGZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Super Resolution [examples/blob/main/super_resolution/main.py](https://github.com/pytorch/examples/blob/main/super_resolution/main.py) [Many More Sorce Files Needed](https://github.com/pytorch/examples/tree/main/super_resolution)"
      ],
      "metadata": {
        "id": "U12wzVA_vwrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "from math import log10\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from model import Net\n",
        "from data import get_training_set, get_test_set\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
        "parser.add_argument('--upscale_factor', type=int, required=True, help=\"super resolution upscale factor\")\n",
        "parser.add_argument('--batchSize', type=int, default=64, help='training batch size')\n",
        "parser.add_argument('--testBatchSize', type=int, default=10, help='testing batch size')\n",
        "parser.add_argument('--nEpochs', type=int, default=2, help='number of epochs to train for')\n",
        "parser.add_argument('--lr', type=float, default=0.01, help='Learning Rate. Default=0.01')\n",
        "parser.add_argument('--cuda', action='store_true', help='use cuda?')\n",
        "parser.add_argument('--mps', action='store_true', default=False, help='enables macOS GPU training')\n",
        "parser.add_argument('--threads', type=int, default=4, help='number of threads for data loader to use')\n",
        "parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
        "opt = parser.parse_args()\n",
        "\n",
        "print(opt)\n",
        "\n",
        "if opt.cuda and not torch.cuda.is_available():\n",
        "    raise Exception(\"No GPU found, please run without --cuda\")\n",
        "if not opt.mps and torch.backends.mps.is_available():\n",
        "    raise Exception(\"Found mps device, please run with --mps to enable macOS GPU\")\n",
        "\n",
        "torch.manual_seed(opt.seed)\n",
        "use_mps = opt.mps and torch.backends.mps.is_available()\n",
        "\n",
        "if opt.cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print('===> Loading datasets')\n",
        "train_set = get_training_set(opt.upscale_factor)\n",
        "test_set = get_test_set(opt.upscale_factor)\n",
        "training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)\n",
        "testing_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.testBatchSize, shuffle=False)\n",
        "\n",
        "print('===> Building model')\n",
        "model = Net(upscale_factor=opt.upscale_factor).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=opt.lr)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    epoch_loss = 0\n",
        "    for iteration, batch in enumerate(training_data_loader, 1):\n",
        "        input, target = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(input), target)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, len(training_data_loader), loss.item()))\n",
        "\n",
        "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))\n",
        "\n",
        "\n",
        "def test():\n",
        "    avg_psnr = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in testing_data_loader:\n",
        "            input, target = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "            prediction = model(input)\n",
        "            mse = criterion(prediction, target)\n",
        "            psnr = 10 * log10(1 / mse.item())\n",
        "            avg_psnr += psnr\n",
        "    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\n",
        "\n",
        "\n",
        "def checkpoint(epoch):\n",
        "    model_out_path = \"model_epoch_{}.pth\".format(epoch)\n",
        "    torch.save(model, model_out_path)\n",
        "    print(\"Checkpoint saved to {}\".format(model_out_path))\n",
        "\n",
        "for epoch in range(1, opt.nEpochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    checkpoint(epoch)"
      ],
      "metadata": {
        "id": "Vq4HdJAIvw9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 13 Neural Style Transfer***"
      ],
      "metadata": {
        "id": "4fu8UsdYIMva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Fast Neural Style [examples/blob/main/fast_neural_style/download_saved_models.py](https://github.com/pytorch/examples/blob/main/fast_neural_style/download_saved_models.py) [Many More Sorce Files Needed](https://github.com/pytorch/examples/tree/main/fast_neural_style)"
      ],
      "metadata": {
        "id": "K10FkeNtvI6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# PyTorch 1.1 moves _download_url_to_file\n",
        "#   from torch.utils.model_zoo to torch.hub\n",
        "# PyTorch 1.0 exists another _download_url_to_file\n",
        "#   2 argument\n",
        "# TODO: If you remove support PyTorch 1.0 or older,\n",
        "#       You should remove torch.utils.model_zoo\n",
        "#       Ref. PyTorch #18758\n",
        "#         https://github.com/pytorch/pytorch/pull/18758/commits\n",
        "try:\n",
        "    from torch.utils.model_zoo import _download_url_to_file\n",
        "except ImportError:\n",
        "    try:\n",
        "        from torch.hub import download_url_to_file as _download_url_to_file\n",
        "    except ImportError:\n",
        "        from torch.hub import _download_url_to_file\n",
        "\n",
        "\n",
        "def unzip(source_filename, dest_dir):\n",
        "    with zipfile.ZipFile(source_filename) as zf:\n",
        "        zf.extractall(path=dest_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    _download_url_to_file('https://www.dropbox.com/s/lrvwfehqdcxoza8/saved_models.zip?dl=1', 'saved_models.zip', None, True)\n",
        "    unzip('saved_models.zip', '.')"
      ],
      "metadata": {
        "id": "SHlfJ_vovJWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 14 NLP***"
      ],
      "metadata": {
        "id": "Bo7c2yA-pTPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [beginner_source/nlp/deep_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/nlp/deep_learning_tutorial.py)"
      ],
      "metadata": {
        "id": "n_oliqdmpW2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Deep Learning with PyTorch\n",
        "**************************\n",
        "\n",
        "Deep Learning Building Blocks: Affine maps, non-linearities and objectives\n",
        "==========================================================================\n",
        "\n",
        "Deep learning consists of composing linearities with non-linearities in\n",
        "clever ways. The introduction of non-linearities allows for powerful\n",
        "models. In this section, we will play with these core components, make\n",
        "up an objective function, and see how the model is trained.\n",
        "\n",
        "\n",
        "Affine Maps\n",
        "~~~~~~~~~~~\n",
        "\n",
        "One of the core workhorses of deep learning is the affine map, which is\n",
        "a function :math:`f(x)` where\n",
        "\n",
        ".. math::  f(x) = Ax + b\n",
        "\n",
        "for a matrix :math:`A` and vectors :math:`x, b`. The parameters to be\n",
        "learned here are :math:`A` and :math:`b`. Often, :math:`b` is refered to\n",
        "as the *bias* term.\n",
        "\n",
        "\n",
        "PyTorch and most other deep learning frameworks do things a little\n",
        "differently than traditional linear algebra. It maps the rows of the\n",
        "input instead of the columns. That is, the :math:`i`'th row of the\n",
        "output below is the mapping of the :math:`i`'th row of the input under\n",
        ":math:`A`, plus the bias term. Look at the example below.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
        "# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
        "data = torch.randn(2, 5)\n",
        "print(lin(data))  # yes\n",
        "\n",
        "################################################################################\n",
        "# Non-Linearities\n",
        "# ~~~~~~~~~~~~~~~\n",
        "#\n",
        "# First, note the following fact, which will explain why we need\n",
        "# non-linearities in the first place. Suppose we have two affine maps\n",
        "# :math:`f(x) = Ax + b` and :math:`g(x) = Cx + d`. What is\n",
        "# :math:`f(g(x))`?\n",
        "#\n",
        "# .. math::  f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\n",
        "#\n",
        "# :math:`AC` is a matrix and :math:`Ad + b` is a vector, so we see that\n",
        "# composing affine maps gives you an affine map.\n",
        "#\n",
        "# From this, you can see that if you wanted your neural network to be long\n",
        "# chains of affine compositions, that this adds no new power to your model\n",
        "# than just doing a single affine map.\n",
        "#\n",
        "# If we introduce non-linearities in between the affine layers, this is no\n",
        "# longer the case, and we can build much more powerful models.\n",
        "#\n",
        "# There are a few core non-linearities.\n",
        "# :math:`\\tanh(x), \\sigma(x), \\text{ReLU}(x)` are the most common. You are\n",
        "# probably wondering: \"why these functions? I can think of plenty of other\n",
        "# non-linearities.\" The reason for this is that they have gradients that\n",
        "# are easy to compute, and computing gradients is essential for learning.\n",
        "# For example\n",
        "#\n",
        "# .. math::  \\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\n",
        "#\n",
        "# A quick note: although you may have learned some neural networks in your\n",
        "# intro to AI class where :math:`\\sigma(x)` was the default non-linearity,\n",
        "# typically people shy away from it in practice. This is because the\n",
        "# gradient *vanishes* very quickly as the absolute value of the argument\n",
        "# grows. Small gradients means it is hard to learn. Most people default to\n",
        "# tanh or ReLU.\n",
        "#\n",
        "#\n",
        "# In pytorch, most non-linearities are in torch.functional \n",
        "# (we have it imported as F)\n",
        "# Note that non-linearites typically don't have parameters like affine maps do.\n",
        "# That is, they don't have weights that are updated during training.\n",
        "data = torch.randn(2, 2)\n",
        "print(data)\n",
        "print(F.relu(data))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Softmax and Probabilities\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# The function :math:`\\text{Softmax}(x)` is also just a non-linearity, but\n",
        "# it is special in that it usually is the last operation done in a\n",
        "# network. This is because it takes in a vector of real numbers and\n",
        "# returns a probability distribution. Its definition is as follows. Let\n",
        "# :math:`x` be a vector of real numbers (positive, negative, whatever,\n",
        "# there are no constraints). Then the i'th component of\n",
        "# :math:`\\text{Softmax}(x)` is\n",
        "#\n",
        "# .. math::  \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
        "#\n",
        "# It should be clear that the output is a probability distribution: each\n",
        "# element is non-negative and the sum over all components is 1.\n",
        "#\n",
        "# You could also think of it as just applying an element-wise\n",
        "# exponentiation operator to the input to make everything non-negative and\n",
        "# then dividing by the normalization constant.\n",
        "#\n",
        "\n",
        "# Softmax is also in torch.nn.functional\n",
        "data = torch.randn(5)\n",
        "print(data)\n",
        "print(F.softmax(data, dim=0))\n",
        "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
        "print(F.log_softmax(data, dim=0))  # theres also log_softmax\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Objective Functions\n",
        "# ~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# The objective function is the function that your network is being\n",
        "# trained to minimize (in which case it is often called a *loss function*\n",
        "# or *cost function*). This proceeds by first choosing a training\n",
        "# instance, running it through your neural network, and then computing the\n",
        "# loss of the output. The parameters of the model are then updated by\n",
        "# taking the derivative of the loss function. Intuitively, if your model\n",
        "# is completely confident in its answer, and its answer is wrong, your\n",
        "# loss will be high. If it is very confident in its answer, and its answer\n",
        "# is correct, the loss will be low.\n",
        "#\n",
        "# The idea behind minimizing the loss function on your training examples\n",
        "# is that your network will hopefully generalize well and have small loss\n",
        "# on unseen examples in your dev set, test set, or in production. An\n",
        "# example loss function is the *negative log likelihood loss*, which is a\n",
        "# very common objective for multi-class classification. For supervised\n",
        "# multi-class classification, this means training the network to minimize\n",
        "# the negative log probability of the correct output (or equivalently,\n",
        "# maximize the log probability of the correct output).\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Optimization and Training\n",
        "# =========================\n",
        "#\n",
        "# So what we can compute a loss function for an instance? What do we do\n",
        "# with that? We saw earlier that Tensors know how to compute gradients\n",
        "# with respect to the things that were used to compute it. Well,\n",
        "# since our loss is an Tensor, we can compute gradients with\n",
        "# respect to all of the parameters used to compute it! Then we can perform\n",
        "# standard gradient updates. Let :math:`\\theta` be our parameters,\n",
        "# :math:`L(\\theta)` the loss function, and :math:`\\eta` a positive\n",
        "# learning rate. Then:\n",
        "#\n",
        "# .. math::  \\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\n",
        "#\n",
        "# There are a huge collection of algorithms and active research in\n",
        "# attempting to do something more than just this vanilla gradient update.\n",
        "# Many attempt to vary the learning rate based on what is happening at\n",
        "# train time. You don't need to worry about what specifically these\n",
        "# algorithms are doing unless you are really interested. Torch provides\n",
        "# many in the torch.optim package, and they are all completely\n",
        "# transparent. Using the simplest gradient update is the same as the more\n",
        "# complicated algorithms. Trying different update algorithms and different\n",
        "# parameters for the update algorithms (like different initial learning\n",
        "# rates) is important in optimizing your network's performance. Often,\n",
        "# just replacing vanilla SGD with an optimizer like Adam or RMSProp will\n",
        "# boost performance noticably.\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Creating Network Components in PyTorch\n",
        "# ======================================\n",
        "#\n",
        "# Before we move on to our focus on NLP, lets do an annotated example of\n",
        "# building a network in PyTorch using only affine maps and\n",
        "# non-linearities. We will also see how to compute a loss function, using\n",
        "# PyTorch's built in negative log likelihood, and update parameters by\n",
        "# backpropagation.\n",
        "#\n",
        "# All network components should inherit from nn.Module and override the\n",
        "# forward() method. That is about it, as far as the boilerplate is\n",
        "# concerned. Inheriting from nn.Module provides functionality to your\n",
        "# component. For example, it makes it keep track of its trainable\n",
        "# parameters, you can swap it between CPU and GPU with the ``.to(device)``\n",
        "# method, where device can be a CPU device ``torch.device(\"cpu\")`` or CUDA\n",
        "# device ``torch.device(\"cuda:0\")``.\n",
        "#\n",
        "# Let's write an annotated example of a network that takes in a sparse\n",
        "# bag-of-words representation and outputs a probability distribution over\n",
        "# two labels: \"English\" and \"Spanish\". This model is just logistic\n",
        "# regression.\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Example: Logistic Regression Bag-of-Words classifier\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# Our model will map a sparse BoW representation to log probabilities over\n",
        "# labels. We assign each word in the vocab an index. For example, say our\n",
        "# entire vocab is two words \"hello\" and \"world\", with indices 0 and 1\n",
        "# respectively. The BoW vector for the sentence \"hello hello hello hello\"\n",
        "# is\n",
        "#\n",
        "# .. math::  \\left[ 4, 0 \\right]\n",
        "#\n",
        "# For \"hello world world hello\", it is\n",
        "#\n",
        "# .. math::  \\left[ 2, 2 \\right]\n",
        "#\n",
        "# etc. In general, it is\n",
        "#\n",
        "# .. math::  \\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\n",
        "#\n",
        "# Denote this BOW vector as :math:`x`. The output of our network is:\n",
        "#\n",
        "# .. math::  \\log \\text{Softmax}(Ax + b)\n",
        "#\n",
        "# That is, we pass the input through an affine map and then do log\n",
        "# softmax.\n",
        "#\n",
        "\n",
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
        "# index into the Bag of words vector\n",
        "word_to_ix = {}\n",
        "for sent, _ in data + test_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = 2\n",
        "\n",
        "\n",
        "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
        "\n",
        "    def __init__(self, num_labels, vocab_size):\n",
        "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "        # just always do it in an nn.Module\n",
        "        super(BoWClassifier, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need.  In this case, we need A and b,\n",
        "        # the parameters of the affine mapping.\n",
        "        # Torch defines nn.Linear(), which provides the affine map.\n",
        "        # Make sure you understand why the input dimension is vocab_size\n",
        "        # and the output is num_labels!\n",
        "        self.linear = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
        "        # to worry about that here\n",
        "\n",
        "    def forward(self, bow_vec):\n",
        "        # Pass the input through the linear layer,\n",
        "        # then pass that through log_softmax.\n",
        "        # Many non-linearities and other functions are in torch.nn.functional\n",
        "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
        "\n",
        "\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        vec[word_to_ix[word]] += 1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "\n",
        "def make_target(label, label_to_ix):\n",
        "    return torch.LongTensor([label_to_ix[label]])\n",
        "\n",
        "\n",
        "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
        "\n",
        "# the model knows its parameters.  The first output below is A, the second is b.\n",
        "# Whenever you assign a component to a class variable in the __init__ function\n",
        "# of a module, which was done with the line\n",
        "# self.linear = nn.Linear(...)\n",
        "# Then through some Python magic from the PyTorch devs, your module\n",
        "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
        "for param in model.parameters():\n",
        "    print(param)\n",
        "\n",
        "# To run the model, pass in a BoW vector\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    sample = data[0]\n",
        "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
        "    log_probs = model(bow_vector)\n",
        "    print(log_probs)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Which of the above values corresponds to the log probability of ENGLISH,\n",
        "# and which to SPANISH? We never defined it, but we need to if we want to\n",
        "# train the thing.\n",
        "#\n",
        "\n",
        "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# So lets train! To do this, we pass instances through to get log\n",
        "# probabilities, compute a loss function, compute the gradient of the loss\n",
        "# function, and then update the parameters with a gradient step. Loss\n",
        "# functions are provided by Torch in the nn package. nn.NLLLoss() is the\n",
        "# negative log likelihood loss we want. It also defines optimization\n",
        "# functions in torch.optim. Here, we will just use SGD.\n",
        "#\n",
        "# Note that the *input* to NLLLoss is a vector of log probabilities, and a\n",
        "# target label. It doesn't compute the log probabilities for us. This is\n",
        "# why the last layer of our network is log softmax. The loss function\n",
        "# nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log\n",
        "# softmax for you.\n",
        "#\n",
        "\n",
        "# Run on test data before we train, just to see a before-and-after\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# Print the matrix column corresponding to \"creo\"\n",
        "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
        "for epoch in range(100):\n",
        "    for instance, label in data:\n",
        "        # Step 1. Remember that PyTorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
        "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
        "        # we wrap the integer 0. The loss function then knows that the 0th\n",
        "        # element of the log probabilities is the log probability\n",
        "        # corresponding to SPANISH\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        target = make_target(label, label_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        log_probs = model(bow_vec)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss = loss_function(log_probs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# Index corresponding to Spanish goes up, English goes down!\n",
        "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# We got the right answer! You can see that the log probability for\n",
        "# Spanish is much higher in the first example, and the log probability for\n",
        "# English is much higher in the second for the test data, as it should be.\n",
        "#\n",
        "# Now you see how to make a PyTorch component, pass some data through it\n",
        "# and do gradient updates. We are ready to dig deeper into what deep NLP\n",
        "# has to offer.\n",
        "#"
      ],
      "metadata": {
        "id": "sV8Yg9E1pW-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [beginner_source/nlp/word_embeddings_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/nlp/word_embeddings_tutorial.py)"
      ],
      "metadata": {
        "id": "FmEFErTspXGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Word Embeddings: Encoding Lexical Semantics\n",
        "===========================================\n",
        "\n",
        "Word embeddings are dense vectors of real numbers, one per word in your\n",
        "vocabulary. In NLP, it is almost always the case that your features are\n",
        "words! But how should you represent a word in a computer? You could\n",
        "store its ascii character representation, but that only tells you what\n",
        "the word *is*, it doesn't say much about what it *means* (you might be\n",
        "able to derive its part of speech from its affixes, or properties from\n",
        "its capitalization, but not much). Even more, in what sense could you\n",
        "combine these representations? We often want dense outputs from our\n",
        "neural networks, where the inputs are :math:`|V|` dimensional, where\n",
        ":math:`V` is our vocabulary, but often the outputs are only a few\n",
        "dimensional (if we are only predicting a handful of labels, for\n",
        "instance). How do we get from a massive dimensional space to a smaller\n",
        "dimensional space?\n",
        "\n",
        "How about instead of ascii representations, we use a one-hot encoding?\n",
        "That is, we represent the word :math:`w` by\n",
        "\n",
        ".. math::  \\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\n",
        "\n",
        "where the 1 is in a location unique to :math:`w`. Any other word will\n",
        "have a 1 in some other location, and a 0 everywhere else.\n",
        "\n",
        "There is an enormous drawback to this representation, besides just how\n",
        "huge it is. It basically treats all words as independent entities with\n",
        "no relation to each other. What we really want is some notion of\n",
        "*similarity* between words. Why? Let's see an example.\n",
        "\n",
        "Suppose we are building a language model. Suppose we have seen the\n",
        "sentences\n",
        "\n",
        "* The mathematician ran to the store.\n",
        "* The physicist ran to the store.\n",
        "* The mathematician solved the open problem.\n",
        "\n",
        "in our training data. Now suppose we get a new sentence never before\n",
        "seen in our training data:\n",
        "\n",
        "* The physicist solved the open problem.\n",
        "\n",
        "Our language model might do OK on this sentence, but wouldn't it be much\n",
        "better if we could use the following two facts:\n",
        "\n",
        "* We have seen  mathematician and physicist in the same role in a sentence. Somehow they\n",
        "  have a semantic relation.\n",
        "* We have seen mathematician in the same role  in this new unseen sentence\n",
        "  as we are now seeing physicist.\n",
        "\n",
        "and then infer that physicist is actually a good fit in the new unseen\n",
        "sentence? This is what we mean by a notion of similarity: we mean\n",
        "*semantic similarity*, not simply having similar orthographic\n",
        "representations. It is a technique to combat the sparsity of linguistic\n",
        "data, by connecting the dots between what we have seen and what we\n",
        "haven't. This example of course relies on a fundamental linguistic\n",
        "assumption: that words appearing in similar contexts are related to each\n",
        "other semantically. This is called the `distributional\n",
        "hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n",
        "\n",
        "\n",
        "Getting Dense Word Embeddings\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "How can we solve this problem? That is, how could we actually encode\n",
        "semantic similarity in words? Maybe we think up some semantic\n",
        "attributes. For example, we see that both mathematicians and physicists\n",
        "can run, so maybe we give these words a high score for the \"is able to\n",
        "run\" semantic attribute. Think of some other attributes, and imagine\n",
        "what you might score some common words on those attributes.\n",
        "\n",
        "If each attribute is a dimension, then we might give each word a vector,\n",
        "like this:\n",
        "\n",
        ".. math::\n",
        "\n",
        "    q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
        "   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\n",
        "\n",
        ".. math::\n",
        "\n",
        "    q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
        "   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\n",
        "\n",
        "Then we can get a measure of similarity between these words by doing:\n",
        "\n",
        ".. math::  \\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\n",
        "\n",
        "Although it is more common to normalize by the lengths:\n",
        "\n",
        ".. math::\n",
        "\n",
        "    \\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n",
        "   {\\| q_\\text{physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\n",
        "\n",
        "Where :math:`\\phi` is the angle between the two vectors. That way,\n",
        "extremely similar words (words whose embeddings point in the same\n",
        "direction) will have similarity 1. Extremely dissimilar words should\n",
        "have similarity -1.\n",
        "\n",
        "\n",
        "You can think of the sparse one-hot vectors from the beginning of this\n",
        "section as a special case of these new vectors we have defined, where\n",
        "each word basically has similarity 0, and we gave each word some unique\n",
        "semantic attribute. These new vectors are *dense*, which is to say their\n",
        "entries are (typically) non-zero.\n",
        "\n",
        "But these new vectors are a big pain: you could think of thousands of\n",
        "different semantic attributes that might be relevant to determining\n",
        "similarity, and how on earth would you set the values of the different\n",
        "attributes? Central to the idea of deep learning is that the neural\n",
        "network learns representations of the features, rather than requiring\n",
        "the programmer to design them herself. So why not just let the word\n",
        "embeddings be parameters in our model, and then be updated during\n",
        "training? This is exactly what we will do. We will have some *latent\n",
        "semantic attributes* that the network can, in principle, learn. Note\n",
        "that the word embeddings will probably not be interpretable. That is,\n",
        "although with our hand-crafted vectors above we can see that\n",
        "mathematicians and physicists are similar in that they both like coffee,\n",
        "if we allow a neural network to learn the embeddings and see that both\n",
        "mathematicians and physicists have a large value in the second\n",
        "dimension, it is not clear what that means. They are similar in some\n",
        "latent semantic dimension, but this probably has no interpretation to\n",
        "us.\n",
        "\n",
        "\n",
        "In summary, **word embeddings are a representation of the *semantics* of\n",
        "a word, efficiently encoding semantic information that might be relevant\n",
        "to the task at hand**. You can embed other things too: part of speech\n",
        "tags, parse trees, anything! The idea of feature embeddings is central\n",
        "to the field.\n",
        "\n",
        "\n",
        "Word Embeddings in Pytorch\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Before we get to a worked example and an exercise, a few quick notes\n",
        "about how to use embeddings in Pytorch and in deep learning programming\n",
        "in general. Similar to how we defined a unique index for each word when\n",
        "making one-hot vectors, we also need to define an index for each word\n",
        "when using embeddings. These will be keys into a lookup table. That is,\n",
        "embeddings are stored as a :math:`|V| \\times D` matrix, where :math:`D`\n",
        "is the dimensionality of the embeddings, such that the word assigned\n",
        "index :math:`i` has its embedding stored in the :math:`i`'th row of the\n",
        "matrix. In all of my code, the mapping from words to indices is a\n",
        "dictionary named word\\_to\\_ix.\n",
        "\n",
        "The module that allows you to use embeddings is torch.nn.Embedding,\n",
        "which takes two arguments: the vocabulary size, and the dimensionality\n",
        "of the embeddings.\n",
        "\n",
        "To index into this table, you must use torch.LongTensor (since the\n",
        "indices are integers, not floats).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)\n",
        "\n",
        "################################################################################\n",
        "# An Example: N-Gram Language Modeling\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# Recall that in an n-gram language model, given a sequence of words\n",
        "# :math:`w`, we want to compute\n",
        "#\n",
        "# .. math::  P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\n",
        "#\n",
        "# Where :math:`w_i` is the ith word of the sequence.\n",
        "#\n",
        "# In this example, we will compute the loss function on some training\n",
        "# examples and update the parameters with backpropagation.\n",
        "#\n",
        "\n",
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.\n",
        "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
        "ngrams = [\n",
        "    (\n",
        "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
        "        test_sentence[i]\n",
        "    )\n",
        "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
        "]\n",
        "# Print the first 3, just so you can see what they look like.\n",
        "print(ngrams[:3])\n",
        "\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in ngrams:\n",
        "\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "# To get the embedding of a particular word, e.g. \"beauty\"\n",
        "print(model.embeddings.weight[word_to_ix[\"beauty\"]])\n",
        "\n",
        "######################################################################\n",
        "# Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
        "# learning. It is a model that tries to predict words given the context of\n",
        "# a few words before and a few words after the target word. This is\n",
        "# distinct from language modeling, since CBOW is not sequential and does\n",
        "# not have to be probabilistic. Typically, CBOW is used to quickly train\n",
        "# word embeddings, and these embeddings are used to initialize the\n",
        "# embeddings of some more complicated model. Usually, this is referred to\n",
        "# as *pretraining embeddings*. It almost always helps performance a couple\n",
        "# of percent.\n",
        "#\n",
        "# The CBOW model is as follows. Given a target word :math:`w_i` and an\n",
        "# :math:`N` context window on each side, :math:`w_{i-1}, \\dots, w_{i-N}`\n",
        "# and :math:`w_{i+1}, \\dots, w_{i+N}`, referring to all context words\n",
        "# collectively as :math:`C`, CBOW tries to minimize\n",
        "#\n",
        "# .. math::  -\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right)\n",
        "#\n",
        "# where :math:`q_w` is the embedding for word :math:`w`.\n",
        "#\n",
        "# Implement this model in Pytorch by filling in the class below. Some\n",
        "# tips:\n",
        "#\n",
        "# * Think about which parameters you need to define.\n",
        "# * Make sure you know what shape each operation expects. Use .view() if you need to\n",
        "#   reshape.\n",
        "#\n",
        "\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "\n",
        "# By deriving a set from `raw_text`, we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "# Create your model and train. Here are some functions to help you make\n",
        "# the data ready for use by your module.\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)  # example"
      ],
      "metadata": {
        "id": "GVHWKlqIpXN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [beginner_source/nlp/sequence_models_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/nlp/sequence_models_tutorial.py)"
      ],
      "metadata": {
        "id": "Ngk5ZNinpXU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Sequence Models and Long Short-Term Memory Networks\n",
        "===================================================\n",
        "\n",
        "At this point, we have seen various feed-forward networks. That is,\n",
        "there is no state maintained by the network at all. This might not be\n",
        "the behavior we want. Sequence models are central to NLP: they are\n",
        "models where there is some sort of dependence through time between your\n",
        "inputs. The classical example of a sequence model is the Hidden Markov\n",
        "Model for part-of-speech tagging. Another example is the conditional\n",
        "random field.\n",
        "\n",
        "A recurrent neural network is a network that maintains some kind of\n",
        "state. For example, its output could be used as part of the next input,\n",
        "so that information can propagate along as the network passes over the\n",
        "sequence. In the case of an LSTM, for each element in the sequence,\n",
        "there is a corresponding *hidden state* :math:`h_t`, which in principle\n",
        "can contain information from arbitrary points earlier in the sequence.\n",
        "We can use the hidden state to predict words in a language model,\n",
        "part-of-speech tags, and a myriad of other things.\n",
        "\n",
        "\n",
        "LSTMs in Pytorch\n",
        "~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Before getting to the example, note a few things. Pytorch's LSTM expects\n",
        "all of its inputs to be 3D tensors. The semantics of the axes of these\n",
        "tensors is important. The first axis is the sequence itself, the second\n",
        "indexes instances in the mini-batch, and the third indexes elements of\n",
        "the input. We haven't discussed mini-batching, so let's just ignore that\n",
        "and assume we will always have just 1 dimension on the second axis. If\n",
        "we want to run the sequence model over the sentence \"The cow jumped\",\n",
        "our input should look like\n",
        "\n",
        ".. math::\n",
        "\n",
        "\n",
        "   \\begin{bmatrix}\n",
        "   \\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n",
        "   q_\\text{cow} \\\\\n",
        "   q_\\text{jumped}\n",
        "   \\end{bmatrix}\n",
        "\n",
        "Except remember there is an additional 2nd dimension with size 1.\n",
        "\n",
        "In addition, you could go through the sequence one at a time, in which\n",
        "case the 1st axis will have size 1 also.\n",
        "\n",
        "Let's see a quick example.\n",
        "\"\"\"\n",
        "\n",
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "# alternatively, we can do the entire sequence all at once.\n",
        "# the first value returned by LSTM is all of the hidden states throughout\n",
        "# the sequence. the second is just the most recent hidden state\n",
        "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "# The reason for this is that:\n",
        "# \"out\" will give you access to all hidden states in the sequence\n",
        "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "# by passing it as an argument  to the lstm at a later time\n",
        "# Add the extra 2nd dimension\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Example: An LSTM for Part-of-Speech Tagging\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# In this section, we will use an LSTM to get part of speech tags. We will\n",
        "# not use Viterbi or Forward-Backward or anything like that, but as a\n",
        "# (challenging) exercise to the reader, think about how Viterbi could be\n",
        "# used after you have seen what is going on. In this example, we also refer\n",
        "# to embeddings. If you are unfamiliar with embeddings, you can read up \n",
        "# about them `here <https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html>`__.\n",
        "#\n",
        "# The model is as follows: let our input sentence be\n",
        "# :math:`w_1, \\dots, w_M`, where :math:`w_i \\in V`, our vocab. Also, let\n",
        "# :math:`T` be our tag set, and :math:`y_i` the tag of word :math:`w_i`.\n",
        "# Denote our prediction of the tag of word :math:`w_i` by\n",
        "# :math:`\\hat{y}_i`.\n",
        "#\n",
        "# This is a structure prediction, model, where our output is a sequence\n",
        "# :math:`\\hat{y}_1, \\dots, \\hat{y}_M`, where :math:`\\hat{y}_i \\in T`.\n",
        "#\n",
        "# To do the prediction, pass an LSTM over the sentence. Denote the hidden\n",
        "# state at timestep :math:`i` as :math:`h_i`. Also, assign each tag a\n",
        "# unique index (like how we had word\\_to\\_ix in the word embeddings\n",
        "# section). Then our prediction rule for :math:`\\hat{y}_i` is\n",
        "#\n",
        "# .. math::  \\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\n",
        "#\n",
        "# That is, take the log softmax of the affine map of the hidden state,\n",
        "# and the predicted tag is the tag that has the maximum value in this\n",
        "# vector. Note this implies immediately that the dimensionality of the\n",
        "# target space of :math:`A` is :math:`|T|`.\n",
        "#\n",
        "# Prepare data:\n",
        "################################################################################\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    # Tags are: DET - determiner; NN - noun; V - verb\n",
        "    # For example, the word \"The\" is a determiner \n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
        "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
        "\n",
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "\n",
        "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
        "    # for word i. The predicted tag is the maximum scoring tag.\n",
        "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "    # since 0 is index of the maximum value of row 1,\n",
        "    # 1 is the index of maximum value of row 2, etc.\n",
        "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "    print(tag_scores)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Exercise: Augmenting the LSTM part-of-speech tagger with character-level features\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "# In the example above, each word had an embedding, which served as the\n",
        "# inputs to our sequence model. Let's augment the word embeddings with a\n",
        "# representation derived from the characters of the word. We expect that\n",
        "# this should help significantly, since character-level information like\n",
        "# affixes have a large bearing on part-of-speech. For example, words with\n",
        "# the affix *-ly* are almost always tagged as adverbs in English.\n",
        "#\n",
        "# To do this, let :math:`c_w` be the character-level representation of\n",
        "# word :math:`w`. Let :math:`x_w` be the word embedding as before. Then\n",
        "# the input to our sequence model is the concatenation of :math:`x_w` and\n",
        "# :math:`c_w`. So if :math:`x_w` has dimension 5, and :math:`c_w`\n",
        "# dimension 3, then our LSTM should accept an input of dimension 8.\n",
        "#\n",
        "# To get the character level representation, do an LSTM over the\n",
        "# characters of a word, and let :math:`c_w` be the final hidden state of\n",
        "# this LSTM. Hints:\n",
        "#\n",
        "# * There are going to be two LSTM's in your new model.\n",
        "#   The original one that outputs POS tag scores, and the new one that\n",
        "#   outputs a character-level representation of each word.\n",
        "# * To do a sequence model over characters, you will have to embed characters.\n",
        "#   The character embeddings will be the input to the character LSTM.\n",
        "#"
      ],
      "metadata": {
        "id": "Cc4n0zPSpXco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [beginner_source/nlp/advanced_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/nlp/advanced_tutorial.py)"
      ],
      "metadata": {
        "id": "gBpHGlyepXqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Advanced: Making Dynamic Decisions and the Bi-LSTM CRF\n",
        "======================================================\n",
        "\n",
        "Dynamic versus Static Deep Learning Toolkits\n",
        "--------------------------------------------\n",
        "\n",
        "Pytorch is a *dynamic* neural network kit. Another example of a dynamic\n",
        "kit is `Dynet <https://github.com/clab/dynet>`__ (I mention this because\n",
        "working with Pytorch and Dynet is similar. If you see an example in\n",
        "Dynet, it will probably help you implement it in Pytorch). The opposite\n",
        "is the *static* tool kit, which includes Theano, Keras, TensorFlow, etc.\n",
        "The core difference is the following:\n",
        "\n",
        "* In a static toolkit, you define\n",
        "  a computation graph once, compile it, and then stream instances to it.\n",
        "* In a dynamic toolkit, you define a computation graph *for each\n",
        "  instance*. It is never compiled and is executed on-the-fly\n",
        "\n",
        "Without a lot of experience, it is difficult to appreciate the\n",
        "difference. One example is to suppose we want to build a deep\n",
        "constituent parser. Suppose our model involves roughly the following\n",
        "steps:\n",
        "\n",
        "* We build the tree bottom up\n",
        "* Tag the root nodes (the words of the sentence)\n",
        "* From there, use a neural network and the embeddings\n",
        "  of the words to find combinations that form constituents. Whenever you\n",
        "  form a new constituent, use some sort of technique to get an embedding\n",
        "  of the constituent. In this case, our network architecture will depend\n",
        "  completely on the input sentence. In the sentence \"The green cat\n",
        "  scratched the wall\", at some point in the model, we will want to combine\n",
        "  the span :math:`(i,j,r) = (1, 3, \\text{NP})` (that is, an NP constituent\n",
        "  spans word 1 to word 3, in this case \"The green cat\").\n",
        "\n",
        "However, another sentence might be \"Somewhere, the big fat cat scratched\n",
        "the wall\". In this sentence, we will want to form the constituent\n",
        ":math:`(2, 4, NP)` at some point. The constituents we will want to form\n",
        "will depend on the instance. If we just compile the computation graph\n",
        "once, as in a static toolkit, it will be exceptionally difficult or\n",
        "impossible to program this logic. In a dynamic toolkit though, there\n",
        "isn't just 1 pre-defined computation graph. There can be a new\n",
        "computation graph for each instance, so this problem goes away.\n",
        "\n",
        "Dynamic toolkits also have the advantage of being easier to debug and\n",
        "the code more closely resembling the host language (by that I mean that\n",
        "Pytorch and Dynet look more like actual Python code than Keras or\n",
        "Theano).\n",
        "\n",
        "Bi-LSTM Conditional Random Field Discussion\n",
        "-------------------------------------------\n",
        "\n",
        "For this section, we will see a full, complicated example of a Bi-LSTM\n",
        "Conditional Random Field for named-entity recognition. The LSTM tagger\n",
        "above is typically sufficient for part-of-speech tagging, but a sequence\n",
        "model like the CRF is really essential for strong performance on NER.\n",
        "Familiarity with CRF's is assumed. Although this name sounds scary, all\n",
        "the model is a CRF but where an LSTM provides the features. This is\n",
        "an advanced model though, far more complicated than any earlier model in\n",
        "this tutorial. If you want to skip it, that is fine. To see if you're\n",
        "ready, see if you can:\n",
        "\n",
        "-  Write the recurrence for the viterbi variable at step i for tag k.\n",
        "-  Modify the above recurrence to compute the forward variables instead.\n",
        "-  Modify again the above recurrence to compute the forward variables in\n",
        "   log-space (hint: log-sum-exp)\n",
        "\n",
        "If you can do those three things, you should be able to understand the\n",
        "code below. Recall that the CRF computes a conditional probability. Let\n",
        ":math:`y` be a tag sequence and :math:`x` an input sequence of words.\n",
        "Then we compute\n",
        "\n",
        ".. math::  P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\n",
        "\n",
        "Where the score is determined by defining some log potentials\n",
        ":math:`\\log \\psi_i(x,y)` such that\n",
        "\n",
        ".. math::  \\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\n",
        "\n",
        "To make the partition function tractable, the potentials must look only\n",
        "at local features.\n",
        "\n",
        "In the Bi-LSTM CRF, we define two kinds of potentials: emission and\n",
        "transition. The emission potential for the word at index :math:`i` comes\n",
        "from the hidden state of the Bi-LSTM at timestep :math:`i`. The\n",
        "transition scores are stored in a :math:`|T|x|T|` matrix\n",
        ":math:`\\textbf{P}`, where :math:`T` is the tag set. In my\n",
        "implementation, :math:`\\textbf{P}_{j,k}` is the score of transitioning\n",
        "to tag :math:`j` from tag :math:`k`. So:\n",
        "\n",
        ".. math::  \\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\n",
        "\n",
        ".. math::  = \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\n",
        "\n",
        "where in this second expression, we think of the tags as being assigned\n",
        "unique non-negative indices.\n",
        "\n",
        "If the above discussion was too brief, you can check out\n",
        "`this <http://www.cs.columbia.edu/%7Emcollins/crf.pdf>`__ write up from\n",
        "Michael Collins on CRFs.\n",
        "\n",
        "Implementation Notes\n",
        "--------------------\n",
        "\n",
        "The example below implements the forward algorithm in log space to\n",
        "compute the partition function, and the viterbi algorithm to decode.\n",
        "Backpropagation will compute the gradients automatically for us. We\n",
        "don't have to do anything by hand.\n",
        "\n",
        "The implementation is not optimized. If you understand what is going on,\n",
        "you'll probably quickly see that iterating over the next tag in the\n",
        "forward algorithm could probably be done in one big operation. I wanted\n",
        "to code to be more readable. If you want to make the relevant change,\n",
        "you could probably use this tagger for real tasks.\n",
        "\"\"\"\n",
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "################################################################################\n",
        "# Helper functions to make the code more readable.\n",
        "################################################################################\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "#####################################################################\n",
        "# Create model\n",
        "\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "#####################################################################\n",
        "# Run training\n",
        "\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 4\n",
        "\n",
        "# Make up some training data\n",
        "training_data = [(\n",
        "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
        "    \"B I I I O O O B I O O\".split()\n",
        "), (\n",
        "    \"georgia tech is a university in georgia\".split(),\n",
        "    \"B I O O O O B\".split()\n",
        ")]\n",
        "\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print(model(precheck_sent))\n",
        "\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for epoch in range(\n",
        "        300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Check predictions after training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    print(model(precheck_sent))\n",
        "# We got it!\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Exercise: A new loss function for discriminative tagging\n",
        "# --------------------------------------------------------\n",
        "#\n",
        "# It wasn't really necessary for us to create a computation graph when\n",
        "# doing decoding, since we do not backpropagate from the viterbi path\n",
        "# score. Since we have it anyway, try training the tagger where the loss\n",
        "# function is the difference between the Viterbi path score and the score\n",
        "# of the gold-standard path. It should be clear that this function is\n",
        "# non-negative and 0 when the predicted tag sequence is the correct tag\n",
        "# sequence. This is essentially *structured perceptron*.\n",
        "#\n",
        "# This modification should be short, since Viterbi and score\\_sentence are\n",
        "# already implemented. This is an example of the shape of the computation\n",
        "# graph *depending on the training instance*. Although I haven't tried\n",
        "# implementing this in a static toolkit, I imagine that it is possible but\n",
        "# much less straightforward.\n",
        "#\n",
        "# Pick up some real data and do a comparison!\n",
        "#"
      ],
      "metadata": {
        "id": "P2E77GcQpXzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 15 Transformer***"
      ],
      "metadata": {
        "id": "53aZxO5swp-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Language Modeling with nn.Transformer and TorchText [beginner_source/transformer_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/transformer_tutorial.py)"
      ],
      "metadata": {
        "id": "yOYGdsKewqFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Language Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on training a sequence-to-sequence model that uses the\n",
        "`nn.Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ module.\n",
        "\n",
        "The PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper `Attention is All You Need <https://arxiv.org/pdf/1706.03762.pdf>`__.\n",
        "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
        "to be superior in quality for many sequence-to-sequence tasks while being more\n",
        "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
        "mechanism (implemented as\n",
        "`nn.MultiheadAttention <https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>`__)\n",
        "to draw global dependencies between input and output. The ``nn.Transformer``\n",
        "module is highly modularized such that a single component (e.g.,\n",
        "`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>`__)\n",
        "can be easily adapted/composed.\n",
        "\n",
        ".. image:: ../_static/img/transformer_architecture.jpg\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
        "# language modeling task. The language modeling task is to assign a\n",
        "# probability for the likelihood of a given word (or a sequence of words)\n",
        "# to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "# layer first, followed by a positional encoding layer to account for the order\n",
        "# of the word (see the next paragraph for more details). The\n",
        "# ``nn.TransformerEncoder`` consists of multiple layers of\n",
        "# `nn.TransformerEncoderLayer <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>`__.\n",
        "# Along with the input sequence, a square attention mask is required because the\n",
        "# self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
        "# the earlier positions in the sequence. For the language modeling task, any\n",
        "# tokens on the future positions should be masked. To produce a probability\n",
        "# distribution over output words, the output of the ``nn.TransformerEncoder``\n",
        "# model is passed through a linear layer followed by a log-softmax function.\n",
        "#\n",
        "\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# ``PositionalEncoding`` module injects some information about the\n",
        "# relative or absolute position of the tokens in the sequence. The\n",
        "# positional encodings have the same dimension as the embeddings so that\n",
        "# the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "# different frequencies.\n",
        "#\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Load and batch data\n",
        "# -------------------\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# This tutorial uses ``torchtext`` to generate Wikitext-2 dataset.\n",
        "# To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data. \n",
        "# %%\n",
        "#  .. code-block:: bash\n",
        "#\n",
        "#      %%bash\n",
        "#      pip install torchdata\n",
        "#\n",
        "# The vocab object is built based on the train dataset and is used to numericalize\n",
        "# tokens into tensors. Wikitext-2 represents rare tokens as `<unk>`.\n",
        "#\n",
        "# Given a 1-D vector of sequential data, ``batchify()`` arranges the data\n",
        "# into ``batch_size`` columns. If the data does not divide evenly into\n",
        "# ``batch_size`` columns, then the data is trimmed to fit. For instance, with\n",
        "# the alphabet as the data (total length of 26) and ``batch_size=4``, we would\n",
        "# divide the alphabet into 4 sequences of length 6:\n",
        "#\n",
        "# .. math::\n",
        "#   \\begin{bmatrix}\n",
        "#   \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "#   \\end{bmatrix}\n",
        "#   \\Rightarrow\n",
        "#   \\begin{bmatrix}\n",
        "#   \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "#   \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "#   \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "#   \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "#   \\end{bmatrix}\n",
        "#\n",
        "# Batching enables more parallelizable processing. However, batching means that\n",
        "# the model treats each column independently; for example, the dependence of\n",
        "# ``G`` and ``F`` can not be learned in the example above.\n",
        "#\n",
        "\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>']) \n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Functions to generate input and target sequence\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# ``get_batch()`` generates a pair of input-target sequences for\n",
        "# the transformer model. It subdivides the source data into chunks of\n",
        "# length ``bptt``. For the language modeling task, the model needs the\n",
        "# following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "# we’d get the following two Variables for ``i`` = 0:\n",
        "#\n",
        "# .. image:: ../_static/img/transformer_input_target.png\n",
        "#\n",
        "# It should be noted that the chunks are along dimension 0, consistent\n",
        "# with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "# ``N`` is along dimension 1.\n",
        "#\n",
        "\n",
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Initiate an instance\n",
        "# --------------------\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# The model hyperparameters are defined below. The vocab size is\n",
        "# equal to the length of the vocab object.\n",
        "#\n",
        "\n",
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Run the model\n",
        "# -------------\n",
        "#\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# We use `CrossEntropyLoss <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__\n",
        "# with the `SGD <https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>`__\n",
        "# (stochastic gradient descent) optimizer. The learning rate is initially set to\n",
        "# 5.0 and follows a `StepLR <https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html>`__\n",
        "# schedule. During training, we use `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html>`__\n",
        "# to prevent gradients from exploding.\n",
        "#\n",
        "\n",
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "        if seq_len != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:seq_len, :seq_len]\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)\n",
        "\n",
        "######################################################################\n",
        "# Loop over epochs. Save the model if the validation loss is the best\n",
        "# we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Evaluate the best model on the test dataset\n",
        "# -------------------------------------------\n",
        "#\n",
        "\n",
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "id": "9-Tf4qRPwz4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Language Translation with nn.Transformer and torchtext [beginner_source/translation_transformer.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/translation_transformer.py)"
      ],
      "metadata": {
        "id": "kYe6KOvaxHJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Language Translation with nn.Transformer and torchtext\n",
        "======================================================\n",
        "\n",
        "This tutorial shows:\n",
        "    - How to train a translation model from scratch using Transformer.\n",
        "    - Use torchtext library to access  `Multi30k <http://www.statmt.org/wmt16/multimodal-task.html#task1>`__ dataset to train a German to English translation model.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Data Sourcing and Processing\n",
        "# ----------------------------\n",
        "#\n",
        "# `torchtext library <https://pytorch.org/text/stable/>`__ has utilities for creating datasets that can be easily\n",
        "# iterated through for the purposes of creating a language translation\n",
        "# model. In this example, we show how to use torchtext's inbuilt datasets,\n",
        "# tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
        "# `Multi30k dataset from torchtext library <https://pytorch.org/text/stable/datasets.html#multi30k>`__\n",
        "# that yields a pair of source-target raw sentences.\n",
        "#\n",
        "# To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data.\n",
        "#\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "# pip install -U torchdata\n",
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "# python -m spacy download de_core_news_sm\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)\n",
        "\n",
        "######################################################################\n",
        "# Seq2Seq Network using Transformer\n",
        "# ---------------------------------\n",
        "#\n",
        "# Transformer is a Seq2Seq model introduced in `“Attention is all you\n",
        "# need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n",
        "# paper for solving machine translation tasks.\n",
        "# Below, we will create a Seq2Seq network that uses Transformer. The network\n",
        "# consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
        "# into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
        "# encodings to provide position information of input tokens to the model. The second part is the\n",
        "# actual `Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ model.\n",
        "# Finally, the output of the Transformer model is passed through linear layer\n",
        "# that gives un-normalized probabilities for each token in the target language.\n",
        "#\n",
        "\n",
        "\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# During training, we need a subsequent word mask that will prevent the model from looking into\n",
        "# the future words when making predictions. We will also need masks to hide\n",
        "# source and target padding tokens. Below, let's define a function that will take care of both.\n",
        "#\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Let's now define the parameters of our model and instantiate the same. Below, we also\n",
        "# define our loss function which is the cross-entropy loss and the optmizer used for training.\n",
        "#\n",
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "######################################################################\n",
        "# Collation\n",
        "# ---------\n",
        "#\n",
        "# As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings.\n",
        "# We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network\n",
        "# defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that\n",
        "# can be fed directly into our model.\n",
        "#\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "######################################################################\n",
        "# Let's define training and evaluation loop that will be called for each\n",
        "# epoch.\n",
        "#\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))\n",
        "\n",
        "######################################################################\n",
        "# Now we have all the ingredients to train our model. Let's do it!\n",
        "#\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#\n",
        "\n",
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# References\n",
        "# ----------\n",
        "#\n",
        "# 1. Attention is all you need paper.\n",
        "#    https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "# 2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding"
      ],
      "metadata": {
        "id": "QSqdQTELxHXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 16 Regularization***"
      ],
      "metadata": {
        "id": "jvl9KzpQjQIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Dropout [examples/blob/main/mnist/main.py](https://github.com/pytorch/examples/blob/main/mnist/main.py)"
      ],
      "metadata": {
        "id": "ddopKiGmjQLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if args.dry_run:\n",
        "                break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
        "                        help='learning rate (default: 1.0)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='Learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                        help='disables macOS GPU training')\n",
        "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                        help='quickly check a single pass')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                        help='For Saving the current Model')\n",
        "    args = parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif use_mps:\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    train_kwargs = {'batch_size': args.batch_size}\n",
        "    test_kwargs = {'batch_size': args.test_batch_size}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "kZtimHTjkDMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### BatchNorm1d [examples/blob/main/mnist_rnn/main.py](https://github.com/pytorch/examples/blob/main/mnist_rnn/main.py)"
      ],
      "metadata": {
        "id": "AgLIFipsj4Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size=28, hidden_size=64, batch_first=True)\n",
        "        self.batchnorm = nn.BatchNorm1d(64)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Shape of input is (batch_size,1, 28, 28)\n",
        "        # converting shape of input to (batch_size, 28, 28)\n",
        "        # as required by RNN when batch_first is set True\n",
        "        input = input.reshape(-1, 28, 28)\n",
        "        output, hidden = self.rnn(input)\n",
        "\n",
        "        # RNN output shape is (seq_len, batch, input_size)\n",
        "        # Get last output of RNN\n",
        "        output = output[:, -1, :]\n",
        "        output = self.batchnorm(output)\n",
        "        output = self.dropout1(output)\n",
        "        output = self.fc1(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.dropout2(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if args.dry_run:\n",
        "                break\n",
        "\n",
        "\n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            if args.dry_run:\n",
        "                break\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example using RNN')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
        "                        help='learning rate (default: 0.1)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                        help='quickly check a single pass')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                        help='for Saving the current Model')\n",
        "    args = parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(args, model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_rnn.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "v7iMe8eWj4cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 17 Control Flow and Weight Sharing***"
      ],
      "metadata": {
        "id": "iAFG7XYEH8td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Control Flow and Weight Sharing [beginner_source/examples_nn/dynamic_net.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/dynamic_net.py)"
      ],
      "metadata": {
        "id": "1K6JJx2Hj_po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PyTorch: Control Flow + Weight Sharing\n",
        "--------------------------------------\n",
        "\n",
        "To showcase the power of PyTorch dynamic graphs, we will implement a very strange\n",
        "model: a third-fifth order polynomial that on each forward pass\n",
        "chooses a random number between 4 and 5 and uses that many orders, reusing\n",
        "the same weights multiple times to compute the fourth and fifth order.\n",
        "\"\"\"\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate five parameters and assign them as members.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "        self.e = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        For the forward pass of the model, we randomly choose either 4, 5\n",
        "        and reuse the e parameter to compute the contribution of these orders.\n",
        "\n",
        "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
        "        Python control-flow operators like loops or conditional statements when\n",
        "        defining the forward pass of the model.\n",
        "\n",
        "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
        "        times when defining a computational graph.\n",
        "        \"\"\"\n",
        "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "        for exp in range(4, random.randint(4, 6)):\n",
        "            y = y + self.e * x ** exp\n",
        "        return y\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = DynamicNet()\n",
        "\n",
        "# Construct our loss function and an Optimizer. Training this strange model with\n",
        "# vanilla stochastic gradient descent is tough, so we use momentum\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
        "for t in range(30000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 2000 == 1999:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "metadata": {
        "id": "YQbFEsr7j_zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 18 Save and Load***"
      ],
      "metadata": {
        "id": "Nf56ji_SiL8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [beginner_source/basics/saveloadrun_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/saveloadrun_tutorial.py)"
      ],
      "metadata": {
        "id": "4cGSn-srXWme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save and Load the Model\n",
        "============================\n",
        "\n",
        "In this section we will look at how to persist model state \n",
        "with saving, loading and running model predictions.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Saving and Loading Model Weights\n",
        "# \n",
        "# PyTorch models store the learned parameters in an internal\n",
        "# state dictionary, called ``state_dict``. \n",
        "# These can be persisted via the ``torch.save`` method:\n",
        "################################################################################\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "################################################################################\n",
        "# To load model weights, \n",
        "# you need to create an instance of the same model first, \n",
        "# and then load the parameters using ``load_state_dict()`` method.\n",
        "################################################################################\n",
        "\n",
        "model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "################################################################################\n",
        "# .. note:: be sure to call ``model.eval()`` method \n",
        "#           before inferencing to set the dropout and batch normalization layers \n",
        "#           to evaluation mode. \n",
        "#           Failing to do this will yield inconsistent inference results.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Saving and Loading Models with Shapes\n",
        "# \n",
        "# When loading model weights, we needed to instantiate the model class first, \n",
        "# because the class\n",
        "# defines the structure of a network. We might want \n",
        "# to save the structure of this class together with\n",
        "# the model, in which case we can pass ``model`` (and not ``model.state_dict()``) \n",
        "# to the saving function:\n",
        "################################################################################\n",
        "\n",
        "torch.save(model, 'model.pth')\n",
        "\n",
        "################################################################################\n",
        "# We can then load the model like this:\n",
        "################################################################################\n",
        "\n",
        "model = torch.load('model.pth')\n",
        "\n",
        "################################################################################\n",
        "# .. note:: This approach uses \n",
        "#           Python `pickle <https://docs.python.org/3/library/pickle.html>`_ \n",
        "#           module when serializing the model, thus it relies on \n",
        "#           the actual class definition to be available when loading the model.\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Related Tutorials\n",
        "# \n",
        "# `Saving and Loading a General Checkpoint in PyTorch \n",
        "# <https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html>`_\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "f9nKt_ZrXWwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Chapter 19 Data Parallelism***"
      ],
      "metadata": {
        "id": "uDtdtQyfiTTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [beginner_source/blitz/data_parallel_tutorial.py](https://github.com/pytorch/tutorials/blob/main/beginner_source/blitz/data_parallel_tutorial.py)"
      ],
      "metadata": {
        "id": "J0YZ9ORPXW3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Optional: Data Parallelism\n",
        "==========================\n",
        "**Authors**: `Sung Kim <https://github.com/hunkim>`_ and `Jenny Kang <https://github.com/jennykang>`_\n",
        "\n",
        "In this tutorial, we will learn how to use multiple GPUs using ``DataParallel``.\n",
        "\n",
        "It's very easy to use GPUs with PyTorch. You can put the model on a GPU:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    model.to(device)\n",
        "\n",
        "Then, you can copy all your tensors to the GPU:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "    mytensor = my_tensor.to(device)\n",
        "\n",
        "Please note that just calling ``my_tensor.to(device)`` returns a new copy of\n",
        "``my_tensor`` on GPU instead of rewriting ``my_tensor``. You need to assign it to\n",
        "a new tensor and use that tensor on the GPU.\n",
        "\n",
        "It's natural to execute your forward, backward propagations on multiple GPUs.\n",
        "However, Pytorch will only use one GPU by default. You can easily run your\n",
        "operations on multiple GPUs by making your model run parallelly using\n",
        "``DataParallel``:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "That's the core behind this tutorial. We will explore it in more detail below.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Imports and parameters\n",
        "#\n",
        "# Import PyTorch modules and define parameters.\n",
        "################################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "input_size = 5\n",
        "output_size = 2\n",
        "\n",
        "batch_size = 30\n",
        "data_size = 100\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Device\n",
        "################################################################################\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "################################################################################\n",
        "# Dummy DataSet\n",
        "#\n",
        "# Make a dummy (random) dataset. You just need to implement the\n",
        "# getitem\n",
        "################################################################################\n",
        "\n",
        "class RandomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, size, length):\n",
        "        self.len = length\n",
        "        self.data = torch.randn(length, size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n",
        "                         batch_size=batch_size, shuffle=True)\n",
        "\n",
        "################################################################################\n",
        "# Simple Model\n",
        "#\n",
        "# For the demo, our model just gets an input, performs a linear operation, and\n",
        "# gives an output. However, you can use ``DataParallel`` on any model (CNN, RNN,\n",
        "# Capsule Net etc.)\n",
        "#\n",
        "# We've placed a print statement inside the model to monitor the size of input\n",
        "# and output tensors.\n",
        "# Please pay attention to what is printed at batch rank 0.\n",
        "################################################################################\n",
        "\n",
        "class Model(nn.Module):\n",
        "    # Our model\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.fc(input)\n",
        "        print(\"\\tIn Model: input size\", input.size(),\n",
        "              \"output size\", output.size())\n",
        "\n",
        "        return output\n",
        "\n",
        "################################################################################\n",
        "# Create Model and DataParallel\n",
        "#\n",
        "# This is the core part of the tutorial. First, we need to make a model instance\n",
        "# and check if we have multiple GPUs. If we have multiple GPUs, we can wrap\n",
        "# our model using ``nn.DataParallel``. Then we can put our model on GPUs by\n",
        "# ``model.to(device)``\n",
        "################################################################################\n",
        "\n",
        "model = Model(input_size, output_size)\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "################################################################################\n",
        "# Run the Model\n",
        "#\n",
        "# Now we can see the sizes of input and output tensors.\n",
        "################################################################################\n",
        "\n",
        "for data in rand_loader:\n",
        "    input = data.to(device)\n",
        "    output = model(input)\n",
        "    print(\"Outside: input size\", input.size(),\n",
        "          \"output_size\", output.size())\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Results\n",
        "#\n",
        "# If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as\n",
        "# expected. But if you have multiple GPUs, then you can get results like this.\n",
        "#\n",
        "# 2 GPUs\n",
        "# ~~~~~~\n",
        "#\n",
        "# If you have 2, you will see:\n",
        "#\n",
        "# .. code:: bash\n",
        "#\n",
        "#     # on 2 GPUs\n",
        "#     Let's use 2 GPUs!\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
        "#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
        "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
        "#\n",
        "# 3 GPUs\n",
        "# ~~~~~~\n",
        "#\n",
        "# If you have 3 GPUs, you will see:\n",
        "#\n",
        "# .. code:: bash\n",
        "#\n",
        "#     Let's use 3 GPUs!\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
        "#\n",
        "# 8 GPUs\n",
        "# ~~~~~~~~~~~~~~\n",
        "#\n",
        "# If you have 8, you will see:\n",
        "#\n",
        "# .. code:: bash\n",
        "#\n",
        "#     Let's use 8 GPUs!\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Summary\n",
        "#\n",
        "# DataParallel splits your data automatically and sends job orders to multiple\n",
        "# models on several GPUs. After each model finishes their job, DataParallel\n",
        "# collects and merges the results before returning it to you.\n",
        "#\n",
        "# For more information, please check out\n",
        "# https://pytorch.org/tutorials/beginner/former\\_torchies/parallelism\\_tutorial.html.\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "fq_vcH5DXW_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}